diff --git a/Algorithm/common/evaluation.py b/Algorithm/common/evaluation.py
index 0c31934..c6c9574 100644
--- a/Algorithm/common/evaluation.py
+++ b/Algorithm/common/evaluation.py
@@ -19,12 +19,42 @@ from Algorithm.common.performance_indicators import (
 from Algorithm.common.weights import equally_spaced_weights
 
 
+def eval_mo_demo(
+        env,
+        w=None,
+        scalarization=np.dot,
+        demo=None,
+        GAMMA=0.99):
+    obs, _ = env.reset()
+    vec_return, disc_vec_return = np.zeros_like(w), np.zeros_like(w)
+    gamma = 1.0
+    for action in demo:
+        obs, r, terminated, truncated, info = env.step(action)
+        vec_return += r
+        disc_vec_return += gamma * r
+        gamma *= GAMMA
+
+    if w is None:
+        scalarized_return = scalarization(vec_return)
+        scalarized_discounted_return = scalarization(disc_vec_return)
+    else:
+        scalarized_return = scalarization(w, vec_return)
+        scalarized_discounted_return = scalarization(w, disc_vec_return)
+
+    return (
+        scalarized_return,
+        scalarized_discounted_return,
+        vec_return,
+        disc_vec_return,
+    )
+
+
 def eval_mo(
-    agent,
-    env,
-    w: Optional[np.ndarray] = None,
-    scalarization=np.dot,
-    render: bool = False,
+        agent,
+        env,
+        w: Optional[np.ndarray] = None,
+        scalarization=np.dot,
+        render: bool = False,
 ) -> Tuple[float, float, np.ndarray, np.ndarray]:
     """Evaluates one episode of the agent in the environment.
 
@@ -67,11 +97,11 @@ def eval_mo(
 
 
 def eval_mo_reward_conditioned(
-    agent,
-    env,
-    scalarization=np.dot,
-    w: Optional[np.ndarray] = None,
-    render: bool = False,
+        agent,
+        env,
+        scalarization=np.dot,
+        w: Optional[np.ndarray] = None,
+        render: bool = False,
 ) -> Tuple[float, float, np.ndarray, np.ndarray]:
     """Evaluates one episode of the agent in the environment. This makes the assumption that the agent is conditioned on the accrued reward i.e. for ESR agent.
 
@@ -139,12 +169,12 @@ def policy_evaluation_mo(agent, env, w: np.ndarray, rep: int = 5) -> Tuple[float
 
 
 def log_all_multi_policy_metrics(
-    current_front: List[np.ndarray],
-    hv_ref_point: np.ndarray,
-    reward_dim: int,
-    global_step: int,
-    n_sample_weights: int = 50,
-    ref_front: Optional[List[np.ndarray]] = None,
+        current_front: List[np.ndarray],
+        hv_ref_point: np.ndarray,
+        reward_dim: int,
+        global_step: int,
+        n_sample_weights: int = 50,
+        ref_front: Optional[List[np.ndarray]] = None,
 ):
     """Logs all metrics for multi-policy training.
 
@@ -214,12 +244,12 @@ def seed_everything(seed: int):
 
 
 def log_episode_info(
-    info: dict,
-    scalarization,
-    weights: Optional[np.ndarray],
-    global_timestep: int,
-    id: Optional[int] = None,
-    verbose: bool = True,
+        info: dict,
+        scalarization,
+        weights: Optional[np.ndarray],
+        global_timestep: int,
+        id: Optional[int] = None,
+        verbose: bool = True,
 ):
     """Logs information of the last episode from the info dict (automatically filled by the RecordStatisticsWrapper).
 
diff --git a/Algorithm/common/weights.py b/Algorithm/common/weights.py
index 16bc67e..e241ea7 100644
--- a/Algorithm/common/weights.py
+++ b/Algorithm/common/weights.py
@@ -56,3 +56,7 @@ def extrema_weights(dim: int) -> List[np.ndarray]:
         dim: size of the weight vector
     """
     return list(np.eye(dim, dtype=np.float32))
+
+if __name__ == '__main__':
+    print(equally_spaced_weights(dim=2,n=101))
+    print(extrema_weights(dim=2))
\ No newline at end of file
diff --git a/Algorithm/go_explore/__pycache__/explore.cpython-311.pyc b/Algorithm/go_explore/__pycache__/explore.cpython-311.pyc
deleted file mode 100644
index b240f8a..0000000
Binary files a/Algorithm/go_explore/__pycache__/explore.cpython-311.pyc and /dev/null differ
diff --git a/Algorithm/go_explore/__pycache__/explore.cpython-39.pyc b/Algorithm/go_explore/__pycache__/explore.cpython-39.pyc
deleted file mode 100644
index 2aeb37a..0000000
Binary files a/Algorithm/go_explore/__pycache__/explore.cpython-39.pyc and /dev/null differ
diff --git a/Algorithm/gpi_ls.py b/Algorithm/gpi_ls.py
index a67b79f..91b3195 100644
--- a/Algorithm/gpi_ls.py
+++ b/Algorithm/gpi_ls.py
@@ -85,48 +85,48 @@ class GPIPD(MOPolicy, MOAgent):
     """
 
     def __init__(
-        self,
-        env,
-        learning_rate: float = 3e-4,
-        initial_epsilon: float = 0.01,
-        final_epsilon: float = 0.01,
-        epsilon_decay_steps: int = None,  # None == fixed epsilon
-        tau: float = 1.0,
-        target_net_update_freq: int = 1000,  # ignored if tau != 1.0
-        buffer_size: int = int(1e6),
-        net_arch: List = [256, 256, 256, 256],
-        num_nets: int = 2,
-        batch_size: int = 128,
-        learning_starts: int = 100,
-        gradient_updates: int = 20,
-        gamma: float = 0.99,
-        max_grad_norm: Optional[float] = None,
-        use_gpi: bool = True,
-        dyna: bool = True,
-        per: bool = True,
-        gpi_pd: bool = True,
-        alpha_per: float = 0.6,
-        min_priority: float = 0.01,
-        drop_rate: float = 0.01,
-        layer_norm: bool = True,
-        dynamics_normalize_inputs: bool = False,
-        dynamics_uncertainty_threshold: float = 1.5,
-        dynamics_train_freq: Callable = lambda timestep: 250,
-        dynamics_rollout_len: int = 1,
-        dynamics_rollout_starts: int = 5000,
-        dynamics_rollout_freq: int = 250,
-        dynamics_rollout_batch_size: int = 25000,
-        dynamics_buffer_size: int = 100000,
-        dynamics_net_arch: List = [256, 256, 256],
-        dynamics_ensemble_size: int = 5,
-        dynamics_num_elites: int = 2,
-        real_ratio: float = 0.5,
-        project_name: str = "MORL-Baselines",
-        experiment_name: str = "GPI-PD",
-        wandb_entity: Optional[str] = None,
-        log: bool = True,
-        seed: Optional[int] = None,
-        device: Union[th.device, str] = "auto",
+            self,
+            env,
+            learning_rate: float = 3e-4,
+            initial_epsilon: float = 0.01,
+            final_epsilon: float = 0.01,
+            epsilon_decay_steps: int = None,  # None == fixed epsilon
+            tau: float = 1.0,
+            target_net_update_freq: int = 1000,  # ignored if tau != 1.0
+            buffer_size: int = int(1e6),
+            net_arch: List = [256, 256, 256, 256],
+            num_nets: int = 2,
+            batch_size: int = 128,
+            learning_starts: int = 100,
+            gradient_updates: int = 20,
+            gamma: float = 0.99,
+            max_grad_norm: Optional[float] = None,
+            use_gpi: bool = True,
+            dyna: bool = True,
+            per: bool = True,
+            gpi_pd: bool = True,
+            alpha_per: float = 0.6,
+            min_priority: float = 0.01,
+            drop_rate: float = 0.01,
+            layer_norm: bool = True,
+            dynamics_normalize_inputs: bool = False,
+            dynamics_uncertainty_threshold: float = 1.5,
+            dynamics_train_freq: Callable = lambda timestep: 250,
+            dynamics_rollout_len: int = 1,
+            dynamics_rollout_starts: int = 5000,
+            dynamics_rollout_freq: int = 250,
+            dynamics_rollout_batch_size: int = 25000,
+            dynamics_buffer_size: int = 100000,
+            dynamics_net_arch: List = [256, 256, 256],
+            dynamics_ensemble_size: int = 5,
+            dynamics_num_elites: int = 2,
+            real_ratio: float = 0.5,
+            project_name: str = "MORL-Baselines",
+            experiment_name: str = "GPI-PD",
+            wandb_entity: Optional[str] = None,
+            log: bool = True,
+            seed: Optional[int] = None,
+            device: Union[th.device, str] = "auto",
     ):
         """Initialize the GPI-PD algorithm.
 
@@ -383,7 +383,8 @@ class GPIPD(MOPolicy, MOAgent):
                 obs_m = obs.unsqueeze(1).repeat(1, M.size(1), 1)
 
                 psi_values = self.q_nets[0](obs_m, M)
-                q_values = th.einsum("r,bar->ba", w, psi_values).view(obs.size(0), len(self.weight_support), self.action_dim)
+                q_values = th.einsum("r,bar->ba", w, psi_values).view(obs.size(0), len(self.weight_support),
+                                                                      self.action_dim)
                 max_q, ac = th.max(q_values, dim=2)
                 pi = th.argmax(max_q, dim=1)
                 actions = ac.gather(1, pi.unsqueeze(1))
@@ -434,7 +435,8 @@ class GPIPD(MOPolicy, MOAgent):
                 )
                 # Half of the batch uses the given weight vector, the other half uses weights sampled from the support set
                 w = th.vstack(
-                    [weight for _ in range(s_obs.size(0) // 2)] + random.choices(self.weight_support, k=s_obs.size(0) // 2)
+                    [weight for _ in range(s_obs.size(0) // 2)] + random.choices(self.weight_support,
+                                                                                 k=s_obs.size(0) // 2)
                 )
             else:
                 w = weight.repeat(s_obs.size(0), 1)
@@ -489,12 +491,13 @@ class GPIPD(MOPolicy, MOAgent):
                 if self.per:
                     td_errors.append(td_error.abs())
             critic_loss = (1 / self.num_nets) * sum(losses)
-
+            # print(f"critic_loss:{critic_loss}")
             self.q_optim.zero_grad()
             critic_loss.backward()
             if self.log and self.global_step % 100 == 0:
                 wandb.log(
-                    {"losses/grad_norm": get_grad_norm(self.q_nets[0].parameters()).item(), "global_step": self.global_step},
+                    {"losses/grad_norm": get_grad_norm(self.q_nets[0].parameters()).item(),
+                     "global_step": self.global_step},
                 )
             if self.max_grad_norm is not None:
                 for psi_net in self.q_nets:
@@ -528,7 +531,8 @@ class GPIPD(MOPolicy, MOAgent):
 
         if self.epsilon_decay_steps is not None:
             self.epsilon = linearly_decaying_value(
-                self.initial_epsilon, self.epsilon_decay_steps, self.global_step, self.learning_starts, self.final_epsilon
+                self.initial_epsilon, self.epsilon_decay_steps, self.global_step, self.learning_starts,
+                self.final_epsilon
             )
 
         if self.log and self.global_step % 100 == 0:
@@ -576,6 +580,7 @@ class GPIPD(MOPolicy, MOAgent):
         action = a[policy_index].detach().item()
 
         if return_policy_index:
+            # print(f"scalar_q_values:{scalar_q_values}\taction:{action}\tpolicy_index:{policy_index}")
             return action, policy_index.item()
         return action
 
@@ -624,12 +629,15 @@ class GPIPD(MOPolicy, MOAgent):
         for i in range(num_batches):
             b = i * 1000
             e = min((i + 1) * 1000, obs_s.size(0))
-            obs, actions, rewards, next_obs, dones = obs_s[b:e], actions_s[b:e], rewards_s[b:e], next_obs_s[b:e], dones_s[b:e]
+            obs, actions, rewards, next_obs, dones = obs_s[b:e], actions_s[b:e], rewards_s[b:e], next_obs_s[
+                                                                                                 b:e], dones_s[b:e]
             q_values = self.q_nets[0](obs, w.repeat(obs.size(0), 1))
-            q_a = q_values.gather(1, actions.long().reshape(-1, 1, 1).expand(q_values.size(0), 1, q_values.size(2))).squeeze(1)
+            q_a = q_values.gather(1, actions.long().reshape(-1, 1, 1).expand(q_values.size(0), 1,
+                                                                             q_values.size(2))).squeeze(1)
 
             if self.gpi_pd:
-                max_next_q, _ = self._envelope_target(next_obs, w.repeat(next_obs.size(0), 1), th.stack(self.weight_support))
+                max_next_q, _ = self._envelope_target(next_obs, w.repeat(next_obs.size(0), 1),
+                                                      th.stack(self.weight_support))
             else:
                 next_q_values = self.q_nets[0](next_obs, w.repeat(next_obs.size(0), 1))
                 max_q = th.einsum("r,bar->ba", w, next_q_values)
@@ -656,7 +664,7 @@ class GPIPD(MOPolicy, MOAgent):
                 for target_net in self.target_q_nets
             ]
         )
-
+        print(f"next_q_target:{next_q_target}")
         q_values = th.einsum("br,nbpar->nbpa", w, next_q_target)
         min_inds = th.argmin(q_values, dim=0)
         min_inds = min_inds.reshape(1, next_q_target.size(1), next_q_target.size(2), next_q_target.size(3), 1).expand(
@@ -672,7 +680,9 @@ class GPIPD(MOPolicy, MOAgent):
             2,
             ac.unsqueeze(2).unsqueeze(3).expand(next_q_target.size(0), next_q_target.size(1), 1, next_q_target.size(3)),
         ).squeeze(2)
-        max_next_q = max_next_q.gather(1, pi.reshape(-1, 1, 1).expand(max_next_q.size(0), 1, max_next_q.size(2))).squeeze(1)
+        max_next_q = max_next_q.gather(1,
+                                       pi.reshape(-1, 1, 1).expand(max_next_q.size(0), 1, max_next_q.size(2))).squeeze(
+            1)
         return max_next_q, next_q_target
 
     def set_weight_support(self, weight_list: List[np.ndarray]):
@@ -681,15 +691,15 @@ class GPIPD(MOPolicy, MOAgent):
         self.weight_support = [th.tensor(w).float().to(self.device) for w in weights_no_repeats]
 
     def train_iteration(
-        self,
-        total_timesteps: int,
-        weight: np.ndarray,
-        weight_support: List[np.ndarray],
-        change_w_every_episode: bool = True,
-        reset_num_timesteps: bool = True,
-        eval_env: Optional[gym.Env] = None,
-        eval_freq: int = 1000,
-        reset_learning_starts: bool = False,
+            self,
+            total_timesteps: int,
+            weight: np.ndarray,
+            weight_support: List[np.ndarray],
+            change_w_every_episode: bool = True,
+            reset_num_timesteps: bool = True,
+            eval_env: Optional[gym.Env] = None,
+            eval_freq: int = 1000,
+            reset_learning_starts: bool = False,
     ):
         """Train the agent for one iteration.
 
@@ -716,7 +726,8 @@ class GPIPD(MOPolicy, MOAgent):
             self._reset_priorities(tensor_w)
 
         obs, info = self.env.reset()
-        for _ in range(1, total_timesteps + 1):
+        for i in range(1, total_timesteps + 1):
+            print(f"train_step:{i}")
             self.global_step += 1
 
             if self.global_step < self.learning_starts:
@@ -773,15 +784,16 @@ class GPIPD(MOPolicy, MOAgent):
                 obs = next_obs
 
     def train(
-        self,
-        total_timesteps: int,
-        eval_env,
-        ref_point: np.ndarray,
-        known_pareto_front: Optional[List[np.ndarray]] = None,
-        num_eval_weights_for_front: int = 100,
-        num_eval_episodes_for_front: int = 5,
-        timesteps_per_iter: int = 10000,
-        weight_selection_algo: str = "gpi-ls",
+            self,
+            total_timesteps: int,
+            eval_env,
+            ref_point: np.ndarray,
+            known_pareto_front: Optional[List[np.ndarray]] = None,
+            num_eval_weights_for_front: int = 100,
+            num_eval_episodes_for_front: int = 5,
+            timesteps_per_iter: int = 4000,
+            weight_selection_algo: str = "gpi-ls",
+            prior_knowledge_available=True
     ):
         """Train agent.
 
@@ -795,10 +807,29 @@ class GPIPD(MOPolicy, MOAgent):
             timesteps_per_iter (int): Number of timesteps to train for per iteration.
             weight_selection_algo (str): Weight selection algorithm to use.
         """
+        print(f"gpi_pd:{self.gpi_pd}")
+        expected_utilities = []
+        # print(f"self.per:{self.per}")
         if self.log:
             self.register_additional_config({"ref_point": ref_point.tolist(), "known_front": known_pareto_front})
         max_iter = total_timesteps // timesteps_per_iter
-        linear_support = LinearSupport(num_objectives=self.reward_dim, epsilon=0.0 if weight_selection_algo == "ols" else None)
+        linear_support = LinearSupport(num_objectives=self.reward_dim,
+                                       epsilon=0.0 if weight_selection_algo == "ols" else None)
+
+        if prior_knowledge_available:
+            action_demo_1 = [2, 2, 1]  # 0.7
+            action_demo_2 = [2, 2, 3, 1, 1]  # 8.2
+            action_demo_3 = [2, 2, 3, 3, 1, 1, 1]  # 11.5
+            action_demo_4 = [2, 2, 3, 3, 3, 1, 1, 1, 1]  # 14.0
+            action_demo_5 = [2, 2, 3, 3, 3, 3, 1, 1, 1, 1]  # 15.1
+            action_demo_6 = [2, 2, 3, 3, 3, 3, 3, 1, 1, 1, 1]  # 16.1
+            action_demo_7 = [2, 2, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1]  # 19.6
+            action_demo_8 = [2, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1]  # 20.3
+            action_demo_9 = [2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 22.4
+            action_demo_10 = [2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 23.7
+            action_demos = [action_demo_1, action_demo_2, action_demo_3, action_demo_4, action_demo_5, action_demo_6,
+                            action_demo_7, action_demo_8, action_demo_9, action_demo_10]
+            linear_support.get_support_weight_from_demo(demos=action_demos, env=eval_env)
 
         weight_history = []
 
@@ -849,12 +880,14 @@ class GPIPD(MOPolicy, MOAgent):
                 for wcw in M:
                     n_value = policy_evaluation_mo(self, eval_env, wcw, rep=num_eval_episodes_for_front)[3]
                     linear_support.add_solution(n_value, wcw)
-
+            print(" ================================================== ")
             if self.log:
                 # Evaluation
                 gpi_returns_test_tasks = [
                     policy_evaluation_mo(self, eval_env, ew, rep=num_eval_episodes_for_front)[3] for ew in eval_weights
                 ]
+                # print(f"!!!expected utility:{gpi_returns_test_tasks[2]}")
+                # expected_utilities.append(gpi_returns_test_tasks[2])
                 log_all_multi_policy_metrics(
                     current_front=gpi_returns_test_tasks,
                     hv_ref_point=ref_point,
@@ -866,6 +899,7 @@ class GPIPD(MOPolicy, MOAgent):
                 mean_gpi_returns_test_tasks = np.mean(
                     [np.dot(ew, q) for ew, q in zip(eval_weights, gpi_returns_test_tasks)], axis=0
                 )
+                print(f"mean_gpi_returns_test_tasks:{mean_gpi_returns_test_tasks}")
                 wandb.log({"eval/Mean Utility - GPI": mean_gpi_returns_test_tasks, "iteration": iter})
 
             self.save(filename=f"GPI-PD {weight_selection_algo} iter={iter}", save_replay_buffer=False)
diff --git a/Algorithm/linear_support.py b/Algorithm/linear_support.py
index 7901722..ae98e82 100644
--- a/Algorithm/linear_support.py
+++ b/Algorithm/linear_support.py
@@ -8,6 +8,9 @@ import cvxpy as cp
 import numpy as np
 from cvxpy import SolverError
 from gymnasium.core import Env
+from Algorithm.common.weights import equally_spaced_weights
+from Algorithm.common.evaluation import eval_mo_demo
+import mo_gymnasium as mo_gym
 
 
 def eval_mo(
@@ -158,6 +161,21 @@ class LinearSupport:
         for w in extrema_weights(self.num_objectives):
             self.queue.append((float("inf"), w))
 
+    def get_support_weight_from_demo(self, demos, env):
+        for demo in demos:
+            _, discounted_return, _, disc_vec_return = eval_mo_demo(demo=demo,
+                                                                    env=env,
+                                                                    w=np.array([1, 0], dtype=float))
+            # return_list.append(discounted_return)
+            self.ccs.append(disc_vec_return)
+        corners = self.compute_corner_weights()
+        for w in corners:
+            self.weight_support.append(w)
+        return corners
+        # corners = sorted(corners, key=lambda x: x[0])
+        # for w in corners:
+        #     print(np.round_(w, 3))
+
     def next_weight(self, algo: str = "ols", gpi_agent=None, env=None, rep_eval=1
                     ):
         """Returns the next weight vector with highest priority.
@@ -190,7 +208,7 @@ class LinearSupport:
             if len(self.queue) > 0:
                 # Sort in descending order of priority
                 self.queue.sort(key=lambda t: t[0], reverse=True)
-                # If all priorities are 0, shuffle the queue to avoid repearting weights every iteration
+                # If all priorities are 0, shuffle the queue to avoid repeating weights every iteration
                 if self.queue[0][0] == 0.0:
                     random.shuffle(self.queue)
 
@@ -487,16 +505,16 @@ class LinearSupport:
 
 
 if __name__ == "__main__":
-    action_demo_1 = [1]  # 0.7
-    action_demo_2 = [3, 1, 1]  # 8.2
-    action_demo_3 = [3, 3, 1, 1, 1]  # 11.5
-    action_demo_4 = [3, 3, 3, 1, 1, 1, 1]  # 14.0
-    action_demo_5 = [3, 3, 3, 3, 1, 1, 1, 1]  # 15.1
-    action_demo_6 = [3, 3, 3, 3, 3, 1, 1, 1, 1]  # 16.1
-    action_demo_7 = [3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1]  # 19.6
-    action_demo_8 = [3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1]  # 20.3
-    action_demo_9 = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 22.4
-    action_demo_10 = [3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 23.7
+    action_demo_1 = [2, 2, 1]  # 0.7
+    action_demo_2 = [2, 2, 3, 1, 1]  # 8.2
+    action_demo_3 = [2, 2, 3, 3, 1, 1, 1]  # 11.5
+    action_demo_4 = [2, 2, 3, 3, 3, 1, 1, 1, 1]  # 14.0
+    action_demo_5 = [2, 2, 3, 3, 3, 3, 1, 1, 1, 1]  # 15.1
+    action_demo_6 = [2, 2, 3, 3, 3, 3, 3, 1, 1, 1, 1]  # 16.1
+    action_demo_7 = [2, 2, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1]  # 19.6
+    action_demo_8 = [2, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1]  # 20.3
+    action_demo_9 = [2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 22.4
+    action_demo_10 = [2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 23.7
 
 
     def _solve(w):
@@ -523,13 +541,17 @@ if __name__ == "__main__":
         # return np.array(list(map(float, input().split())), dtype=np.float32)
 
 
+    action_demos = [action_demo_1, action_demo_2, action_demo_3, action_demo_4, action_demo_5, action_demo_6,
+                    action_demo_7, action_demo_8, action_demo_9, action_demo_10]
+    eval_env = mo_gym.make("deep-sea-treasure-v0")
     num_objectives = 2
     ols = LinearSupport(num_objectives=num_objectives, epsilon=0.0001, verbose=True)
-    ols.train(total_timesteps=1000, timesteps_per_iteration=10)
-    sorted_vectors = sorted(ols.ccs, key=lambda x: x[1])
-    weight_support = sorted(ols.weight_support, key=lambda x: x[1])
-    for s in sorted_vectors:
-        print(f"solution:{s}")
-    for w in weight_support:
-        print(f"w:{np.round_(w, 3)}")
-    # print(ols.ccs)
+    ols.get_support_weight_from_demo(demos=action_demos, env=eval_env)
+    # ols.train(total_timesteps=1000, timesteps_per_iteration=10)
+    # sorted_vectors = sorted(ols.ccs, key=lambda x: x[1])
+    # weight_support = sorted(ols.weight_support, key=lambda x: x[1])
+    # for s in sorted_vectors:
+    #     print(f"solution:{s}")
+    # for w in weight_support:
+    #     print(f"w:{np.round_(w, 3)}")
+    # # print(ols.ccs)
diff --git a/Algorithm/rl_algorithm/PPO/__pycache__/buffer.cpython-39.pyc b/Algorithm/rl_algorithm/PPO/__pycache__/buffer.cpython-39.pyc
deleted file mode 100644
index 83b39ac..0000000
Binary files a/Algorithm/rl_algorithm/PPO/__pycache__/buffer.cpython-39.pyc and /dev/null differ
diff --git a/Algorithm/rl_algorithm/PPO/__pycache__/distributions.cpython-39.pyc b/Algorithm/rl_algorithm/PPO/__pycache__/distributions.cpython-39.pyc
deleted file mode 100644
index 50d43b1..0000000
Binary files a/Algorithm/rl_algorithm/PPO/__pycache__/distributions.cpython-39.pyc and /dev/null differ
diff --git a/Algorithm/rl_algorithm/PPO/__pycache__/ppo.cpython-39.pyc b/Algorithm/rl_algorithm/PPO/__pycache__/ppo.cpython-39.pyc
deleted file mode 100644
index 58397c6..0000000
Binary files a/Algorithm/rl_algorithm/PPO/__pycache__/ppo.cpython-39.pyc and /dev/null differ
diff --git a/Algorithm/rl_algorithm/__pycache__/D_shape_tabular_Q.cpython-39.pyc b/Algorithm/rl_algorithm/__pycache__/D_shape_tabular_Q.cpython-39.pyc
deleted file mode 100644
index 9c74864..0000000
Binary files a/Algorithm/rl_algorithm/__pycache__/D_shape_tabular_Q.cpython-39.pyc and /dev/null differ
diff --git a/Algorithm/rl_algorithm/__pycache__/D_shaped_DQN.cpython-39.pyc b/Algorithm/rl_algorithm/__pycache__/D_shaped_DQN.cpython-39.pyc
deleted file mode 100644
index 1e71f38..0000000
Binary files a/Algorithm/rl_algorithm/__pycache__/D_shaped_DQN.cpython-39.pyc and /dev/null differ
diff --git a/Algorithm/rl_algorithm/__pycache__/backward_Q_agent.cpython-311.pyc b/Algorithm/rl_algorithm/__pycache__/backward_Q_agent.cpython-311.pyc
deleted file mode 100644
index 69b059e..0000000
Binary files a/Algorithm/rl_algorithm/__pycache__/backward_Q_agent.cpython-311.pyc and /dev/null differ
diff --git a/Algorithm/rl_algorithm/__pycache__/backward_Q_agent.cpython-39.pyc b/Algorithm/rl_algorithm/__pycache__/backward_Q_agent.cpython-39.pyc
deleted file mode 100644
index 229ee3e..0000000
Binary files a/Algorithm/rl_algorithm/__pycache__/backward_Q_agent.cpython-39.pyc and /dev/null differ
diff --git a/simulators/__pycache__/abstract_simulator.cpython-311.pyc b/simulators/__pycache__/abstract_simulator.cpython-311.pyc
deleted file mode 100644
index de86957..0000000
Binary files a/simulators/__pycache__/abstract_simulator.cpython-311.pyc and /dev/null differ
diff --git a/simulators/__pycache__/abstract_simulator.cpython-39.pyc b/simulators/__pycache__/abstract_simulator.cpython-39.pyc
deleted file mode 100644
index 03ce43f..0000000
Binary files a/simulators/__pycache__/abstract_simulator.cpython-39.pyc and /dev/null differ
diff --git a/simulators/__pycache__/discrete_grid_world.cpython-39.pyc b/simulators/__pycache__/discrete_grid_world.cpython-39.pyc
deleted file mode 100644
index 4f1a760..0000000
Binary files a/simulators/__pycache__/discrete_grid_world.cpython-39.pyc and /dev/null differ
diff --git a/simulators/deep_sea_treasure/__pycache__/deep_sea_treasure.cpython-311.pyc b/simulators/deep_sea_treasure/__pycache__/deep_sea_treasure.cpython-311.pyc
index 4ff1f78..b798eef 100644
Binary files a/simulators/deep_sea_treasure/__pycache__/deep_sea_treasure.cpython-311.pyc and b/simulators/deep_sea_treasure/__pycache__/deep_sea_treasure.cpython-311.pyc differ
diff --git a/simulators/deep_sea_treasure/__pycache__/deep_sea_treasure.cpython-39.pyc b/simulators/deep_sea_treasure/__pycache__/deep_sea_treasure.cpython-39.pyc
index d472bc9..3b1d9b6 100644
Binary files a/simulators/deep_sea_treasure/__pycache__/deep_sea_treasure.cpython-39.pyc and b/simulators/deep_sea_treasure/__pycache__/deep_sea_treasure.cpython-39.pyc differ
diff --git a/simulators/deep_sea_treasure/__pycache__/preference_space.cpython-311.pyc b/simulators/deep_sea_treasure/__pycache__/preference_space.cpython-311.pyc
index 98480bc..8d583f0 100644
Binary files a/simulators/deep_sea_treasure/__pycache__/preference_space.cpython-311.pyc and b/simulators/deep_sea_treasure/__pycache__/preference_space.cpython-311.pyc differ
diff --git a/simulators/deep_sea_treasure/__pycache__/preference_space.cpython-39.pyc b/simulators/deep_sea_treasure/__pycache__/preference_space.cpython-39.pyc
index 18b5d1a..0558383 100644
Binary files a/simulators/deep_sea_treasure/__pycache__/preference_space.cpython-39.pyc and b/simulators/deep_sea_treasure/__pycache__/preference_space.cpython-39.pyc differ
diff --git a/simulators/deep_sea_treasure/deep_sea_treasure.py b/simulators/deep_sea_treasure/deep_sea_treasure.py
index b3a3efc..cc811e4 100644
--- a/simulators/deep_sea_treasure/deep_sea_treasure.py
+++ b/simulators/deep_sea_treasure/deep_sea_treasure.py
@@ -1,8 +1,7 @@
 import random
-
-import matplotlib.pyplot as plt
 import numpy as np
 from simulators.deep_sea_treasure.abstract_simulator import AbstractSimulator
+import matplotlib.pyplot as plt
 
 GAMMA = 0.99
 
@@ -313,38 +312,44 @@ class DeepSeaTreasure(AbstractSimulator):
 if __name__ == '__main__':
     dst_env = DeepSeaTreasure(visualization=True)
     dst_env.reset(put_submarine=False)
-    traj_to_10_9 = [(0, 0), (0, 1), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8),
-                    (1, 9), (2, 9), (3, 9), (4, 9), (5, 9), (6, 9), (7, 9), (8, 9), (9, 9), (10, 9)]
-
-    traj_to_9_8 = [(0, 0), (0, 1), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8),
-                   (2, 8), (3, 8), (4, 8), (5, 8), (6, 8), (7, 8), (8, 8), (9, 8)]
-
-    traj_to_7_7 = [(0, 0), (0, 1), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (2, 7),
-                   (3, 7), (4, 7), (5, 7), (6, 7), (7, 7)]
-
-    traj_to_7_6 = [(0, 0), (0, 1), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 6), (3, 6),
-                   (4, 6), (5, 6), (6, 6), (7, 6)]
-
-    traj_to_4_5 = [(0, 0), (0, 1), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (2, 5), (3, 5), (4, 5)]
-    traj_to_4_4 = [(0, 0), (0, 1), (1, 1), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]
-    traj_to_4_3 = [(0, 0), (0, 1), (1, 1), (1, 2), (1, 3), (2, 3), (3, 3), (4, 3)]
-    traj_to_3_2 = [(0, 0), (0, 1), (1, 1), (1, 2), (2, 2), (3, 2)]
-    traj_to_2_1 = [(0, 0), (0, 1), (1, 1), (2, 1)]
-    traj_to_1_0 = [(0, 0), (1, 0)]
-    trajs = [traj_to_10_9, traj_to_9_8, traj_to_7_7, traj_to_7_6, traj_to_4_5, traj_to_4_4, traj_to_4_3, traj_to_3_2,
-             traj_to_2_1, traj_to_1_0]
+    action_demo_1 = [1]  # 0.7
+    action_demo_2 = [3, 1, 1]  # 8.2
+    action_demo_3 = [3, 3, 1, 1, 1]  # 11.5
+    action_demo_4 = [3, 3, 3, 1, 1, 1, 1]  # 14.0
+    action_demo_5 = [3, 3, 3, 3, 1, 1, 1, 1]  # 15.1
+    action_demo_6 = [3, 3, 3, 3, 3, 1, 1, 1, 1]  # 16.1
+    action_demo_7 = [3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1]  # 19.6
+    action_demo_8 = [3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1]  # 20.3
+    action_demo_9 = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 22.4
+    action_demo_10 = [3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 23.7
+    action_demos = [action_demo_1, action_demo_2, action_demo_3, action_demo_4, action_demo_5, action_demo_6,
+                    action_demo_7, action_demo_8, action_demo_9, action_demo_10]
+
     treasure_w = 0.0
     sum_utility = 0
     # pref_list = [1, 0.7, 0.67, 0.66, 0.58, 0.54, 0.51, 0.47, 0.39, 0.21]
+    return_list = []
+    treasure_ws = []
+    value_vecs = []
     for i in range(101):
         # for treasure_w in pref_list:
         treasure_w = round((100 - i) / 100, 2)
         utility_list = []
-        for traj in trajs:
-            utility,_ = dst_env.calculate_utility(demo=traj[1:], pref_w=np.array([1 - treasure_w, treasure_w]))
-            utility_list.append(utility)
-        sum_utility += max(utility_list)
-        idx = np.argmax(utility_list)
-        print(f"pref:{[1 - treasure_w, treasure_w]}\tmax_utility:{max(utility_list)}\tpos:{10 - idx}")
-    print(f"avg utility:{sum_utility / 101}")
-    dst_env.state_traj_to_actions(traj_to_4_5)
+        for demo in action_demos:
+            value_scalar, value_vec = dst_env.calculate_utility_from_actions(action_demo=demo,
+                                                                             pref_w=np.array([1 - treasure_w, treasure_w]))
+            # value_vecs.append(value_vec)
+            utility_list.append(value_scalar)
+        print(f"w_vec:{[1 - treasure_w, treasure_w]}\tV*S(w):{max(utility_list)}\tdemo:{np.argmax(utility_list)}")
+    #     sum_utility += max(utility_list)
+    #     idx = np.argmax(utility_list)
+    #     print(f"pref:{[1 - treasure_w, treasure_w]}\tmax_utility:{max(utility_list)}\tpos:{10 - idx}")
+    #     return_list.append(max(utility_list))
+    #     treasure_ws.append(treasure_w)
+    # print(f"return_list:{return_list}\ttreasure_ws:{treasure_ws}")
+    # return_list = return_list[::-1]
+    # treasure_ws = treasure_ws[::-1]
+    # print(f"avg utility:{sum_utility / 101}")
+    # # dst_env.state_traj_to_actions(traj_to_4_5)
+    # plt.plot(treasure_ws, return_list, color="red")
+    # plt.show()
diff --git a/util/__pycache__/archives.cpython-39.pyc b/util/__pycache__/archives.cpython-39.pyc
index 1889210..722a094 100644
Binary files a/util/__pycache__/archives.cpython-39.pyc and b/util/__pycache__/archives.cpython-39.pyc differ
diff --git a/util/__pycache__/dataclass.cpython-39.pyc b/util/__pycache__/dataclass.cpython-39.pyc
index f714feb..6f29722 100644
Binary files a/util/__pycache__/dataclass.cpython-39.pyc and b/util/__pycache__/dataclass.cpython-39.pyc differ
diff --git a/util/__pycache__/utils.cpython-311.pyc b/util/__pycache__/utils.cpython-311.pyc
index 905e82b..ee45603 100644
Binary files a/util/__pycache__/utils.cpython-311.pyc and b/util/__pycache__/utils.cpython-311.pyc differ
diff --git a/util/__pycache__/utils.cpython-39.pyc b/util/__pycache__/utils.cpython-39.pyc
index 1c309b1..cd496cc 100644
Binary files a/util/__pycache__/utils.cpython-39.pyc and b/util/__pycache__/utils.cpython-39.pyc differ
