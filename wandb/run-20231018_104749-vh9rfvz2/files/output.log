{'env_id': 'deep-sea-treasure-v0', 'learning_rate': 0.0003, 'initial_epsilon': 0.01, 'epsilon_decay_steps:': None, 'batch_size': 128, 'per': True, 'gpi_pd': False, 'alpha_per': 0.6, 'min_priority': 0.01, 'tau': 1.0, 'num_nets': 2, 'clip_grand_norm': None, 'target_net_update_freq': 1000, 'gamma': 0.99, 'net_arch': [256, 256, 256, 256], 'dynamics_model_arch': [256, 256, 256], 'gradient_updates': 20, 'buffer_size': 1000000, 'learning_starts': 100, 'dyna': False, 'dynamics_rollout_len': 1, 'dynamics_uncertainty_threshold': 1.5, 'dynamics_rollout_starts': 5000, 'dynamics_rollout_freq': 250, 'dynamics_rollout_batch_size': 25000, 'dynamics_buffer_size': 100000, 'dynamics_normalize_inputs': False, 'dynamics_ensemble_size': 5, 'dynamics_num_elites': 2, 'real_ratio': 0.5, 'drop_rate': 0.01, 'layer_norm': True, 'seed': 42}
Training starts... Let's roll!
gpi_pd:False
CCS: [] CCS size: 0
Next weight: [1. 0.]
Next weight vector: [1. 0.]
Episode infos:
Steps: 27, Time: 0.001436999998986721
Total Reward: [  8.2 -27. ], Discounted: [  6.25121  -23.528069]
Scalarized Reward: 8.199999809265137, Discounted: 6.2512102127075195
Episode infos:
Steps: 2, Time: 0.0011470000026747584
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
Episode infos:
Steps: 18, Time: 0.0017030000453814864
Total Reward: [ 14. -18.], Discounted: [ 11.683192 -16.383137]
Scalarized Reward: -18.0, Discounted: -16.383136749267578
scalar_q_values:tensor([[-0.2199,  0.0041, -1.5572,  0.1959],
        [-0.2513, -0.2084, -1.3020, -0.0543]], device='cuda:0')	action:3	policy_index:0
scalar_q_values:tensor([[ 0.1501,  0.2298, -1.1640,  0.4849],
        [-0.1320, -0.2168, -1.0886, -0.1318]], device='cuda:0')	action:3	policy_index:0
scalar_q_values:tensor([[ 0.7107,  0.3306, -1.0169,  0.2094],
        [ 0.1228, -0.1889, -0.6998, -0.0642]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[ 0.8427,  0.6535, -0.6398,  0.4353],
        [ 0.4389,  0.3078, -0.2199, -0.1440]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[ 0.4606,  0.8411, -0.3768,  0.6623],
        [ 0.0227,  0.0081, -0.0456, -0.0638]], device='cuda:0')	action:1	policy_index:0
scalar_q_values:tensor([[ 0.7921,  0.6262, -0.1396,  0.5009],
        [ 0.0304,  0.2241, -0.0862,  0.0444]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[ 0.6594,  0.4407, -0.3518,  0.5571],
        [-0.2069,  0.1748, -0.0337, -0.0439]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[ 0.4499,  0.7768,  0.2902,  0.4311],
        [-0.2155,  0.0582,  0.1027, -0.2058]], device='cuda:0')	action:1	policy_index:0
scalar_q_values:tensor([[ 4.8259e-01,  9.3563e-01,  5.5741e-01,  3.0163e-01],
        [-4.5626e-01,  7.8083e-02,  2.5739e-04,  1.1829e-01]], device='cuda:0')	action:1	policy_index:0
scalar_q_values:tensor([[ 0.5776,  0.5783,  0.4730,  0.1812],
        [-0.3745,  0.1255, -0.2356, -0.0943]], device='cuda:0')	action:1	policy_index:0
scalar_q_values:tensor([[ 0.6154,  0.9184,  0.4165,  0.3302],
        [-0.3057,  0.4863, -0.2200, -0.3190]], device='cuda:0')	action:1	policy_index:0
scalar_q_values:tensor([[ 0.8542,  0.8730,  0.3571,  0.0821],
        [-0.3088,  0.5693, -0.5368, -0.1285]], device='cuda:0')	action:1	policy_index:0
scalar_q_values:tensor([[ 0.9290,  1.0382,  0.3740,  0.3189],
        [ 0.0117,  0.8343, -0.8385, -0.2000]], device='cuda:0')	action:1	policy_index:0
scalar_q_values:tensor([[ 0.9243,  1.3313, -0.0294,  0.4751],
        [ 0.0779,  1.0138, -0.8090, -0.0766]], device='cuda:0')	action:1	policy_index:0
scalar_q_values:tensor([[ 1.1302,  1.4981, -0.2513,  0.6844],
        [ 0.1641,  1.2896, -0.6756, -0.3375]], device='cuda:0')	action:1	policy_index:0
scalar_q_values:tensor([[ 0.8292,  1.6486, -0.0852,  0.5232],
        [ 0.1780,  1.3838, -0.6221, -0.5099]], device='cuda:0')	action:1	policy_index:0
scalar_q_values:tensor([[ 0.8901,  1.8179, -0.1089,  0.4764],
        [ 0.2760,  1.3266, -0.6946, -0.3880]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 69, Time: 0.563372015953064
Total Reward: [ 23.7 -69. ], Discounted: [ 11.846138 -49.51613 ]
Scalarized Reward: 23.700000762939453, Discounted: 11.846138000488281
scalar_q_values:tensor([[-0.1882,  0.5081, -0.0084, -0.0096],
        [-0.0455,  0.8051,  0.0686,  0.0666]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002830999903380871
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0025,  0.8727,  0.0954, -0.1341],
        [ 0.0706,  0.8073,  0.0287, -0.1157]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002736999886110425
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0903, -1.1339, -0.3850, -1.4265],
        [-1.0147, -1.1824, -0.3842, -1.4636]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0713, -1.3857, -0.5290, -1.4689],
        [-1.0836, -1.3705, -0.5769, -1.3947]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0581, -1.4575, -0.7505, -1.4387],
        [-0.8748, -1.5388, -0.5716, -1.5815]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0622, -1.5405, -1.1048, -1.4513],
        [-1.0768, -1.5151, -1.0811, -1.4275]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9419, -1.5244, -1.3686, -1.4235],
        [-0.9509, -1.5147, -1.3461, -1.4274]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0228, -1.2916, -1.7045, -1.3571],
        [-1.0244, -1.4302, -1.5115, -1.4736]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.8867, -1.0740, -1.6299, -1.3825],
        [-1.0203, -1.2610, -1.5608, -1.2423]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.1528, -0.7365, -1.7822, -1.4573],
        [-1.0896, -1.0117, -1.8151, -1.5085]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 8, Time: 0.04341199994087219
Total Reward: [ 0.7 -8. ], Discounted: [ 0.6459213 -7.6482754]
Scalarized Reward: -8.0, Discounted: -7.648275375366211
scalar_q_values:tensor([[-1.1983, -1.0300, -1.6965, -1.3400],
        [-1.1661, -0.9750, -1.7169, -1.3654]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002259999979287386
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0661,  0.5952,  0.0826,  0.2581],
        [ 0.0418,  0.6442, -0.0396,  0.1831]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022189998999238014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0659,  0.6599, -0.1455,  0.2875],
        [-0.0007,  0.6330, -0.2532,  0.3125]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002609000075608492
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0499,  0.8473, -0.0655,  0.3959],
        [-0.0787,  0.8291, -0.0255,  0.2155]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023819999769330025
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.1079,  1.0463, -0.1188,  0.2036],
        [-0.0877,  0.8775,  0.1030,  0.1945]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022849999368190765
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.1277, -1.0391, -1.6219, -1.4473],
        [-1.2548, -1.1232, -1.3626, -1.3381]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023360000923275948
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0220,  0.7263,  0.1041,  0.0144],
        [-0.0448,  0.7665,  0.0158,  0.1416]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023129999171942472
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0052, 0.6607, 0.1666, 0.0681],
        [0.0830, 0.6414, 0.0253, 0.0880]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022249999456107616
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0795,  0.5616, -0.0588,  0.1091],
        [ 0.0134,  0.6451,  0.1094,  0.0339]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022070000413805246
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0195, -1.0233, -1.0889, -1.5443],
        [-1.0166, -0.6803, -1.1788, -1.6536]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002257999964058399
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.8652, -1.1530, -1.2359, -1.5197],
        [-0.9545, -1.0417, -1.0883, -1.4445]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9761, -1.2223, -1.1653, -1.4245],
        [-0.8544, -1.1644, -1.1515, -1.3348]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9937, -1.1414, -1.0497, -1.4090],
        [-0.7683, -1.0804, -1.0155, -1.3617]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0750, -1.0616, -1.0153, -1.3945],
        [-1.0301, -1.1154, -1.1060, -1.4233]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.1217, -1.0858, -1.0683, -1.4401],
        [-1.1715, -1.0282, -1.0187, -1.3557]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0159, -0.9917, -1.0839, -1.5155],
        [-0.9894, -0.9551, -0.9283, -1.4021]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.1369, -0.9637, -0.9661, -1.4666],
        [-1.1966, -0.9807, -0.9426, -1.4965]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.2487, -0.9574, -0.8826, -1.4002],
        [-1.2401, -0.9670, -0.8127, -1.4117]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.2037, -0.9214, -0.9173, -1.3974],
        [-1.2212, -0.9424, -0.9076, -1.3438]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.1638, -0.9771, -0.9511, -1.3277],
        [-1.1243, -0.9955, -0.9212, -1.3500]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9358, -0.9956, -0.9492, -1.3821],
        [-1.2089, -0.9250, -1.0862, -1.2304]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 11, Time: 0.0556350015103817
Total Reward: [  0.7 -11. ], Discounted: [  0.62673676 -10.361512  ]
Scalarized Reward: -11.0, Discounted: -10.361512184143066
scalar_q_values:tensor([[ 7.6337e-02,  7.4190e-01, -6.9388e-02,  2.0588e-01],
        [ 1.3010e-01,  6.6201e-01, -4.3862e-05,  2.1424e-01]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002374999923631549
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0817,  0.7923, -0.0166,  0.3315],
        [ 0.0101,  0.8378,  0.0752,  0.2425]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002265000017359853
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0171,  0.8968, -0.0422,  0.3587],
        [ 0.1251,  0.8861,  0.0780,  0.2975]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022690000478178263
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0452,  0.8584,  0.0034,  0.2948],
        [ 0.0060,  0.8469, -0.0853,  0.1703]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.006233000196516514
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-0.0126,  0.8402, -0.0455,  0.2370],
        [-0.0200,  0.8011, -0.0732,  0.1630]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021669999696314335
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0051, -0.9666, -0.9083, -1.4498],
        [-0.9521, -0.9470, -0.9916, -1.3800]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.1363, -1.0403, -0.8119, -1.0870],
        [-0.9126, -1.0578, -0.8738, -1.4358]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0476, -1.0171, -0.9056, -1.3851],
        [-1.0218, -0.9706, -0.8224, -1.3855]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.1289, -1.0770, -0.9798, -1.4058],
        [-1.1796, -0.9542, -0.8616, -1.1762]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0891, -0.9672, -1.0300, -1.3161],
        [-1.0136, -1.0066, -1.0753, -1.3223]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.023458000272512436
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-1.0074, -0.9033, -1.1049, -1.2725],
        [-0.9959, -0.9107, -1.0528, -1.5577]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00228899996727705
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0478,  0.7758, -0.0989,  0.2527],
        [ 0.0466,  0.7740, -0.0965,  0.0937]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021800000686198473
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0513,  0.8277, -0.1229,  0.2446],
        [ 0.0894,  0.7138, -0.0466,  0.1805]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002266000024974346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0832,  0.5052, -0.0606,  0.2813],
        [ 0.1584,  0.6775, -0.1442,  0.0391]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021619999315589666
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.7581, -0.8457, -1.1579, -1.3637],
        [-0.9338, -1.0777, -1.0234, -1.3595]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9330, -0.9933, -1.0128, -1.4045],
        [-0.9359, -1.0739, -1.0353, -1.5527]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0716, -0.9678, -0.9288, -1.3572],
        [-1.0388, -0.9399, -1.0350, -1.4588]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0710, -0.9163, -0.9890, -1.4440],
        [-1.1647, -0.9081, -1.1887, -1.4155]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.020104000344872475
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.1803, -0.9309, -0.9719, -1.4072],
        [-1.1222, -1.0132, -0.9881, -1.4372]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022690000478178263
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0676,  0.7643, -0.0383,  0.1652],
        [ 0.0227,  0.7670, -0.0327,  0.1700]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00215999991632998
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0085,  0.7042,  0.0433,  0.1472],
        [-0.0043,  0.6074,  0.0458,  0.1059]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022360000293701887
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0914, -1.0273, -1.0540, -1.3902],
        [-0.8942, -0.6307, -0.9314, -1.3801]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002107999986037612
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0387, -0.9917, -1.0059, -1.3062],
        [-1.0162, -0.9878, -0.9687, -1.3427]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9899, -0.9226, -0.9531, -1.3816],
        [-1.0709, -0.9572, -1.0311, -1.3340]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007400999777019024
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0397,  0.7760,  0.0746,  0.3336],
        [-0.0895,  0.6151, -0.0667,  0.3206]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021609999239444733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-6.6368e-02,  5.9675e-01, -8.7939e-02,  2.5989e-01],
        [-5.9655e-04,  7.0861e-01,  2.9437e-02, -1.4476e-02]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002051000017672777
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0047,  0.7766, -0.1235,  0.3667],
        [-0.0256,  0.6329,  0.0686,  0.1960]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020759999752044678
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.1087,  0.7119, -0.1555,  0.3662],
        [-0.0497,  0.6951, -0.0394,  0.2283]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022899999748915434
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9982, -1.0000, -1.0532, -1.3158],
        [-0.7955, -0.9852, -1.0523, -1.2961]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0685, -0.9342, -0.9953, -1.3936],
        [-0.9236, -1.0436, -0.9909, -1.2711]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0304, -0.9558, -0.9744, -1.3143],
        [-1.0228, -1.0689, -0.9703, -1.4753]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01254200004041195
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.9284, -0.8979, -0.9257, -1.3046],
        [-0.9596, -1.0059, -0.9976, -1.2179]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002213000087067485
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0083,  0.6304,  0.0273,  0.2691],
        [-0.0936,  0.7301,  0.1269,  0.1200]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020810000132769346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0909, -1.0406, -0.8986, -1.1853],
        [-0.9942, -0.9354, -0.9442, -1.2016]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.1211, -0.9578, -1.0067, -1.1799],
        [-0.9172, -0.9813, -0.9748, -1.2518]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0089, -0.9621, -0.9542, -1.2107],
        [-0.9836, -0.9105, -0.9780, -1.3432]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012258999980986118
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.9706, -0.9922, -0.9939, -1.3101],
        [-0.9542, -0.9352, -0.9214, -1.3056]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.1216, -1.0184, -1.0426, -1.3010],
        [-1.0068, -1.0162, -1.0370, -1.3771]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0323, -1.0098, -1.0623, -1.3984],
        [-1.0233, -1.0026, -1.0388, -1.3928]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012445000000298023
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-1.0376, -1.0077, -1.0786, -1.5209],
        [-1.0460, -0.8927, -1.1169, -1.5910]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0092,  0.7428, -0.0324,  0.3156],
        [ 0.0286,  0.7441,  0.0044,  0.2939]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021669999696314335
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9928, -0.9496, -1.1958, -1.3825],
        [-1.1218, -1.0719, -1.0472, -1.4783]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021089999936521053
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0264,  0.7892, -0.0049,  0.3088],
        [ 0.0317,  0.6923,  0.1826,  0.3287]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002044999971985817
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0854, -0.8715, -1.1170, -1.4646],
        [-0.8994, -1.0266, -1.1529, -1.4602]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021120000164955854
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0089,  0.5968,  0.0865,  0.3165],
        [ 0.0499,  0.5877, -0.0188,  0.4169]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002111000008881092
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0337,  0.6412,  0.0032,  0.2374],
        [-0.0438,  0.4682,  0.1117,  0.3811]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020989999175071716
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9541, -0.8910, -0.8586, -1.3787],
        [-0.8928, -0.9034, -0.9942, -1.4211]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9108, -1.0340, -0.9858, -1.3173],
        [-0.8972, -1.0554, -0.8762, -1.4703]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.8689, -1.0573, -0.9503, -1.4674],
        [-0.9677, -1.0436, -0.9657, -1.4551]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0032, -1.0837, -0.9903, -1.4491],
        [-0.9849, -1.0448, -1.0990, -1.3519]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.1167, -0.9265, -1.0643, -1.4335],
        [-1.0805, -0.9093, -1.2267, -1.4936]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 5, Time: 0.023414000868797302
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-9.5037e-02,  7.3405e-01,  5.8580e-02,  2.5656e-01],
        [-3.8175e-04,  6.8263e-01,  2.0504e-02,  2.3321e-01]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022640000097453594
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0908,  0.7500, -0.0628,  0.2464],
        [ 0.0974,  0.7198,  0.0184,  0.0692]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022100000642240047
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0810,  0.8471, -0.0067,  0.2482],
        [ 0.0304,  0.8560, -0.0339,  0.2358]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021909999195486307
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.1566, 0.8034, 0.0100, 0.2086],
        [0.0498, 0.7445, 0.0105, 0.1934]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023169999476522207
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9680, -0.9891, -0.9712, -1.4020],
        [-0.9995, -0.9669, -0.9692, -1.4913]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021909999195486307
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9107, -1.0279, -0.8135, -1.4767],
        [-0.9456, -0.8995, -0.8675, -1.5606]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9984, -1.0624, -0.9630, -1.4121],
        [-0.7288, -1.0872, -1.0467, -1.3353]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0267, -1.0841, -0.9330, -1.4452],
        [-0.9962, -0.9905, -0.9555, -1.4808]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0209, -0.9476, -1.0013, -1.3688],
        [-1.0168, -0.8907, -1.0001, -1.4567]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.018154000863432884
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.0678,  0.7229, -0.0592,  0.1288],
        [-0.1909,  0.7015, -0.0108,  0.2332]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021649999544024467
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9419, -0.9943, -0.9526, -1.3563],
        [-1.0632, -0.9942, -1.0632, -1.4301]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9702, -1.0884, -0.9781, -1.4041],
        [-0.9835, -1.0436, -0.9988, -1.4626]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.8658, -1.0648, -0.8058, -1.3054],
        [-0.9184, -1.0107, -0.9471, -1.4595]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9496, -1.0677, -0.9602, -1.3018],
        [-0.9313, -1.0355, -0.9634, -1.4154]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0789, -0.9723, -0.9686, -1.2713],
        [-1.0118, -0.9311, -0.9021, -1.3731]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.1418, -0.9939, -1.0131, -1.3764],
        [-0.9869, -0.9216, -1.0594, -1.3385]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 6, Time: 0.028686000034213066
Total Reward: [ 0.7 -6. ], Discounted: [ 0.6590361 -5.793465 ]
Scalarized Reward: -6.0, Discounted: -5.7934651374816895
scalar_q_values:tensor([[-1.1438, -0.9138, -1.0378, -1.3837],
        [-1.0291, -0.9293, -0.9413, -1.4412]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021810000762343407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0717, -1.0171, -0.9040, -1.2984],
        [-1.0206, -0.9791, -1.0074, -1.4076]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0154, -1.0213, -1.0275, -1.3691],
        [-0.9484, -0.8910, -0.9853, -1.6243]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007350000087171793
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0285,  0.8020, -0.0881,  0.2682],
        [-0.0167,  0.7178, -0.0109,  0.2054]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002094999887049198
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0496, -0.9216, -1.2235, -1.4121],
        [-1.0242, -0.9756, -1.1183, -1.3546]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022799998987466097
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9750, -0.9405, -1.1331, -1.3667],
        [-0.9078, -0.9170, -0.8923, -1.3673]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0965, -0.8590, -0.9339, -1.2909],
        [-0.9007, -0.9662, -1.0625, -1.2618]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007627999875694513
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0436,  0.6399, -0.0282,  0.4293],
        [ 0.0785,  0.6637, -0.0641,  0.2910]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002185000106692314
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0579,  0.7722, -0.0045,  0.2806],
        [-0.0253,  0.6852,  0.0037,  0.2763]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022070000413805246
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0328, -0.9094, -1.0708, -1.3304],
        [-0.9973, -1.0156, -0.9276, -1.4104]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0373,  0.7743,  0.0542,  0.1355],
        [-0.0686,  0.7329,  0.0926,  0.2128]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002254999941214919
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0255,  0.6472,  0.0879,  0.2820],
        [-0.0213,  0.8333,  0.0571,  0.2820]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002090000081807375
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0207,  0.6742,  0.0325,  0.2560],
        [-0.0358,  0.6732,  0.0305,  0.1631]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021269998978823423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0568, -1.0546, -0.9824, -1.2768],
        [-1.0204, -1.0723, -0.9027, -1.3791]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9580, -0.9481, -0.9486, -1.3075],
        [-1.0287, -0.9530, -0.9905, -1.4002]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007780000101774931
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0646,  0.6958, -0.0338,  0.1892],
        [ 0.0209,  0.6946,  0.0031,  0.2388]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002319999970495701
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0505,  0.7195, -0.0270,  0.2427],
        [ 0.0669,  0.7346,  0.0904,  0.1028]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021609999239444733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0738,  0.8415, -0.0294,  0.1985],
        [ 0.1209,  0.7078,  0.1779,  0.1218]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023529999889433384
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0580, -1.0444, -1.1190, -1.3496],
        [-0.9085, -0.8837, -0.9742, -1.4147]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0024939998984336853
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.1694,  0.6575,  0.2045,  0.0800],
        [-0.0163,  0.7461,  0.0875,  0.1261]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022140000946819782
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0288, -1.0633, -0.9429, -1.5291],
        [-1.0695, -0.9869, -0.9804, -1.3762]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0683, -1.1129, -0.9567, -1.5009],
        [-0.9172, -1.0150, -0.9553, -1.4421]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0132, -1.0076, -0.9365, -1.4867],
        [-1.0659, -0.9016, -1.0687, -1.4504]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01374099962413311
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[ 0.0013,  0.6832, -0.0636,  0.1201],
        [-0.0126,  0.6407, -0.0865,  0.0987]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022249999456107616
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 1.5689e-04,  5.7025e-01,  2.3813e-02,  1.4088e-01],
        [ 1.5027e-02,  6.8509e-01, -4.9658e-02,  3.6336e-02]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002088000066578388
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0446,  0.6985, -0.0700,  0.0721],
        [ 0.1285,  0.6444, -0.0407, -0.0975]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020910000894218683
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.8775, -1.1022, -1.1702, -1.4800],
        [-0.8748, -1.0076, -1.0734, -1.5504]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0312, -1.0411, -1.0072, -1.5152],
        [-1.0500, -0.9935, -1.0607, -1.5302]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007385999895632267
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0122,  0.6455,  0.1821,  0.1800],
        [-0.0126,  0.6486,  0.0177, -0.0170]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020649998914450407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0250, -1.0093, -1.0356, -1.5458],
        [-0.9154, -0.9071, -0.9599, -1.6594]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0417,  0.6826,  0.0417,  0.1763],
        [-0.0168,  0.7184, -0.0109,  0.0092]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020550000481307507
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0359, -0.9711, -0.9326, -1.3654],
        [-1.0464, -0.9571, -0.8066, -1.4740]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0583, -0.8848, -0.9986, -1.3834],
        [-1.1664, -1.0119, -0.9351, -1.4303]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007472999859601259
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-1.0284, -0.9818, -1.0848, -1.3131],
        [-0.9907, -1.0172, -1.0089, -1.3740]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020580000709742308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0156,  0.7633, -0.0331,  0.1221],
        [ 0.0375,  0.8095,  0.0805,  0.2492]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020280000753700733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9296, -0.9699, -1.0508, -1.3813],
        [-0.7864, -1.1155, -1.0240, -1.3951]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9870, -1.0080, -0.9983, -1.2792],
        [-0.9157, -0.9318, -1.0097, -1.3479]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0549, -0.9940, -0.9639, -1.3467],
        [-0.9768, -0.9729, -1.0631, -1.3627]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9528, -0.9121, -1.1871, -1.4764],
        [-1.0831, -0.9824, -0.9884, -1.3668]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.01765199936926365
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.0875, -1.0359, -0.8741, -1.3368],
        [-1.1004, -1.0358, -0.8807, -1.4291]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.1217, -1.0597, -0.9615, -1.3716],
        [-1.0372, -1.0484, -0.7522, -1.2751]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9637, -1.0195, -1.0198, -1.3156],
        [-0.9806, -1.1003, -0.9892, -1.3751]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9963, -0.9841, -1.0171, -1.2637],
        [-0.9445, -1.0026, -0.9615, -1.3265]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9432, -0.9925, -1.0529, -1.3082],
        [-0.9081, -0.9244, -1.0081, -1.3233]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0337, -0.9902, -1.0812, -1.2849],
        [-0.8850, -0.9403, -0.9603, -1.4821]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0327, -1.0600, -0.9888, -1.3490],
        [-0.9539, -1.0521, -0.9607, -1.4543]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0161, -0.9952, -1.0380, -1.3604],
        [-0.9583, -0.9603, -0.9789, -1.3415]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9609, -1.0753, -0.9446, -1.3356],
        [-1.0207, -0.9873, -0.9475, -1.4143]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9401, -1.0468, -1.0164, -1.2437],
        [-0.9193, -1.0339, -0.9587, -1.3100]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0793, -1.0129, -1.1192, -1.3600],
        [-0.9878, -0.9841, -0.9801, -1.4183]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9290, -0.8458, -0.9549, -1.4057],
        [-0.9681, -1.0115, -0.9427, -1.3365]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 12, Time: 0.05937600135803223
Total Reward: [  0.7 -12. ], Discounted: [  0.6204694 -11.247897 ]
Scalarized Reward: -12.0, Discounted: -11.247897148132324
scalar_q_values:tensor([[-1.0689, -0.9627, -0.9443, -1.3574],
        [-0.9634, -0.9972, -0.9480, -1.3973]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9832, -0.8707, -0.9760, -1.4501],
        [-0.9821, -0.9837, -0.9733, -1.3925]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007801999803632498
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-1.0779, -1.0434, -1.0525, -1.3764],
        [-1.0470, -0.9785, -1.0199, -1.4106]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0029509998857975006
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.1235, -0.9821, -1.1179, -1.2905],
        [-1.0594, -1.0223, -1.0229, -1.4355]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0026370000559836626
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0373,  0.7246,  0.0040,  0.3933],
        [-0.0081,  0.7330,  0.0212,  0.2975]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002225999953225255
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0079,  0.6463, -0.0674,  0.4364],
        [ 0.0475,  0.6378, -0.0120,  0.3166]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021299999207258224
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.1014,  0.7083, -0.0008,  0.4873],
        [-0.0259,  0.6571,  0.0241,  0.3349]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002128999913111329
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9304, -1.0273, -0.9587, -1.2795],
        [-0.7445, -0.9080, -0.9484, -1.5497]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9327, -0.9833, -0.9622, -1.2906],
        [-1.0135, -0.9579, -0.8202, -1.3053]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.8604, -0.9058, -0.9934, -1.2995],
        [-0.9015, -1.0279, -0.9954, -1.2759]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9766, -1.0261, -1.1534, -1.2991],
        [-1.1074, -0.9418, -1.1447, -1.3395]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.018130000680685043
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.0256, -0.9752, -0.9654, -1.2765],
        [-1.0585, -1.0149, -1.0166, -1.3720]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0628, -1.0972, -1.1144, -1.3301],
        [-1.0448, -1.0127, -1.0119, -1.3362]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0351, -0.9982, -0.9618, -1.3051],
        [-1.1228, -0.9365, -0.9395, -1.3479]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012574000284075737
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-1.0607, -1.0069, -0.9852, -1.3369],
        [-1.0296, -1.0162, -0.9868, -1.3335]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.1510, -0.9891, -1.0106, -1.3104],
        [-0.9684, -0.9082, -1.0086, -1.3070]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007311000023037195
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0484,  0.7480, -0.2023,  0.4050],
        [-0.0836,  0.7709, -0.0148,  0.3649]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0024419999681413174
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.8720, -1.0072, -0.8724, -1.3777],
        [-0.9795, -1.0790, -0.8712, -1.4372]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0499, -1.0282, -0.8829, -1.2918],
        [-0.9551, -1.1043, -1.0147, -1.3572]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9647, -1.0586, -1.0544, -1.2710],
        [-0.8505, -0.9679, -1.0837, -1.2201]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0576, -0.9389, -1.0344, -1.3084],
        [-1.0363, -1.0236, -0.9780, -1.2741]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.017896000295877457
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.0069, -0.9330, -1.0719, -1.3177],
        [-1.0197, -0.9734, -1.0718, -1.3437]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002090000081807375
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0377, -0.9655, -1.0318, -1.3060],
        [-1.0301, -0.8562, -0.9325, -1.2548]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022770001087337732
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9108, -0.9248, -0.8565, -1.2937],
        [-0.9510, -1.0217, -0.9692, -1.3597]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9296, -1.0289, -1.1055, -1.3558],
        [-1.0127, -1.0422, -0.8620, -1.3013]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9677, -1.0327, -0.9489, -1.3164],
        [-1.0168, -1.0037, -0.9967, -1.3345]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9340, -0.9773, -1.1025, -1.3322],
        [-1.0102, -0.9695, -1.0499, -1.3582]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9928, -0.9893, -1.1090, -1.3196],
        [-1.0382, -0.9636, -1.0020, -1.3016]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 5, Time: 0.02293200045824051
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-1.0385, -0.9949, -1.0406, -1.3747],
        [-0.9143, -0.9930, -1.0431, -1.5450]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9668, -0.9785, -1.0631, -1.3182],
        [-0.8325, -0.9205, -1.0076, -1.3663]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9873, -1.0138, -1.0518, -1.4744],
        [-1.0238, -1.0611, -0.9823, -1.2570]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0105, -1.0202, -0.9721, -1.4144],
        [-1.0000, -1.1041, -0.9876, -1.4344]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0401, -0.9363, -1.0623, -1.4221],
        [-1.0747, -1.0136, -0.9994, -1.4681]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.023517999798059464
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-0.0187,  0.7253,  0.0008,  0.2822],
        [-0.0038,  0.7754,  0.1162,  0.2359]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021909999195486307
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0007, -1.0911, -1.0163, -1.2784],
        [-1.1199, -0.9903, -1.0504, -1.3990]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0468,  0.6206,  0.0100,  0.3051],
        [-0.0751,  0.6914, -0.0336,  0.1098]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0533,  0.6670,  0.0061,  0.3212],
        [ 0.0063,  0.6420, -0.0334,  0.2783]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002242000075057149
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0345,  0.7323,  0.0064,  0.2335],
        [-0.0775,  0.7847,  0.1006,  0.1713]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002937000012025237
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0017,  0.6922, -0.0205,  0.3186],
        [-0.1391,  0.6216,  0.0216,  0.1633]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002213000087067485
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9876, -0.9516, -0.9249, -1.3867],
        [-0.9891, -0.9703, -1.0038, -1.4407]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0023, -0.8341, -0.9703, -1.3551],
        [-0.9774, -0.9336, -0.9988, -1.4436]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007765999995172024
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.0239,  0.7154,  0.0901,  0.2163],
        [ 0.0022,  0.6631, -0.1040,  0.1740]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002145000034943223
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0527,  0.7716,  0.0563,  0.2808],
        [-0.0113,  0.5357, -0.0894,  0.2117]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002234000014141202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0445,  0.7399, -0.0778,  0.2633],
        [-0.0514,  0.6535, -0.0235,  0.1614]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002133999951183796
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0200,  0.7015, -0.0272,  0.2270],
        [-0.0237,  0.7247, -0.0613,  0.1820]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022040000185370445
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0425,  0.7330,  0.0222,  0.2318],
        [ 0.0023,  0.6484, -0.0504,  0.2006]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9524, -1.0066, -1.0456, -1.5039],
        [-0.9954, -0.9024, -1.0378, -1.4444]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021569998934865
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0074, -1.1220, -0.9970, -1.5585],
        [-0.9946, -0.9374, -1.0242, -1.4476]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022269999608397484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9650, -1.0095, -1.0379, -1.4825],
        [-1.0060, -1.0259, -0.9864, -1.4641]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0353, -0.9694, -0.9125, -1.4944],
        [-1.0020, -1.0595, -1.0393, -1.4680]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9502, -0.9853, -1.0812, -1.4472],
        [-1.0074, -0.9235, -1.0055, -1.3956]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01257999986410141
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-1.0230, -1.0965, -0.9934, -1.4654],
        [-0.9589, -1.0637, -1.0414, -1.4307]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0334, -1.1107, -1.0812, -1.3831],
        [-0.9864, -1.0144, -0.9668, -1.4330]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0238, -1.0357, -0.9490, -1.3485],
        [-0.9475, -0.9960, -0.9724, -1.3574]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0044, -0.9615, -0.9943, -1.4018],
        [-0.8949, -1.0157, -0.9260, -1.3513]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.8981, -0.8663, -0.9827, -1.2682],
        [-0.9897, -1.0033, -1.0242, -1.3915]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.023724999278783798
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-0.9808, -1.0635, -0.9502, -1.3837],
        [-0.9383, -1.0493, -0.9785, -1.4221]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0341, -1.0069, -0.9832, -1.3862],
        [-0.9778, -1.1318, -0.9841, -1.3143]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0178, -0.9610, -1.0771, -1.4279],
        [-0.9634, -1.0116, -0.9781, -1.4184]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012888000346720219
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.0037,  0.6652, -0.0836,  0.3503],
        [-0.1263,  0.6288, -0.0376,  0.1906]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0412, -0.9880, -0.9043, -1.3867],
        [-1.0234, -1.0016, -1.0401, -1.4112]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0524, -1.0700, -1.0471, -1.3975],
        [-0.9788, -1.0892, -1.0618, -1.2731]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0035, -1.0337, -0.9361, -1.3510],
        [-1.0027, -1.0324, -1.0201, -1.3479]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9370, -0.9310, -1.0467, -1.3845],
        [-0.9549, -1.0072, -1.2040, -1.4575]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.018688000738620758
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.0096, -0.9242, -1.0086, -1.4125],
        [-0.9834, -0.9438, -1.0256, -1.4268]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002319999970495701
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0147,  0.6827, -0.0797,  0.2921],
        [ 0.0532,  0.7333,  0.0433,  0.2838]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002283999929204583
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-3.3938e-02,  6.7048e-01, -2.7346e-02,  3.1098e-01],
        [-3.6848e-04,  7.5315e-01, -3.9710e-02,  2.5066e-01]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9769, -0.9868, -0.8949, -1.3335],
        [-1.0140, -1.0635, -0.9910, -1.3930]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0264, -0.9955, -1.0559, -1.3474],
        [-0.9364, -1.0493, -1.0453, -1.3920]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0114, -1.0201, -1.0239, -1.3489],
        [-0.9387, -1.0188, -1.0565, -1.3680]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9649, -0.9938, -1.0467, -1.2959],
        [-0.9949, -1.0173, -1.0256, -1.3545]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0054, -0.9716, -1.0303, -1.4756],
        [-1.0598, -1.0186, -1.0758, -1.3365]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.02322399988770485
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[ 0.0043,  0.7371, -0.0852,  0.3442],
        [ 0.1080,  0.6449,  0.0454,  0.3263]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002435999922454357
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0372,  0.6645, -0.0081,  0.2865],
        [ 0.0689,  0.6170, -0.0098,  0.3335]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002372999908402562
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9817, -1.0259, -1.0065, -1.3764],
        [-1.0033, -0.9445, -0.9622, -1.4211]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021869998890906572
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0229,  0.7157,  0.0171,  0.3257],
        [-0.0288,  0.8387,  0.0635,  0.2306]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021909999195486307
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9356, -1.0109, -0.9852, -1.3091],
        [-0.9692, -0.9988, -0.9634, -1.3300]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9769, -0.9801, -0.9742, -1.3110],
        [-0.9018, -0.9565, -0.9588, -1.2956]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0159, -0.9848, -1.0037, -1.4197],
        [-0.8423, -1.0352, -0.9997, -1.3944]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0180, -1.0813, -1.0039, -1.3789],
        [-0.9935, -0.9936, -1.0720, -1.4442]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0165, -0.9810, -1.0085, -1.2944],
        [-1.1014, -1.0406, -0.9774, -1.4351]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0567, -1.0735, -0.9525, -1.4238],
        [-1.0885, -1.0400, -1.0219, -1.4676]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9937, -0.9818, -1.0382, -1.4174],
        [-1.0189, -0.9963, -1.0246, -1.4168]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 7, Time: 0.033883001655340195
Total Reward: [ 0.7 -7. ], Discounted: [ 0.65244573 -6.7255306 ]
Scalarized Reward: -7.0, Discounted: -6.725530624389648
scalar_q_values:tensor([[-0.0614,  0.6939,  0.0299,  0.3168],
        [-0.0240,  0.6698,  0.0330,  0.2756]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021460000425577164
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9402, -0.9321, -1.0059, -1.1730],
        [-1.0045, -0.9594, -0.9349, -1.4312]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022060000337660313
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9874, -1.0383, -1.0187, -1.4340],
        [-0.9822, -1.0195, -1.0513, -1.3712]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9717, -1.0251, -1.0392, -1.3432],
        [-0.9459, -0.9922, -1.0228, -1.3924]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9589, -1.0386, -1.0991, -1.2962],
        [-0.9872, -0.9124, -0.8687, -1.3915]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0947, -1.0635, -0.9498, -1.3967],
        [-1.0191, -1.0035, -1.0044, -1.4660]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9267, -1.0433, -1.0328, -1.3841],
        [-1.0591, -0.8681, -0.9543, -1.3166]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 5, Time: 0.023788999766111374
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[0.1224, 0.7056, 0.0629, 0.2697],
        [0.0923, 0.6635, 0.0194, 0.2152]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021009999327361584
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0326,  0.6261,  0.0219,  0.2703],
        [-0.0322,  0.6801,  0.0771,  0.1701]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021810000762343407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9407, -0.9077, -1.0245, -1.4924],
        [-1.0015, -0.9315, -1.0026, -1.3804]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002124000107869506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0082,  0.7635,  0.0399,  0.2008],
        [-0.0592,  0.7149, -0.0518,  0.2016]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021329999435693026
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9762, -0.8323, -1.0065, -1.5533],
        [-1.0364, -1.0133, -0.9490, -1.4599]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9970, -0.9871, -0.9269, -1.4275],
        [-1.0435, -1.0200, -0.9771, -1.3251]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9363, -0.9924, -1.0352, -1.4694],
        [-0.9941, -0.8691, -1.0256, -1.4663]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007472999859601259
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0202,  0.6746,  0.0261,  0.2389],
        [ 0.0585,  0.7215, -0.0125,  0.2010]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021639999467879534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0638, -1.0039, -1.0038, -1.3440],
        [-0.8860, -0.9782, -1.0026, -1.4382]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9137, -1.0767, -1.0335, -1.2782],
        [-0.9809, -1.0179, -0.9882, -1.4051]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0114, -1.0615, -1.0022, -1.2046],
        [-1.0521, -0.9878, -0.9851, -1.3920]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0232, -1.0505, -0.9047, -1.4748],
        [-0.9788, -1.0444, -0.9889, -1.3639]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9428, -0.9286, -0.9758, -1.3666],
        [-0.9362, -1.0929, -1.0032, -1.3642]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.023169999942183495
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-0.9889, -0.9768, -0.9808, -1.3401],
        [-0.9480, -0.9161, -0.9947, -1.3722]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002211000071838498
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0028, -1.0981, -0.9961, -1.3197],
        [-1.0880, -1.0319, -1.0445, -1.4123]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0497, -1.0503, -1.0516, -1.3275],
        [-0.9621, -1.0648, -1.0539, -1.3018]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0339, -1.0951, -0.9606, -1.3572],
        [-1.0203, -1.0347, -0.9891, -1.4056]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9902, -0.9637, -1.0111, -1.3335],
        [-1.0150, -0.9089, -0.9791, -1.4154]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.01816299930214882
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.9936, -0.9567, -0.9838, -1.3174],
        [-1.0706, -0.9651, -0.9685, -1.4787]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024190000258386135
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.8520, -0.8622, -0.9388, -1.5800],
        [-0.9798, -0.9963, -0.9657, -1.4251]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9159, -1.0161, -0.9936, -1.3654],
        [-0.9921, -1.0343, -0.9608, -1.3798]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9813, -1.0622, -1.0016, -1.3403],
        [-1.0216, -1.0386, -1.0064, -1.4223]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0245, -1.0206, -1.0366, -1.3989],
        [-1.0247, -0.9869, -1.0398, -1.4547]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.018635999411344528
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.0058,  0.7121,  0.1436,  0.2177],
        [-0.0045,  0.8023,  0.0279,  0.1632]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002283999929204583
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0346,  0.6546,  0.0037,  0.2878],
        [ 0.0318,  0.6836,  0.0037,  0.1700]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021569998934865
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0589, 0.7047, 0.0866, 0.1546],
        [0.0493, 0.7635, 0.0602, 0.3939]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020669999066740274
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0220,  0.6916, -0.0478,  0.2085],
        [-0.0334,  0.6265,  0.0599,  0.2914]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002211000071838498
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9856, -1.0147, -1.0370, -1.3516],
        [-1.0074, -1.0000, -0.9996, -1.3974]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9806, -0.9752, -0.9975, -1.3580],
        [-0.9870, -1.0121, -1.0367, -1.4198]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007519000209867954
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0400,  0.7559,  0.0217,  0.2631],
        [-0.0339,  0.6585,  0.0693,  0.2855]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00227600010111928
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9695, -0.9663, -0.9954, -1.4164],
        [-1.0036, -1.0025, -1.0973, -1.3812]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002107999986037612
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9838, -0.9593, -0.9661, -1.4235],
        [-1.0532, -1.0351, -1.0328, -1.3917]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002093000104650855
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0369,  0.6389,  0.0496,  0.2036],
        [ 0.0835,  0.7044, -0.0889,  0.1395]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002088000066578388
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0557,  0.7212, -0.0567,  0.2911],
        [ 0.0901,  0.6467, -0.1749,  0.0976]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020749999675899744
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0487, -0.9407, -1.0778, -1.4274],
        [-1.0114, -1.0061, -1.0280, -1.3957]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021869998890906572
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0299, -1.0259, -0.9986, -1.4420],
        [-1.0320, -1.0065, -1.0193, -1.4646]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0364, -1.0745, -1.0629, -1.4904],
        [-0.9322, -1.0689, -1.0987, -1.2788]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9871, -0.9672, -1.0255, -1.5302],
        [-1.0060, -0.9825, -1.0338, -1.4785]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013442999683320522
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[ 0.0543,  0.6613,  0.0138,  0.2632],
        [-0.0174,  0.6439, -0.0063,  0.1667]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002377999946475029
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0054, -0.9385, -0.9859, -1.4094],
        [-1.0028, -0.9847, -0.9899, -1.4717]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022980000358074903
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0435, 0.7895, 0.0629, 0.2950],
        [0.0800, 0.7739, 0.0664, 0.1808]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0527, -1.0245, -0.9467, -1.3975],
        [-1.0670, -1.0013, -0.9882, -1.4795]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0319, -0.9919, -0.8341, -1.4302],
        [-1.0833, -0.9937, -1.0017, -1.4685]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0946, -0.9769, -0.9949, -1.3910],
        [-1.0229, -0.9772, -0.9623, -1.4208]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9773, -0.9104, -1.0671, -1.3935],
        [-0.9335, -0.9451, -1.0325, -1.4134]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.018928999081254005
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.0091, -1.0311, -1.0117, -1.3414],
        [-0.8846, -1.0423, -1.0117, -1.3427]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9768, -1.0013, -0.9900, -1.3777],
        [-0.9523, -1.0388, -0.9190, -1.3587]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9765, -0.9865, -1.0182, -1.3770],
        [-0.9839, -0.9929, -1.0030, -1.3524]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9367, -0.8862, -0.9571, -1.3302],
        [-1.0464, -1.0928, -1.0054, -1.3512]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.017703000456094742
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[ 0.0958,  0.7447, -0.0045,  0.2140],
        [ 0.0068,  0.8000, -0.0237,  0.2720]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0108, -1.0037, -0.9990, -1.3557],
        [-0.8781, -1.0497, -1.0768, -1.3621]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9055, -1.0400, -1.0414, -1.3757],
        [-0.9679, -1.0009, -0.9238, -1.3345]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9366, -0.9250, -0.9855, -1.3150],
        [-1.0466, -1.0159, -1.0329, -1.3790]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01232299953699112
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.8797, -0.8789, -0.9783, -1.4855],
        [-0.9887, -1.0086, -1.0510, -1.3424]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024530000519007444
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0234,  0.7707, -0.0023,  0.3226],
        [ 0.0057,  0.7572,  0.0909,  0.2594]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024109999649226665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0362,  0.8000, -0.0136,  0.2963],
        [-0.0098,  0.7306, -0.0949,  0.2779]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002148000057786703
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0370, -1.1125, -1.0133, -1.3316],
        [-1.0330, -0.9859, -1.0142, -1.4223]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020600000862032175
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0353, -1.0188, -1.0233, -1.2987],
        [-1.0307, -0.9842, -1.0262, -1.4214]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022410000674426556
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9906, -1.0289, -0.9978, -1.3928],
        [-0.9738, -1.0334, -1.0095, -1.5054]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9691, -1.0580, -1.0321, -1.3589],
        [-1.0399, -0.9964, -1.0137, -1.3843]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9858, -1.0429, -1.0543, -1.3444],
        [-1.0890, -0.9940, -1.1219, -1.4532]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.1056, -1.1846, -1.0603, -1.2891],
        [-1.0039, -0.9740, -0.9789, -1.5332]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.0182499997317791
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[ 0.0026,  0.6682, -0.0154,  0.2608],
        [-0.0772,  0.6687, -0.0187,  0.2385]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022360000293701887
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0763, -0.9963, -1.0391, -1.4405],
        [-0.9707, -0.9675, -0.9543, -1.4119]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0210, -0.9909, -1.0129, -1.3980],
        [-0.9440, -1.0024, -0.9541, -1.4142]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0259, -0.9971, -1.0220, -1.4272],
        [-1.0220, -0.9947, -1.0110, -1.4474]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012527000159025192
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[ 0.0051,  0.7157, -0.0284,  0.2299],
        [ 0.0360,  0.6542, -0.0580,  0.2086]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021780000533908606
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9375, -0.9539, -0.9814, -1.4273],
        [-1.0602, -1.0157, -1.0180, -1.3574]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0169, -1.0250, -1.0536, -1.3826],
        [-0.9730, -0.9756, -1.1086, -1.3777]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0391, -0.9728, -1.0732, -1.4218],
        [-0.8449, -1.0474, -0.9320, -1.4842]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0653, -1.0051, -1.0474, -1.4406],
        [-0.9968, -1.0057, -1.0383, -1.4633]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0103, -0.9763, -1.0095, -1.4226],
        [-1.0022, -0.9600, -0.9778, -1.4246]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 5, Time: 0.023228000849485397
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-0.0254,  0.7360, -0.0659,  0.3179],
        [-0.1745,  0.5710,  0.1886,  0.3472]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002131999935954809
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9660, -1.0225, -1.0434, -1.3743],
        [-0.9706, -1.0719, -1.0511, -1.4042]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9762, -0.9034, -0.9590, -1.4427],
        [-0.9934, -1.0258, -0.9860, -1.3787]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007400000002235174
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.0091,  0.6561, -0.0097,  0.3568],
        [ 0.0065,  0.7104, -0.0612,  0.2310]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022060000337660313
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0060, -1.0225, -1.0142, -1.3871],
        [-0.8792, -1.0801, -1.0391, -1.3800]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9592, -0.9451, -0.9752, -1.3272],
        [-1.0319, -0.9146, -0.9469, -1.4013]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007300000172108412
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0393,  0.6426, -0.0285,  0.2975],
        [ 0.0436,  0.7360, -0.0076,  0.2915]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002041999949142337
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0785,  0.7751, -0.0395,  0.2885],
        [-0.0316,  0.7388, -0.0072,  0.3314]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002070999937132001
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0216,  0.6191,  0.0104,  0.2372],
        [-0.0582,  0.6642, -0.0290,  0.2608]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002094999887049198
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0065,  0.7226, -0.0127,  0.2998],
        [ 0.0606,  0.6634,  0.1150,  0.3448]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024129999801516533
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0106, -1.0186, -0.9935, -1.3541],
        [-1.0867, -1.0401, -1.0277, -1.3760]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0566, -0.9708, -1.0504, -1.3483],
        [-1.0699, -0.9638, -1.0211, -1.4457]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.009568000212311745
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.1204,  0.6743,  0.0871,  0.2386],
        [-0.0930,  0.7363, -0.0190,  0.1833]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0026660000439733267
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0188,  0.7293, -0.0523,  0.2887],
        [ 0.0400,  0.7568,  0.0122,  0.1735]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002732000080868602
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0696,  0.6994, -0.0882,  0.2351],
        [-0.0043,  0.7170,  0.0940,  0.2488]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002830999903380871
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9634, -1.0241, -0.9512, -1.4047],
        [-1.0262, -1.0286, -0.9672, -1.3998]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9737, -1.0280, -0.9932, -1.3813],
        [-0.9272, -1.0369, -0.9294, -1.3899]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9662, -1.0346, -0.9899, -1.3382],
        [-0.9703, -0.9577, -0.9577, -1.4790]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9627, -1.0321, -0.9952, -1.3727],
        [-0.9641, -1.0276, -1.0499, -1.4179]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9919, -0.9443, -0.9786, -1.4688],
        [-1.0306, -0.9476, -1.0132, -1.4428]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.02877900004386902
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[0.0040, 0.6803, 0.0009, 0.2387],
        [0.0293, 0.6564, 0.0214, 0.1609]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002592999953776598
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0548,  0.7198, -0.0138,  0.2087],
        [ 0.0452,  0.6993,  0.0308,  0.2272]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002830999903380871
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0599, -0.9312, -0.9098, -1.4225],
        [-1.0045, -1.0286, -0.9561, -1.3648]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.8804, -0.9984, -0.9658, -1.3816],
        [-1.0963, -0.9750, -1.0506, -1.4513]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9691, -0.9714, -0.9537, -1.3502],
        [-1.0024, -0.9814, -0.9671, -1.3647]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9945, -0.9981, -1.0578, -1.3706],
        [-0.8709, -1.0216, -0.9341, -1.2003]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0104, -1.0501, -1.0392, -1.2699],
        [-0.9982, -0.9877, -1.0016, -1.3217]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 5, Time: 0.02926900051534176
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-1.0154, -1.0099, -1.0479, -1.4219],
        [-1.0187, -1.0223, -0.9716, -1.3669]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0507, -0.9924, -1.0371, -1.4132],
        [-0.9467, -0.8907, -1.0384, -1.4235]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.009545999579131603
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.9620, -0.8776, -0.9675, -1.3610],
        [-1.0549, -0.9758, -0.9586, -1.3662]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025710000190883875
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.4117e-02,  7.4763e-01,  9.5326e-03,  3.0846e-01],
        [ 6.2944e-04,  6.9601e-01, -2.1894e-02,  2.7243e-01]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002698000054806471
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9970, -1.0514, -0.9223, -1.3660],
        [-0.9500, -1.0901, -0.9973, -1.4683]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9897, -1.0507, -0.9946, -1.4453],
        [-0.9678, -0.9789, -1.0119, -1.3725]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0282, -0.9962, -1.0566, -1.3590],
        [-1.0039, -1.0188, -1.0100, -1.4516]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.016265999525785446
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.9645, -0.9494, -1.0592, -1.4067],
        [-0.9156, -0.9589, -1.0367, -1.4114]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9864, -1.0040, -1.0622, -1.3907],
        [-0.9189, -0.8676, -1.0148, -1.6034]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.009386000223457813
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0078,  0.7993,  0.0358,  0.3144],
        [-0.0050,  0.7472, -0.0017,  0.2707]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0028230000752955675
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0200,  0.7727, -0.0287,  0.3066],
        [ 0.0081,  0.7414, -0.0129,  0.2517]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0026390000712126493
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0264,  0.6371, -0.0372,  0.3388],
        [ 0.0464,  0.6508, -0.0334,  0.3128]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002662000013515353
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0205, -1.0257, -0.9687, -1.3520],
        [-1.0307, -1.0427, -0.9858, -1.4039]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0335, -0.9920, -1.0646, -1.4203],
        [-0.9942, -0.9970, -1.0343, -1.4047]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.009432000108063221
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.0021,  0.6970,  0.0532,  0.3125],
        [ 0.0259,  0.7625,  0.1041,  0.2194]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002807999961078167
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9952, -1.0258, -1.0598, -1.3943],
        [-1.0459, -0.9420, -1.0640, -1.2920]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0028379999566823244
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-3.2850e-02,  7.5949e-01,  9.2223e-02,  2.6170e-01],
        [ 4.6809e-02,  7.3044e-01, -5.1439e-04,  3.2114e-01]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025869999080896378
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0270,  0.5831, -0.0185,  0.3139],
        [ 0.0363,  0.6834,  0.0077,  0.2394]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00305499997921288
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 2.6187e-03,  5.2560e-01, -9.1863e-02,  3.3243e-01],
        [ 6.1462e-04,  6.5736e-01, -6.6997e-02,  3.1608e-01]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002761000068858266
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9206, -0.9925, -0.9944, -1.4933],
        [-1.0101, -0.9680, -0.9788, -1.4512]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0089, -1.0411, -0.9136, -1.3694],
        [-1.0661, -1.0528, -0.8662, -1.3764]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0034, -0.9701, -0.9024, -1.4314],
        [-0.9361, -0.9587, -0.9499, -1.4351]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9660, -0.8835, -1.0076, -1.3478],
        [-1.0021, -0.9994, -0.9838, -1.4345]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.022606000304222107
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.0020, -0.9338, -0.9764, -1.3189],
        [-0.9216, -0.9982, -1.0087, -1.4536]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9930, -0.9621, -1.0176, -1.4829],
        [-1.0147, -0.9321, -0.9482, -1.3931]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.009775999933481216
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.9871, -1.0240, -1.0041, -1.3675],
        [-0.8327, -0.9451, -0.9637, -1.6021]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9711, -1.0209, -0.9858, -1.3592],
        [-0.9729, -1.1001, -1.0402, -1.3308]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9867, -1.0593, -1.0518, -1.3339],
        [-0.9989, -1.0268, -0.9684, -1.4066]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9324, -1.0325, -0.9848, -1.3791],
        [-0.9375, -1.0397, -1.0402, -1.3472]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9969, -0.9807, -1.0204, -1.3674],
        [-1.0049, -0.9872, -0.9643, -1.3312]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0129, -0.9647, -1.0139, -1.3745],
        [-1.0504, -0.9435, -0.9848, -1.3880]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 6, Time: 0.03366899862885475
Total Reward: [ 0.7 -6. ], Discounted: [ 0.6590361 -5.793465 ]
Scalarized Reward: -6.0, Discounted: -5.7934651374816895
scalar_q_values:tensor([[ 0.0111,  0.7158, -0.0027,  0.2708],
        [ 0.0210,  0.6886, -0.0532,  0.2382]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022070000413805246
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0750, -1.0382, -1.0054, -1.3653],
        [-1.0533, -1.0256, -0.9745, -1.3625]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9922, -1.0408, -0.9966, -1.3538],
        [-0.9790, -1.0156, -0.9961, -1.3299]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0012, -1.0465, -0.9933, -1.3791],
        [-1.0087, -1.0286, -1.0102, -1.3903]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0033, -1.0510, -1.0399, -1.3641],
        [-1.0331, -0.9664, -1.0382, -1.3214]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.01891399919986725
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.0392, -0.9497, -1.0130, -1.3635],
        [-1.0366, -0.9373, -1.0054, -1.3232]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022249999456107616
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0383, -1.0215, -1.0296, -1.3380],
        [-1.0171, -0.9841, -0.9857, -1.3759]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0025420000310987234
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0347,  0.6871,  0.0756,  0.2882],
        [ 0.0114,  0.5989, -0.0297,  0.2442]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002116000046953559
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0087,  0.7163, -0.0348,  0.3070],
        [-0.1303,  0.7329,  0.0905,  0.3569]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020389999262988567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0226, -0.9816, -1.0081, -1.3862],
        [-1.0106, -0.9982, -0.9647, -1.4364]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0051, -1.0328, -0.9914, -1.3925],
        [-1.0269, -0.9681, -1.0028, -1.4301]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007884000428020954
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.9896, -0.9291, -1.0164, -1.3202],
        [-1.0795, -0.9579, -1.0222, -1.4851]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023900000378489494
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0106, -0.9985, -1.0464, -1.4169],
        [-1.0071, -0.9794, -1.0262, -1.4270]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002471999963745475
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0328,  0.6066, -0.0357,  0.3047],
        [ 0.0375,  0.7484,  0.0514,  0.2536]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0289,  0.6319, -0.0354,  0.2994],
        [ 0.0114,  0.6955, -0.0445,  0.2129]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002002999885007739
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0385,  0.7064, -0.0898,  0.2557],
        [-0.0258,  0.6929, -0.0499,  0.2401]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002908000024035573
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0368, -0.9881, -0.9903, -1.4125],
        [-0.9218, -0.9113, -0.9850, -1.3211]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002191999927163124
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0548,  0.7069, -0.0798,  0.3331],
        [ 0.0089,  0.7253,  0.0177,  0.2795]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0025649999734014273
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0482, -1.0089, -1.0209, -1.3857],
        [-1.0289, -1.0298, -1.0074, -1.4234]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9932, -1.0356, -0.9955, -1.3667],
        [-1.0299, -1.0347, -1.0176, -1.3539]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0184, -1.0059, -0.9988, -1.3286],
        [-0.9608, -0.9585, -1.0005, -1.3518]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01421399973332882
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.9637, -0.9651, -1.0471, -1.3708],
        [-0.9703, -1.0148, -1.0558, -1.3856]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0119, -0.9836, -0.9525, -1.2938],
        [-0.9976, -0.9356, -0.9967, -1.3769]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007592999842017889
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.9795, -0.9734, -1.0035, -1.2600],
        [-0.9931, -1.0375, -1.0962, -1.2904]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0590, -1.0003, -0.9904, -1.3554],
        [-0.9890, -0.9861, -0.9625, -1.3863]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0507, -1.0378, -0.9383, -1.2567],
        [-1.0455, -1.0294, -0.9887, -1.3864]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0782, -1.0078, -0.9996, -1.3938],
        [-1.0510, -0.9960, -1.0414, -1.2932]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012416999787092209
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-1.0130, -1.0333, -0.9843, -1.3466],
        [-1.0051, -0.9807, -1.0558, -1.3174]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021879998967051506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0217,  0.6793,  0.0449,  0.3266],
        [ 0.0039,  0.6467,  0.0335,  0.2694]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00216599996201694
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9418, -1.0487, -1.0709, -1.4345],
        [-0.9749, -1.0182, -1.0304, -1.3208]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9538, -1.0077, -0.9434, -1.3350],
        [-1.0058, -0.9720, -1.0337, -1.3722]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0037, -0.9158, -1.0978, -1.3583],
        [-0.9999, -1.0214, -1.0094, -1.3377]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012312999926507473
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.0748,  0.5435,  0.0568,  0.4296],
        [-0.0204,  0.6816, -0.0388,  0.3696]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002196999965235591
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0094,  0.8268,  0.0152,  0.3755],
        [-0.0225,  0.9750, -0.0128,  0.2521]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020669999066740274
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0192,  0.7437,  0.0472,  0.2832],
        [-0.0295,  0.6880, -0.0048,  0.3184]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
Episode infos:
Steps: 1, Time: 0.0008730000117793679
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.1135,  0.7128,  0.0322,  0.3249],
        [ 0.0426,  0.7268,  0.0272,  0.3644]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002061000093817711
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0055,  0.6782, -0.0283,  0.3774],
        [ 0.0308,  0.7234,  0.1002,  0.3100]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0024870000779628754
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0131, 0.7004, 0.0519, 0.2819],
        [0.0246, 0.6455, 0.0231, 0.3009]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020920000970363617
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0186,  0.7126, -0.0224,  0.2861],
        [ 0.0068,  0.7182,  0.0099,  0.2674]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002053000032901764
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0017, -1.0493, -0.9935, -1.4433],
        [-0.9952, -0.9969, -0.9863, -1.4783]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9367, -1.0425, -1.0079, -1.4491],
        [-0.9846, -0.9635, -0.9747, -1.5337]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9450, -1.0051, -0.9928, -1.4373],
        [-0.9609, -0.9794, -1.0160, -1.5199]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9569, -0.9582, -0.9416, -1.4772],
        [-1.0356, -0.9653, -1.0343, -1.4649]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0689, -0.9585, -0.9971, -1.4600],
        [-1.0233, -1.0242, -0.9977, -1.4857]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.023310000076889992
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-1.0483, -1.0346, -1.0169, -1.4069],
        [-1.0538, -1.0123, -1.0352, -1.4894]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002194999950006604
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0665,  0.5830, -0.0142,  0.3152],
        [ 0.0627,  0.7077,  0.0352,  0.3195]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002266000024974346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0259, -0.9529, -1.0385, -1.4026],
        [-1.0329, -0.9613, -0.9821, -1.4531]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020800000056624413
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9824, -1.0123, -1.0172, -1.4079],
        [-0.9412, -1.0148, -1.0246, -1.4214]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9623, -1.0141, -0.9283, -1.4469],
        [-0.9820, -1.0590, -0.9305, -1.4205]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0608, -1.0925, -1.0313, -1.4440],
        [-0.9900, -1.0335, -0.9677, -1.4508]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9822, -1.1037, -1.0570, -1.4274],
        [-0.9692, -1.0139, -1.0204, -1.4194]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0716, -1.0086, -0.9795, -1.4311],
        [-1.0663, -0.9455, -0.9971, -1.4987]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 5, Time: 0.02386399917304516
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-0.9790, -1.0463, -0.9993, -1.4669],
        [-0.9959, -1.0165, -1.0096, -1.4404]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0432, -0.9615, -0.9833, -1.4124],
        [-0.9865, -1.0210, -1.0261, -1.4931]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007706999778747559
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.9664, -0.9842, -0.9844, -1.4551],
        [-0.9908, -1.0094, -0.9998, -1.4377]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0157, -0.9830, -0.9854, -1.4485],
        [-0.9971, -0.9968, -0.9897, -1.4761]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007385000120848417
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-1.0566, -0.9686, -1.0630, -1.4950],
        [-1.0157, -0.9858, -1.0013, -1.4864]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002136999974027276
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0414,  0.8043, -0.0282,  0.1821],
        [ 0.0106,  0.6183,  0.0502,  0.2100]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021909999195486307
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0267, -0.9949, -1.0369, -1.4249],
        [-1.0459, -0.9550, -0.9981, -1.4256]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002133999951183796
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0261,  0.8187, -0.1334,  0.3156],
        [ 0.0424,  0.7480,  0.0678,  0.2415]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00215000007301569
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0516,  0.6330, -0.0161,  0.2916],
        [-0.0293,  0.6946,  0.0377,  0.2439]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021279999054968357
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[6.6532e-02, 7.9059e-01, 3.8602e-02, 3.5209e-01],
        [4.2564e-02, 6.8580e-01, 7.8474e-04, 2.3027e-01]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002234000014141202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9488, -0.8840, -0.9752, -1.5231],
        [-0.9423, -0.9882, -0.9645, -1.4722]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021329999435693026
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9706, -0.9834, -1.0128, -1.3634],
        [-0.9761, -0.9808, -1.0145, -1.4270]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9610, -1.0407, -1.0295, -1.4443],
        [-0.9628, -0.8987, -0.9604, -1.4427]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.00735800014808774
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0349,  0.7394, -0.0485,  0.2981],
        [-0.0627,  0.7364, -0.0440,  0.2558]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020860000513494015
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0144,  0.7268,  0.0723,  0.3192],
        [-0.0011,  0.7055,  0.0174,  0.2327]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021180000621825457
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0166,  0.8754,  0.0306,  0.2118],
        [-0.0094,  0.6928, -0.0137,  0.2065]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020989999175071716
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0220,  0.6892,  0.0093,  0.3013],
        [-0.0035,  0.7109,  0.0231,  0.2506]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002090000081807375
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9602, -0.9967, -0.9704, -1.3759],
        [-1.0568, -1.0104, -0.9715, -1.3771]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0117, -1.0144, -0.9688, -1.3808],
        [-1.0258, -1.0286, -1.0060, -1.4489]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9893, -0.9900, -0.9705, -1.4065],
        [-0.9871, -0.9951, -0.9893, -1.4415]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0106, -0.9860, -0.9839, -1.4029],
        [-1.0026, -0.9867, -0.9890, -1.4410]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0153, -0.9573, -0.9989, -1.3675],
        [-1.0323, -1.0307, -1.0067, -1.4293]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.02252800017595291
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-0.0247,  0.7811, -0.0377,  0.2003],
        [ 0.0236,  0.6630, -0.0365,  0.2493]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002125999890267849
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0823, -1.0455, -1.0703, -1.4541],
        [-0.9869, -1.0217, -1.0307, -1.4555]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9533, -0.9673, -1.0176, -1.4236],
        [-1.0510, -0.9917, -1.0103, -1.4560]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9490, -1.0194, -0.9536, -1.3748],
        [-1.0686, -1.0738, -1.0137, -1.3405]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9637, -1.0220, -1.0077, -1.4597],
        [-0.9924, -0.9907, -1.0255, -1.4566]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9786, -1.0172, -1.0102, -1.2941],
        [-0.9808, -0.9953, -0.9640, -1.4194]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9978, -1.0379, -1.0134, -1.3662],
        [-1.0297, -1.0164, -0.9865, -1.4454]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0294, -0.9918, -1.0170, -1.3899],
        [-1.0077, -1.0040, -0.9539, -1.4130]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9981, -0.9699, -1.0550, -1.3650],
        [-1.0128, -0.9446, -0.9890, -1.4463]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 8, Time: 0.03954799845814705
Total Reward: [ 0.7 -8. ], Discounted: [ 0.6459213 -7.6482754]
Scalarized Reward: -8.0, Discounted: -7.648275375366211
scalar_q_values:tensor([[-0.0188,  0.7550,  0.0507,  0.2721],
        [ 0.0026,  0.6134, -0.0131,  0.2167]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021279999054968357
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0559,  0.7146, -0.0305,  0.1776],
        [-0.0496,  0.7870,  0.0921,  0.1885]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022879999596625566
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0321,  0.8217, -0.0465,  0.2277],
        [-0.0424,  0.6818, -0.0545,  0.1997]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023189999628812075
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0166,  0.7558, -0.0433,  0.1628],
        [-0.0189,  0.6550,  0.0068,  0.1543]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021089999936521053
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0218,  0.6789,  0.0339,  0.1904],
        [-0.0926,  0.7862,  0.0057,  0.1591]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0255, -0.9633, -1.0397, -1.4412],
        [-1.1013, -1.0666, -1.0261, -1.4334]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021720000077039003
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0044,  0.6192, -0.0241,  0.0778],
        [-0.0437,  0.6724,  0.0072,  0.1001]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002065999899059534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0036,  0.6366,  0.0035,  0.1478],
        [-0.0160,  0.6834,  0.0118,  0.1758]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022380000445991755
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9989, -1.0072, -1.0368, -1.4052],
        [-1.0289, -0.9764, -1.0577, -1.4505]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9849, -0.9928, -1.0962, -1.4427],
        [-0.9715, -1.0131, -0.9717, -1.5111]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0238, -0.9650, -0.9640, -1.4081],
        [-1.0287, -1.0219, -0.9676, -1.4719]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9972, -0.9697, -1.0262, -1.4745],
        [-1.0174, -1.0022, -0.9910, -1.4407]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01258699968457222
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.9622, -1.0054, -1.0023, -1.4403],
        [-1.0202, -1.0217, -0.9490, -1.4598]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0085, -0.9728, -1.0487, -1.3993],
        [-1.0329, -1.0765, -0.9645, -1.4766]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9889, -1.0507, -1.0369, -1.3829],
        [-0.9947, -1.0103, -1.0430, -1.4747]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9031, -0.9749, -1.1103, -1.3286],
        [-1.0052, -1.0452, -1.0039, -1.4814]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9718, -0.9590, -1.0006, -1.4426],
        [-0.9868, -0.9495, -1.0615, -1.4368]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 5, Time: 0.023250000551342964
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-0.9933, -0.9524, -0.9733, -1.4233],
        [-0.9671, -1.0130, -1.0333, -1.4305]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0026,  0.6238, -0.0065,  0.2826],
        [-0.0359,  0.7310, -0.0013,  0.1121]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00215000007301569
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0022,  0.6952,  0.0021,  0.2465],
        [-0.0052,  0.6421, -0.0346,  0.1672]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00215300009585917
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0039,  0.6917,  0.0234,  0.1968],
        [-0.0126,  0.7008,  0.0066,  0.1264]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002056000055745244
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0564,  0.7412,  0.0739,  0.2217],
        [-0.0197,  0.6812,  0.0055,  0.1329]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002409999957308173
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9965, -1.0339, -1.0055, -1.3925],
        [-1.0255, -0.9637, -0.9654, -1.4364]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0025569999124854803
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0447,  0.6579, -0.0482,  0.1172],
        [ 0.0196,  0.6679, -0.0242,  0.1245]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002211000071838498
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0451, -0.9819, -0.9884, -1.4135],
        [-0.9812, -1.0263, -0.9829, -1.4221]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9670, -0.9924, -1.0233, -1.4413],
        [-0.9960, -0.9772, -1.0312, -1.3999]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0248, -0.9595, -1.0449, -1.4886],
        [-0.9870, -0.9525, -1.0163, -1.4369]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012474000453948975
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.9611, -0.9475, -1.0473, -1.3612],
        [-0.9976, -1.0392, -0.9548, -1.4975]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002242000075057149
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0454,  0.7095,  0.0235,  0.2219],
        [ 0.0732,  0.6578, -0.1075,  0.1848]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0521, -0.9958, -0.9116, -1.3800],
        [-1.0107, -1.0396, -0.9987, -1.3587]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0856, -1.0048, -0.9901, -1.3749],
        [-1.0756, -0.9644, -0.9967, -1.4367]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.009219000115990639
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0061,  0.7459, -0.0178,  0.2266],
        [-0.1103,  0.9139,  0.0035,  0.3205]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021639999467879534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0126,  0.6855,  0.0041,  0.1876],
        [ 0.0323,  0.6921, -0.0049,  0.1951]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021100000012665987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9455, -1.0554, -0.9334, -1.3427],
        [-0.9946, -1.0412, -0.9782, -1.4320]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9801, -0.9721, -0.9976, -1.3164],
        [-1.0195, -0.9995, -1.0090, -1.3570]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007466999813914299
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-1.0488, -0.9739, -0.9690, -1.4297],
        [-0.9613, -0.9943, -0.9932, -1.4161]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9718, -0.9872, -0.9565, -1.3565],
        [-1.0060, -1.0019, -1.0034, -1.4127]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0044, -0.9653, -1.0171, -1.3980],
        [-1.0293, -0.9872, -0.9643, -1.3987]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0197, -0.9605, -1.0267, -1.3539],
        [-1.0473, -0.9879, -0.9847, -1.4163]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.018476000055670738
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.0229,  0.6570, -0.0311,  0.2430],
        [-0.0063,  0.7001, -0.0999,  0.2348]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021490000654011965
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0219,  0.6832, -0.0634,  0.2709],
        [ 0.0176,  0.7332, -0.0588,  0.1634]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021569998934865
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0688, -0.9694, -0.9747, -1.3742],
        [-0.9909, -0.9747, -1.0191, -1.4287]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021909999195486307
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0051, -1.0171, -0.9886, -1.4358],
        [-1.0523, -1.0203, -0.9929, -1.4172]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9134, -0.9828, -0.9674, -1.4023],
        [-0.9890, -0.9999, -0.9925, -1.4337]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9545, -1.0456, -0.9549, -1.3802],
        [-0.9924, -1.0010, -0.9727, -1.4959]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0205, -1.0456, -0.9530, -1.4703],
        [-0.9695, -1.0060, -1.0780, -1.4341]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0150, -0.9995, -1.0229, -1.4164],
        [-1.0537, -1.0147, -1.0069, -1.4392]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.02367500029504299
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-0.0114,  0.7218,  0.0162,  0.3458],
        [-0.0084,  0.7113, -0.0259,  0.2197]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9982, -1.0164, -1.0115, -1.4186],
        [-0.9726, -0.9964, -1.0241, -1.4073]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0176, -0.9969, -1.0136, -1.3992],
        [-0.9884, -1.0690, -1.0268, -1.3849]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0353, -1.0117, -0.9479, -1.4006],
        [-0.9948, -0.9893, -1.0332, -1.4346]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0168, -1.0392, -0.9574, -1.3762],
        [-1.0167, -1.0246, -0.9770, -1.4152]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9842, -0.9915, -1.0228, -1.3242],
        [-1.0121, -0.9909, -0.9918, -1.3943]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0456, -0.9848, -0.9475, -1.3729],
        [-0.9458, -0.9757, -1.0366, -1.3463]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0161, -1.0060, -1.0117, -1.4330],
        [-0.9972, -0.9984, -0.9973, -1.4017]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0119, -0.9806, -0.9981, -1.3836],
        [-0.9657, -1.0000, -1.0248, -1.3941]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9937, -1.0032, -1.0339, -1.3977],
        [-0.9960, -0.9983, -1.0582, -1.4131]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0072, -1.0028, -0.9556, -1.3511],
        [-1.0313, -1.0068, -0.9296, -1.3757]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9357, -0.9991, -1.0020, -1.4005],
        [-1.0410, -1.0330, -0.9991, -1.4416]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9844, -0.9501, -1.0179, -1.4116],
        [-1.0130, -1.0074, -1.0302, -1.4375]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 12, Time: 0.05890199914574623
Total Reward: [  0.7 -12. ], Discounted: [  0.6204694 -11.247897 ]
Scalarized Reward: -12.0, Discounted: -11.247897148132324
scalar_q_values:tensor([[-1.0093, -1.0139, -0.9350, -1.3906],
        [-1.0183, -0.9948, -0.9858, -1.4356]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0371, -0.9998, -0.9973, -1.3845],
        [-0.9718, -1.0278, -1.0093, -1.4048]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0239, -0.9910, -1.0220, -1.3497],
        [-1.0417, -0.9766, -0.9782, -1.4216]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013558000326156616
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.8975, -0.9556, -0.9696, -1.4248],
        [-0.9443, -1.0231, -1.0316, -1.3883]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0245, -1.0207, -0.9878, -1.3203],
        [-0.9895, -0.9967, -1.0271, -1.4645]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0135, -1.0121, -0.9662, -1.4403],
        [-1.0301, -1.0203, -0.9717, -1.3827]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9807, -1.0167, -1.0084, -1.4295],
        [-0.9900, -1.0033, -1.0006, -1.4072]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9805, -1.0391, -1.0339, -1.3455],
        [-1.1033, -1.0452, -1.0055, -1.4407]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0305, -0.9869, -0.9898, -1.4088],
        [-0.9956, -0.9873, -0.9827, -1.3995]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0022, -1.0400, -0.9843, -1.3213],
        [-0.9788, -1.0162, -0.9601, -1.3968]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0125, -0.9927, -0.9828, -1.2718],
        [-0.9836, -0.9883, -0.9879, -1.3447]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0098, -0.9924, -0.9629, -1.3166],
        [-0.9903, -1.0063, -0.9559, -1.3991]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9881, -1.0041, -1.0142, -1.3733],
        [-1.0292, -1.0287, -1.0171, -1.4898]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0475, -0.9694, -1.0443, -1.3516],
        [-0.9957, -0.9501, -0.9984, -1.3436]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 11, Time: 0.057055000215768814
Total Reward: [  0.7 -11. ], Discounted: [  0.62673676 -10.361512  ]
Scalarized Reward: -11.0, Discounted: -10.361512184143066
scalar_q_values:tensor([[-1.0093, -0.9852, -1.0313, -1.3724],
        [-1.1026, -1.0113, -1.0489, -1.3986]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025909999385476112
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9821, -0.9467, -1.0337, -1.3108],
        [-0.9751, -1.0184, -1.0046, -1.4204]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002503999974578619
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9991, -1.0202, -0.9827, -1.3692],
        [-1.0232, -1.0111, -0.9811, -1.3976]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9019, -1.0189, -0.9717, -1.3622],
        [-0.9978, -0.9946, -0.9871, -1.4262]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9898, -0.9859, -1.1034, -1.3697],
        [-1.0217, -0.9842, -0.9855, -1.4273]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.014293000102043152
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.9946, -0.9647, -0.9216, -1.3467],
        [-1.0123, -1.0113, -0.9937, -1.3716]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9854, -1.0352, -0.9838, -1.3751],
        [-0.9778, -0.9815, -0.9860, -1.3640]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9462, -1.0272, -0.9900, -1.3474],
        [-1.0145, -0.9845, -0.9824, -1.3712]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9516, -1.0282, -0.9663, -1.3330],
        [-1.0166, -1.0332, -0.9784, -1.4053]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0419, -1.0063, -1.0224, -1.3106],
        [-0.9950, -1.0187, -0.9304, -1.3224]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9534, -1.0201, -1.0251, -1.2729],
        [-1.0037, -0.9855, -0.9826, -1.3675]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9451, -0.9986, -1.0056, -1.2766],
        [-0.9798, -0.9744, -0.9435, -1.3294]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9733, -1.0453, -1.0352, -1.2790],
        [-1.0058, -1.0396, -0.9870, -1.3348]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9574, -1.0245, -1.0289, -1.2191],
        [-1.0486, -0.9928, -1.0083, -1.2192]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9843, -1.0368, -0.9393, -1.2239],
        [-1.0350, -1.0135, -1.0160, -1.3389]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9606, -1.0371, -1.0151, -1.1835],
        [-1.0391, -1.0173, -0.9705, -1.3383]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9602, -0.9857, -1.0214, -1.3067],
        [-1.0152, -0.9579, -1.0209, -1.3020]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 12, Time: 0.05964700132608414
Total Reward: [  0.7 -12. ], Discounted: [  0.6204694 -11.247897 ]
Scalarized Reward: -12.0, Discounted: -11.247897148132324
scalar_q_values:tensor([[-0.9402, -1.0167, -0.9592, -1.3703],
        [-0.9894, -0.9893, -0.9823, -1.3202]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9968, -0.9962, -0.9904, -1.3496],
        [-1.0081, -0.9638, -0.9994, -1.3326]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007459999993443489
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-1.0327, -0.9957, -1.0150, -1.3362],
        [-0.9982, -1.0102, -0.9094, -1.2702]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9765, -0.9798, -0.9939, -1.3004],
        [-1.0477, -0.9834, -1.0215, -1.3939]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0231, -0.9976, -0.9932, -1.3294],
        [-1.0054, -0.9910, -1.0175, -1.3963]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01266700029373169
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.9896, -0.9934, -0.9954, -1.4039],
        [-1.0487, -1.0005, -1.0361, -1.4397]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9985, -1.0094, -1.0173, -1.3334],
        [-0.9031, -1.0067, -0.9559, -1.3485]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0155, -0.9572, -1.0276, -1.3687],
        [-0.9763, -0.9912, -1.0213, -1.3573]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012891000136733055
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[ 0.0515,  0.7119,  0.0101,  0.2913],
        [-0.0897,  0.6683,  0.1103,  0.3424]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023040000814944506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.8946, -1.0003, -1.0265, -1.2407],
        [-1.0293, -1.0019, -0.9514, -1.3293]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9582, -1.0067, -0.9801, -1.3193],
        [-1.0083, -1.0522, -0.9898, -1.3072]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9730, -0.9893, -0.9817, -1.2500],
        [-0.9990, -1.0070, -1.0022, -1.3912]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9624, -1.0240, -1.0054, -1.3226],
        [-0.9908, -1.0352, -1.0034, -1.3969]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0224, -1.0027, -1.0298, -1.3659],
        [-0.9528, -1.0344, -0.9542, -1.4443]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9953, -0.9839, -1.0277, -1.3029],
        [-0.9998, -1.0177, -0.9905, -1.3710]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 6, Time: 0.02776000089943409
Total Reward: [ 0.7 -6. ], Discounted: [ 0.6590361 -5.793465 ]
Scalarized Reward: -6.0, Discounted: -5.7934651374816895
scalar_q_values:tensor([[0.0010, 0.7317, 0.0612, 0.2512],
        [0.0246, 0.6666, 0.0368, 0.2415]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002176000038161874
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0570, 0.6893, 0.1023, 0.3214],
        [0.0379, 0.6548, 0.0684, 0.3304]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021569998934865
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0292,  0.7188,  0.1257,  0.3465],
        [-0.0040,  0.7082, -0.0497,  0.2910]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024939998984336853
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0225, -0.9993, -1.0020, -1.4188],
        [-1.0145, -1.0100, -0.9882, -1.3913]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0398, -0.9873, -0.9965, -1.3538],
        [-1.0267, -0.9869, -0.9925, -1.3772]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.008785000070929527
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0223,  0.7015,  0.0072,  0.3301],
        [-0.0134,  0.7412, -0.0030,  0.2550]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002297000028192997
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9782, -1.0007, -1.0255, -1.3579],
        [-1.0094, -1.0140, -1.0117, -1.4200]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9302, -0.9163, -1.0534, -1.3595],
        [-0.9905, -1.0413, -1.0252, -1.3831]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.00800899975001812
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0039,  0.7313,  0.0055,  0.2779],
        [-0.0080,  0.7246,  0.0026,  0.2452]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002188999904319644
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0306,  0.7744, -0.0112,  0.2856],
        [ 0.0583,  0.6837, -0.0398,  0.3350]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022799998987466097
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0300,  0.6685,  0.0423,  0.3362],
        [ 0.0123,  0.6401, -0.0198,  0.2788]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020800000056624413
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-4.4058e-04,  6.8222e-01, -5.5250e-02,  3.9788e-01],
        [-1.3898e-02,  6.3927e-01,  6.7609e-02,  2.8789e-01]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002196999965235591
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9326, -0.9125, -0.8660, -1.3538],
        [-1.0293, -0.9960, -0.9576, -1.4344]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9847, -0.9967, -1.0118, -1.3572],
        [-0.9392, -1.0705, -0.9914, -1.4085]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0058, -1.0204, -1.0448, -1.3753],
        [-0.9774, -1.0084, -1.0540, -1.4114]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9808, -0.9780, -1.0119, -1.2905],
        [-1.0019, -0.9887, -1.0446, -1.4599]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.018226999789476395
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.0127,  0.6485,  0.0113,  0.3344],
        [ 0.0411,  0.7145,  0.0947,  0.2950]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002322999993339181
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0200, -0.9948, -1.0099, -1.3732],
        [-1.0260, -1.0106, -0.9998, -1.4030]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.006310999859124422
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.0344,  0.7319,  0.0047,  0.3961],
        [-0.0069,  0.7238,  0.0587,  0.2575]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002167999977245927
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9622, -0.9920, -0.9730, -1.3755],
        [-1.0710, -1.0232, -0.9453, -1.3881]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9846, -0.9624, -0.9840, -1.2956],
        [-1.0045, -1.0041, -1.0242, -1.3975]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007294000126421452
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.9898, -1.0033, -1.0545, -1.3289],
        [-0.9793, -1.0139, -1.0372, -1.3790]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9960, -0.9679, -1.0694, -1.3522],
        [-1.0124, -1.0161, -1.0465, -1.4090]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.00749099999666214
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[0.0656, 0.7394, 0.1475, 0.2521],
        [0.0138, 0.7150, 0.0313, 0.2533]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020699999295175076
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0342,  0.7497,  0.0179,  0.2356],
        [-0.0348,  0.8218, -0.0392,  0.2575]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002191999927163124
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0069,  0.6902, -0.0464,  0.3528],
        [-0.0226,  0.7385,  0.0197,  0.2539]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020699999295175076
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0130,  0.6816, -0.0059,  0.3396],
        [-0.0217,  0.7290, -0.0259,  0.2940]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002116000046953559
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0214, -1.0247, -0.9399, -1.3878],
        [-1.0260, -0.9640, -1.0546, -1.3866]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9741, -0.9719, -0.9964, -1.3307],
        [-0.9842, -1.0081, -0.9250, -1.3642]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9973, -1.0043, -1.0003, -1.3611],
        [-0.9763, -0.9942, -1.0464, -1.3574]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0403, -0.9805, -0.9967, -1.3590],
        [-1.0066, -1.0008, -1.0399, -1.3956]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.01793999969959259
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[ 0.0030,  0.6618, -0.0265,  0.3478],
        [-0.0486,  0.6854,  0.0574,  0.2734]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021510000806301832
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0572,  0.5730, -0.0033,  0.3767],
        [-0.0087,  0.6506, -0.0352,  0.2662]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002124000107869506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0024,  0.6929,  0.0178,  0.3022],
        [-0.0198,  0.6461, -0.0864,  0.2656]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021230001002550125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0771, -1.0799, -0.9605, -1.3566],
        [-1.0079, -0.9992, -1.0071, -1.4168]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0197, -1.0086, -0.9436, -1.4047],
        [-0.9859, -0.9485, -0.9834, -1.4389]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9847, -1.0104, -0.9843, -1.4278],
        [-0.9750, -1.0174, -0.9687, -1.3935]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9898, -0.9782, -1.0009, -1.2853],
        [-1.0228, -1.0020, -1.0073, -1.4006]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.01849300041794777
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.9924, -0.9588, -0.9632, -1.4178],
        [-0.9579, -1.0026, -1.0233, -1.4645]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0301, -1.0410, -0.9807, -1.3528],
        [-1.0027, -1.0279, -1.0324, -1.4725]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0644, -1.0165, -1.0516, -1.4474],
        [-0.9645, -1.0225, -1.0268, -1.4707]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9851, -0.9852, -1.0211, -1.3888],
        [-0.9509, -0.9155, -0.9999, -1.4453]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.0181489996612072
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.0047,  0.6745, -0.0030,  0.2616],
        [ 0.0283,  0.8960, -0.0951,  0.1676]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00216599996201694
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9861, -0.9996, -0.9375, -1.4300],
        [-0.9413, -0.9726, -0.9546, -1.4442]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0099, -1.0215, -0.9878, -1.4269],
        [-1.0124, -1.0098, -0.9704, -1.4484]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0524, -0.9935, -1.0237, -1.4187],
        [-0.9723, -0.9810, -1.0155, -1.4409]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0391, -0.9712, -1.0363, -1.3878],
        [-0.9883, -1.0408, -1.0375, -1.4404]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.017673000693321228
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[ 0.0067,  0.7054,  0.1880,  0.2231],
        [-0.0116,  0.7824,  0.0501,  0.1537]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021039999555796385
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9448, -1.0227, -0.9831, -1.3933],
        [-0.9954, -1.0255, -0.9937, -1.3734]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9513, -1.0098, -0.9753, -1.4174],
        [-0.9908, -1.0102, -0.9783, -1.4251]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9793, -1.0128, -0.9939, -1.4055],
        [-1.0083, -0.9912, -0.9791, -1.3933]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0003, -0.9912, -0.9631, -1.3249],
        [-1.0567, -1.0160, -0.9613, -1.4645]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0162, -1.0045, -1.0028, -1.4066],
        [-1.0036, -1.0193, -1.0395, -1.4507]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0273, -1.0200, -1.0154, -1.4011],
        [-0.9861, -0.9775, -1.0498, -1.4604]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 6, Time: 0.028015000745654106
Total Reward: [ 0.7 -6. ], Discounted: [ 0.6590361 -5.793465 ]
Scalarized Reward: -6.0, Discounted: -5.7934651374816895
scalar_q_values:tensor([[-1.0108, -1.0154, -0.9931, -1.4286],
        [-0.9737, -1.0043, -1.0197, -1.4043]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9946, -0.9959, -1.0131, -1.4314],
        [-1.0121, -0.9925, -0.9546, -1.4527]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9793, -1.0187, -0.9332, -1.4648],
        [-1.0194, -1.0009, -0.9738, -1.4718]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9612, -1.0288, -0.9860, -1.3527],
        [-1.0063, -0.9879, -0.9631, -1.4343]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0189, -0.9894, -0.9762, -1.4208],
        [-0.9563, -0.9395, -0.9832, -1.4848]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 5, Time: 0.023730000481009483
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-0.9743, -0.9804, -1.0015, -1.3772],
        [-0.9772, -0.9342, -1.0124, -1.4087]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021979999728500843
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9382, -1.0192, -0.9758, -1.3663],
        [-1.0086, -1.0262, -0.9950, -1.3938]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9807, -1.0098, -1.0111, -1.3585],
        [-1.0256, -0.9363, -1.0032, -1.3869]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007408000063151121
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.9649, -1.0231, -0.9831, -1.3423],
        [-0.9912, -1.0011, -1.0123, -1.3854]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9997, -0.9688, -1.0384, -1.4050],
        [-1.0145, -0.9991, -0.9912, -1.3902]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007538999896496534
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.9987, -0.9773, -0.9885, -1.3536],
        [-1.0094, -1.0162, -0.9882, -1.3858]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021170000545680523
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0154, -1.0144, -0.9856, -1.3584],
        [-1.0149, -1.0117, -0.9863, -1.4083]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9799, -1.0160, -0.9596, -1.2944],
        [-1.0130, -0.9898, -0.9872, -1.3859]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0347, -0.9799, -0.9937, -1.2832],
        [-0.9942, -1.0073, -1.0348, -1.4365]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013008000329136848
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.9719, -0.9906, -1.0026, -1.3474],
        [-1.0317, -0.9883, -1.0419, -1.3736]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9954, -1.0246, -0.9967, -1.3509],
        [-1.0646, -1.0414, -1.0664, -1.3275]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9808, -0.9667, -0.9889, -1.3641],
        [-0.9960, -1.0081, -0.9990, -1.4242]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.016797000542283058
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.0204, -0.9679, -0.9548, -1.4077],
        [-0.9956, -1.0279, -0.9609, -1.4493]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9032, -0.9412, -1.0970, -1.3893],
        [-0.9873, -1.0080, -0.9871, -1.4428]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0175, -1.0264, -1.0470, -1.3396],
        [-0.9663, -1.0431, -1.0207, -1.3653]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0376, -1.0289, -1.0401, -1.3363],
        [-0.9962, -1.0226, -1.0171, -1.4032]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9937, -0.9975, -0.9514, -1.3619],
        [-0.9711, -1.0291, -1.0070, -1.4180]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0083, -1.0017, -1.0395, -1.3665],
        [-0.9047, -0.9193, -1.0124, -1.5210]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9545, -0.9529, -1.0274, -1.4062],
        [-0.9865, -1.0153, -0.9878, -1.3232]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 7, Time: 0.03430899977684021
Total Reward: [ 0.7 -7. ], Discounted: [ 0.65244573 -6.7255306 ]
Scalarized Reward: -7.0, Discounted: -6.725530624389648
scalar_q_values:tensor([[-0.9910, -1.0023, -0.9953, -1.3562],
        [-0.9695, -1.0116, -0.9948, -1.3647]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0095, -1.0236, -1.0378, -1.3926],
        [-1.0265, -0.9987, -0.9722, -1.3829]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0163, -1.0363, -1.0568, -1.3692],
        [-1.0228, -0.9040, -1.0315, -1.3564]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01295199990272522
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-1.0414, -0.9892, -1.0359, -1.3442],
        [-1.0134, -0.9765, -1.0367, -1.3270]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020840000361204147
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0012,  0.6792, -0.0793,  0.2465],
        [-0.0296,  0.6860, -0.0316,  0.2495]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020979999098926783
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9584, -0.9721, -1.0264, -1.3010],
        [-0.9905, -0.9845, -1.0370, -1.3820]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9874, -1.0098, -0.9828, -1.3384],
        [-0.9434, -0.9622, -0.9618, -1.2723]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9981, -0.9988, -1.0581, -1.2691],
        [-0.9964, -1.0010, -1.0154, -1.3154]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9557, -1.0018, -1.0035, -1.3629],
        [-1.0239, -0.9461, -1.0280, -1.3466]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.017469000071287155
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.0290, -0.9937, -0.9667, -1.3176],
        [-1.0180, -0.9653, -0.9960, -1.3354]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021639999467879534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.1077,  0.7053, -0.0420,  0.2641],
        [-0.0034,  0.6449,  0.0226,  0.2091]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002124000107869506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0221,  0.6761, -0.0265,  0.3250],
        [ 0.0434,  0.7499,  0.0319,  0.2977]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020729999523609877
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0188,  0.6504, -0.0470,  0.3112],
        [-0.0074,  0.6895, -0.0053,  0.3018]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002440999960526824
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0187,  0.6991,  0.0147,  0.3449],
        [-0.0650,  0.7566,  0.0333,  0.2496]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002242000075057149
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9832, -0.9820, -0.9863, -1.3552],
        [-1.0424, -1.0087, -0.9665, -1.4108]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9293, -0.9926, -1.0024, -1.3047],
        [-1.0106, -0.9985, -1.0111, -1.4060]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0354, -1.0387, -0.9968, -1.3570],
        [-1.0487, -0.9439, -1.0036, -1.4293]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012520999647676945
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[ 0.0281,  0.6950,  0.0180,  0.3226],
        [-0.0054,  0.6595, -0.0283,  0.2621]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002174000022932887
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9888, -0.9764, -0.9706, -1.3716],
        [-0.9792, -1.0047, -1.0139, -1.4459]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9708, -0.9897, -1.0173, -1.3730],
        [-1.0232, -1.0032, -1.0344, -1.4068]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9894, -0.9816, -1.0196, -1.4239],
        [-0.9772, -1.0072, -0.9837, -1.4457]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9942, -1.0020, -1.0037, -1.3802],
        [-1.0002, -1.0040, -0.9913, -1.4027]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0100, -0.9882, -1.0211, -1.3972],
        [-1.0370, -1.0001, -0.9530, -1.4250]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0516, -1.0188, -1.0009, -1.2831],
        [-0.9986, -1.0414, -1.0066, -1.3622]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9977, -0.9887, -0.9899, -1.3643],
        [-1.0310, -0.9599, -1.0488, -1.4275]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 7, Time: 0.03472200036048889
Total Reward: [ 0.7 -7. ], Discounted: [ 0.65244573 -6.7255306 ]
Scalarized Reward: -7.0, Discounted: -6.725530624389648
scalar_q_values:tensor([[-0.0150,  0.6225, -0.0565,  0.2582],
        [-0.0125,  0.6891, -0.0529,  0.2647]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022509999107569456
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0089, -0.9832, -0.9605, -1.3187],
        [-1.0345, -0.9282, -0.9781, -1.3884]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002157999901100993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0083,  0.7220, -0.0049,  0.3038],
        [-0.0404,  0.6219,  0.1116,  0.3310]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002047999994829297
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0068,  0.7221,  0.0146,  0.2779],
        [-0.0408,  0.7505,  0.0250,  0.2214]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002047999994829297
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9976, -0.9698, -1.0365, -1.3695],
        [-1.0048, -1.0126, -0.9803, -1.4020]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023930000606924295
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0255, -0.9764, -0.9202, -1.2870],
        [-1.0092, -0.9787, -0.9713, -1.2911]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9752, -0.9869, -1.0069, -1.3785],
        [-0.9940, -0.9888, -1.0258, -1.4073]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9972, -0.9913, -1.0109, -1.3589],
        [-1.0128, -1.0008, -1.0138, -1.3942]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.014210999943315983
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.0424,  0.7208,  0.0062,  0.2751],
        [ 0.0038,  0.6725, -0.0262,  0.1754]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00216299993917346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9807, -1.0101, -0.9826, -1.2883],
        [-1.0420, -0.9867, -0.9976, -1.3990]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0176, -1.0617, -0.9443, -1.3420],
        [-1.0486, -1.0494, -1.0101, -1.3562]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9752, -1.0002, -1.0045, -1.3419],
        [-0.9959, -0.9952, -1.0158, -1.4041]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9953, -0.9833, -1.0262, -1.3733],
        [-0.9901, -0.9976, -1.0247, -1.3967]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.017506999894976616
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[0.0164, 0.6740, 0.0404, 0.3117],
        [0.0079, 0.6384, 0.0060, 0.1828]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002093000104650855
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9682, -1.0085, -1.0427, -1.3139],
        [-1.0702, -1.0239, -1.0263, -1.4054]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0090, -1.0260, -0.9897, -1.3906],
        [-1.0139, -1.0277, -0.9964, -1.4411]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9385, -1.0405, -0.9940, -1.4448],
        [-1.0085, -1.0204, -0.9970, -1.4480]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0031, -0.9487, -0.9683, -1.4156],
        [-1.0010, -0.9974, -1.0027, -1.4184]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.017775999382138252
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.9889, -0.9405, -1.0404, -1.3775],
        [-0.9474, -0.9848, -0.9885, -1.4240]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022319999989122152
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 9.4334e-03,  6.7573e-01, -1.3932e-02,  2.9348e-01],
        [-6.5868e-04,  6.8721e-01, -1.1843e-02,  2.7166e-01]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0019759999122470617
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0407,  0.6822,  0.0020,  0.3739],
        [-0.0314,  0.7036,  0.0144,  0.2591]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002051000017672777
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0082, -1.0082, -1.0078, -1.3887],
        [-1.0156, -1.0118, -1.0102, -1.4259]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9930, -0.9758, -0.9449, -1.3917],
        [-1.0161, -1.0178, -1.0235, -1.3970]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0226, -1.0252, -0.9704, -1.3547],
        [-1.0226, -0.9979, -1.0185, -1.3996]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9990, -0.9933, -1.0093, -1.3687],
        [-1.0123, -0.9827, -1.0117, -1.3906]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.017542000859975815
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.0403,  0.6645,  0.0307,  0.3451],
        [ 0.0373,  0.7000, -0.0368,  0.1963]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022279999684542418
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0621,  0.6485, -0.0082,  0.2829],
        [-0.0181,  0.7594,  0.0608,  0.2312]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020729999523609877
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9335, -0.8956, -0.9410, -1.3734],
        [-1.0365, -0.9706, -0.9848, -1.3705]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021069999784231186
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0333,  0.6861,  0.0340,  0.2127],
        [-0.0059,  0.7264,  0.0239,  0.1882]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020240000449121
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0094,  0.6975,  0.0069,  0.2615],
        [-0.0294,  0.6657, -0.0329,  0.2090]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002073999959975481
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0242,  0.7532,  0.0588,  0.3041],
        [-0.0092,  0.6376, -0.0307,  0.2089]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020580000709742308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0014, -1.0553, -1.0276, -1.3296],
        [-0.9901, -1.0008, -0.9869, -1.4352]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9912, -0.9781, -0.9728, -1.3237],
        [-0.9724, -1.0107, -0.9759, -1.3834]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9781, -1.0048, -0.9988, -1.3463],
        [-0.9637, -0.9961, -1.0136, -1.4017]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9347, -0.9914, -0.9871, -1.3240],
        [-0.9647, -1.0091, -1.0282, -1.4608]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9486, -0.9871, -1.0648, -1.3462],
        [-0.9909, -1.0008, -0.9540, -1.3336]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9978, -0.9954, -1.0094, -1.3724],
        [-1.0050, -0.9917, -1.0380, -1.4307]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 6, Time: 0.028147999197244644
Total Reward: [ 0.7 -6. ], Discounted: [ 0.6590361 -5.793465 ]
Scalarized Reward: -6.0, Discounted: -5.7934651374816895
scalar_q_values:tensor([[ 0.0162,  0.6594,  0.0520,  0.2492],
        [ 0.0205,  0.7129, -0.0057,  0.2122]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021100000012665987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0000, -0.9982, -1.0155, -1.3843],
        [-0.9739, -0.9731, -1.0171, -1.4068]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.9995, -0.9912, -0.9901, -1.3530],
        [-0.9788, -0.9893, -0.9481, -1.5011]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9444, -1.0241, -0.9683, -1.4091],
        [-0.9976, -0.9556, -0.9518, -1.4245]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9352, -0.9291, -0.9389, -1.3382],
        [-0.9907, -1.0006, -1.0161, -1.4160]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013380000367760658
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.9864, -1.0553, -0.9767, -1.4552],
        [-0.9476, -0.9583, -1.0230, -1.3269]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9779, -1.0075, -1.0069, -1.3681],
        [-1.0088, -0.9905, -0.9985, -1.4098]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0327, -1.0255, -0.9927, -1.4063],
        [-0.9754, -0.9976, -1.0547, -1.4457]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9299, -0.9996, -0.9753, -1.3958],
        [-0.9860, -0.9946, -1.0109, -1.4311]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0292, -0.9486, -1.0244, -1.3074],
        [-0.9806, -1.0034, -0.9814, -1.3921]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.023090999573469162
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[ 0.0022,  0.7026, -0.0147,  0.2326],
        [ 0.0143,  0.7425,  0.0126,  0.1592]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002167999977245927
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0025,  0.6849,  0.0269,  0.2549],
        [-0.0037,  0.7079,  0.0558,  0.2360]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002096999902278185
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0001, -0.9790, -0.9611, -1.4120],
        [-0.9993, -1.0044, -1.0104, -1.4171]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0739, -1.0127, -1.0189, -1.3191],
        [-0.9981, -1.0103, -0.9754, -1.4449]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0337, -0.9859, -1.0000, -1.4140],
        [-0.9818, -1.0515, -0.9717, -1.4307]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0106, -0.9795, -0.9781, -1.4103],
        [-1.0018, -1.0149, -0.9870, -1.4485]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9982, -1.0115, -0.9925, -1.3716],
        [-1.0075, -1.0162, -0.9846, -1.3850]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9757, -0.9827, -1.0140, -1.3646],
        [-1.0234, -1.0026, -0.9909, -1.4718]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9502, -0.9630, -0.9376, -1.4112],
        [-1.0160, -1.0686, -1.0122, -1.4632]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9994, -0.9672, -0.9758, -1.3913],
        [-0.9549, -1.0183, -0.9867, -1.4468]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0307, -1.0210, -0.9973, -1.3950],
        [-1.0130, -0.9902, -0.9827, -1.4289]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9592, -0.9483, -0.9947, -1.3791],
        [-1.0078, -1.0020, -1.0064, -1.4230]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 11, Time: 0.05376800149679184
Total Reward: [  0.7 -11. ], Discounted: [  0.62673676 -10.361512  ]
Scalarized Reward: -11.0, Discounted: -10.361512184143066
scalar_q_values:tensor([[-0.0028,  0.6975, -0.0255,  0.2365],
        [-0.0353,  0.7553, -0.0362,  0.1852]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021730000153183937
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9883, -0.9789, -0.9962, -1.3675],
        [-1.0165, -0.9877, -0.9960, -1.4379]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.006357000209391117
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.0960,  0.6680,  0.0469,  0.2929],
        [ 0.0573,  0.6845, -0.0203,  0.2558]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021979999728500843
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0091, -1.0124, -0.9989, -1.3912],
        [-0.9834, -1.0189, -0.9911, -1.4179]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0536, -1.0234, -1.0257, -1.3924],
        [-0.9732, -0.9996, -0.9938, -1.4537]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9941, -1.0034, -0.9957, -1.2551],
        [-1.0180, -1.0228, -0.9751, -1.3974]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0157, -0.9648, -0.9871, -1.3261],
        [-1.0195, -0.9615, -0.9940, -1.4140]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.01779399998486042
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.9882, -1.0197, -1.0152, -1.3544],
        [-0.9409, -0.9894, -1.0223, -1.3224]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0085, -0.9902, -0.9917, -1.4134],
        [-0.9764, -0.9841, -0.9086, -1.3766]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0275, -0.9858, -0.9676, -1.3296],
        [-0.9944, -0.9970, -1.0428, -1.3702]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0303, -0.9965, -0.9739, -1.3609],
        [-1.0183, -0.9991, -1.0163, -1.3825]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0083, -1.0099, -1.0265, -1.3584],
        [-1.0048, -0.9859, -0.9346, -1.3922]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9913, -1.0041, -0.9943, -1.3713],
        [-1.0052, -0.9952, -1.0094, -1.4135]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0243, -1.0078, -1.0055, -1.3889],
        [-0.9840, -1.0099, -1.0095, -1.4023]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9573, -1.0231, -0.9728, -1.3709],
        [-1.0137, -0.9694, -0.9742, -1.3710]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0314, -0.9892, -1.0326, -1.3988],
        [-1.0310, -0.9913, -0.9862, -1.3892]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9712, -0.9894, -1.0265, -1.3722],
        [-0.9990, -0.9588, -0.9931, -1.3665]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 10, Time: 0.04878599941730499
Total Reward: [  0.7 -10. ], Discounted: [ 0.6330674 -9.466174 ]
Scalarized Reward: -10.0, Discounted: -9.466174125671387
scalar_q_values:tensor([[-1.0379, -0.9969, -1.0200, -1.3209],
        [-0.9852, -1.0129, -0.9904, -1.3549]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0019, -0.9697, -0.9814, -1.3360],
        [-0.9948, -1.0008, -0.9564, -1.4214]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9561, -1.0080, -1.0253, -1.3944],
        [-1.0301, -1.0095, -1.0125, -1.4648]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0016, -0.9955, -1.0001, -1.3838],
        [-1.0373, -1.0323, -0.9811, -1.4531]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0048, -0.9628, -0.9895, -1.4059],
        [-0.9973, -0.9520, -0.9564, -1.4269]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 5, Time: 0.024020999670028687
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-0.0086,  0.6919, -0.0125,  0.2556],
        [ 0.0035,  0.7198, -0.0174,  0.2392]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.006275000050663948
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[ 0.0072,  0.6913, -0.0430,  0.3143],
        [ 0.0136,  0.6964,  0.0719,  0.2226]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023300000466406345
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0328,  0.7487,  0.0134,  0.2749],
        [ 0.0429,  0.7088, -0.0045,  0.1308]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0029170000925660133
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9952, -0.9995, -1.0128, -1.3512],
        [-0.9756, -0.9780, -0.9421, -1.4635]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0212, -1.0329, -0.9957, -1.3595],
        [-1.0166, -0.9766, -0.9989, -1.3758]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007662000134587288
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0014,  0.7338,  0.0178,  0.2308],
        [ 0.0198,  0.7024, -0.0045,  0.1720]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0021,  0.6921, -0.0102,  0.2488],
        [ 0.0251,  0.7167, -0.0516,  0.1978]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021180000621825457
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9710, -1.0573, -0.9759, -1.2759],
        [-1.0046, -0.9931, -1.0521, -1.4352]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.2850, -1.0753, -1.0408, -1.3456],
        [-1.4369, -1.0924, -1.0859, -1.5285]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9575, -0.9746, -1.0128, -1.3593],
        [-1.0272, -1.0075, -0.9847, -1.3960]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0119, -1.0103, -0.9876, -1.3405],
        [-1.0034, -1.0161, -1.0228, -1.3912]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0257, -1.0118, -0.9832, -1.4014],
        [-0.9645, -0.9832, -1.0140, -1.4347]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9647, -1.0148, -1.0071, -1.4027],
        [-1.0300, -1.0333, -0.9924, -1.3586]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0253, -0.9621, -0.9883, -1.3527],
        [-1.0230, -1.0121, -1.0273, -1.3667]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 8, Time: 0.03893100097775459
Total Reward: [ 0.7 -8. ], Discounted: [ 0.6459213 -7.6482754]
Scalarized Reward: -8.0, Discounted: -7.648275375366211
scalar_q_values:tensor([[-0.9667, -1.0311, -1.0096, -1.3216],
        [-0.9927, -0.9923, -1.0622, -1.3653]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9971, -0.9999, -1.0234, -1.3524],
        [-0.9974, -0.9851, -0.9744, -1.3305]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0586, -0.9823, -0.9918, -1.3005],
        [-0.9771, -1.0135, -1.0387, -1.4111]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0163, -1.0050, -1.0192, -1.3495],
        [-0.9712, -0.9831, -1.0292, -1.3606]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0076, -0.9880, -0.9607, -1.3266],
        [-0.9707, -0.9989, -0.9542, -1.3573]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9678, -1.0096, -0.9979, -1.3488],
        [-1.0144, -0.9408, -0.9826, -1.3563]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 6, Time: 0.029808999970555305
Total Reward: [ 0.7 -6. ], Discounted: [ 0.6590361 -5.793465 ]
Scalarized Reward: -6.0, Discounted: -5.7934651374816895
scalar_q_values:tensor([[-0.9447, -0.9692, -1.0399, -1.3497],
        [-0.9977, -0.9504, -1.0352, -1.3656]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0258, -0.9955, -0.9230, -1.3650],
        [-1.0241, -1.0125, -1.0278, -1.4137]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9984, -1.0123, -1.0218, -1.3465],
        [-1.0442, -0.9930, -0.9902, -1.3850]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0665, -0.9889, -1.0476, -1.4369],
        [-1.0172, -1.0097, -1.0315, -1.4314]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.017746999859809875
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.0254, -1.0261, -0.9999, -1.3574],
        [-1.0363, -1.0189, -0.9838, -1.4342]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9732, -0.9785, -0.9339, -1.3874],
        [-1.0152, -1.0036, -1.0921, -1.3111]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9769, -0.9573, -0.9775, -1.4685],
        [-1.0161, -0.9674, -0.9721, -1.4385]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012888000346720219
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-1.0032, -1.0052, -0.9780, -1.3973],
        [-1.0036, -0.9956, -1.0101, -1.4587]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0199, -0.9798, -1.0097, -1.3755],
        [-0.9698, -0.9935, -0.9861, -1.4191]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9459, -1.0036, -0.9881, -1.3976],
        [-0.9861, -0.9971, -1.0566, -1.3469]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9838, -0.9873, -0.9953, -1.3668],
        [-1.0346, -0.9786, -0.9931, -1.3798]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.018317999318242073
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.4266e-04,  6.6462e-01,  2.2739e-02,  3.3048e-01],
        [ 1.3115e-02,  6.9625e-01,  2.8693e-02,  3.1328e-01]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002222999930381775
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0158,  0.6662,  0.0084,  0.3415],
        [-0.0024,  0.6840,  0.0206,  0.2648]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022670000325888395
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0097, -1.0066, -0.9951, -1.3377],
        [-0.9815, -0.9975, -1.0065, -1.4105]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0443, -0.9670, -0.9978, -1.3236],
        [-0.9684, -0.9897, -1.0482, -1.2826]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.00861900020390749
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.9803, -0.9846, -1.0015, -1.3281],
        [-0.9563, -0.9971, -1.0090, -1.3827]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9927, -1.0018, -1.0143, -1.3478],
        [-1.0153, -1.0016, -0.9966, -1.3931]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0015, -0.9702, -1.0404, -1.4061],
        [-1.0239, -1.0071, -1.0249, -1.3630]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012562000192701817
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-1.0152, -0.9695, -0.9977, -1.3606],
        [-0.9941, -0.9748, -0.9928, -1.3570]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002124000107869506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0359,  0.6834,  0.0150,  0.2580],
        [ 0.0577,  0.7995,  0.0742,  0.2163]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00215000007301569
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0187,  0.7783,  0.0619,  0.3095],
        [-0.0079,  0.6914,  0.0051,  0.2444]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0049,  0.6929, -0.0108,  0.3275],
        [ 0.0155,  0.7167,  0.0310,  0.2984]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002047999994829297
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9972, -0.9785, -0.9473, -1.3604],
        [-0.9878, -1.0034, -0.9859, -1.4099]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.2196, -1.1100, -1.0171, -1.3405],
        [-1.4356, -1.0963, -1.0778, -1.4876]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9604, -1.0136, -1.0488, -1.3875],
        [-0.9008, -0.9984, -0.9657, -1.4450]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9995, -0.9605, -0.9434, -1.2887],
        [-0.9978, -0.9732, -0.9957, -1.3151]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0209, -0.9765, -0.9650, -1.3826],
        [-1.0349, -0.9738, -1.0012, -1.2904]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0068, -1.0404, -0.9045, -1.3619],
        [-0.9681, -1.0099, -0.9337, -1.4370]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9806, -1.0173, -0.9344, -1.3279],
        [-1.0219, -0.9899, -1.0063, -1.4004]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9752, -1.0227, -1.0078, -1.3438],
        [-0.9794, -1.0035, -0.9829, -1.3579]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0075, -0.9935, -0.9935, -1.3182],
        [-1.0329, -1.0469, -0.9768, -1.3748]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9910, -0.9796, -1.0680, -1.3253],
        [-1.0135, -1.0038, -1.0183, -1.3911]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 11, Time: 0.05234700068831444
Total Reward: [  0.7 -11. ], Discounted: [  0.62673676 -10.361512  ]
Scalarized Reward: -11.0, Discounted: -10.361512184143066
scalar_q_values:tensor([[-0.0074,  0.6715, -0.0063,  0.4051],
        [ 0.0131,  0.7018, -0.0082,  0.3315]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022120000794529915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0288, -0.9519, -0.9702, -1.3671],
        [-1.0244, -0.9796, -0.9467, -1.4587]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0152, -1.0046, -0.9963, -1.3392],
        [-0.9966, -1.0078, -0.9922, -1.3916]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0282, -1.0037, -1.0131, -1.3051],
        [-1.0141, -1.0093, -0.9905, -1.3658]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9838, -1.0053, -0.9829, -1.3484],
        [-0.9641, -0.9968, -1.0281, -1.3396]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9849, -1.0191, -1.0072, -1.3216],
        [-1.0199, -0.9936, -0.9980, -1.4199]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0323, -0.9572, -0.9882, -1.3743],
        [-0.9813, -1.0121, -1.0056, -1.4036]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 6, Time: 0.029100000858306885
Total Reward: [ 0.7 -6. ], Discounted: [ 0.6590361 -5.793465 ]
Scalarized Reward: -6.0, Discounted: -5.7934651374816895
scalar_q_values:tensor([[-0.9802, -1.0061, -0.9925, -1.3985],
        [-1.0204, -0.9979, -0.9714, -1.4588]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0238, -0.9980, -0.9850, -1.3963],
        [-1.0690, -1.0001, -0.9906, -1.4138]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0044, -1.0255, -1.0098, -1.3757],
        [-1.0299, -1.0060, -1.0079, -1.4518]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0152, -0.9989, -1.0152, -1.3972],
        [-1.0009, -0.9933, -1.0086, -1.4120]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.017315000295639038
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[ 0.0311,  0.7488, -0.0426,  0.3555],
        [ 0.0393,  0.7618, -0.0757,  0.2355]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0157,  0.7623,  0.1230,  0.2656],
        [ 0.0399,  0.7675, -0.0641,  0.2656]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002090000081807375
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0021, -1.0260, -0.9849, -1.4080],
        [-0.9958, -0.9958, -1.0524, -1.4056]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9957, -1.0058, -1.0057, -1.3809],
        [-0.9972, -0.9884, -1.0090, -1.4418]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.008669000118970871
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[ 0.0080,  0.7349, -0.0120,  0.2724],
        [ 0.0440,  0.7608,  0.0306,  0.2713]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002203000010922551
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0290,  0.6962,  0.0283,  0.3525],
        [-0.0271,  0.7427, -0.0386,  0.2964]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002328000031411648
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0779, -1.0165, -0.9953, -1.3873],
        [-1.0344, -0.9823, -0.9892, -1.4121]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020840000361204147
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0109,  0.6926, -0.0471,  0.3145],
        [-0.0321,  0.7170, -0.0083,  0.2833]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020630001090466976
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9661, -0.9707, -1.0273, -1.3712],
        [-1.0101, -0.9948, -1.0409, -1.4266]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9493, -1.0181, -0.9930, -1.3798],
        [-1.0000, -0.9973, -0.9917, -1.4409]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0245, -1.0125, -1.0481, -1.3642],
        [-0.9976, -1.0153, -1.0259, -1.4337]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0045, -0.9685, -1.0105, -1.3378],
        [-1.0116, -0.9901, -0.9844, -1.4138]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.01765800081193447
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-1.0133, -0.9796, -0.9506, -1.2895],
        [-0.9688, -0.9582, -0.9698, -1.4379]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0260, -1.0050, -1.0040, -1.3806],
        [-1.0304, -1.0496, -1.0191, -1.3774]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9658, -1.0093, -1.0288, -1.3478],
        [-0.9775, -1.0190, -1.0032, -1.4083]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0097, -0.9831, -1.0422, -1.3948],
        [-0.9884, -0.9841, -1.0053, -1.4225]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.01757100038230419
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[ 0.0141,  0.8501,  0.0534,  0.2577],
        [-0.0209,  0.7502, -0.0248,  0.1445]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002032000105828047
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0395, -0.9887, -0.9748, -1.3844],
        [-0.9836, -1.0395, -0.9911, -1.4011]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9817, -1.0148, -0.9950, -1.3812],
        [-0.9538, -0.9715, -0.9939, -1.4376]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0363, -1.0164, -0.9745, -1.3980],
        [-0.9869, -1.0190, -0.9929, -1.3631]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0020, -0.9980, -1.0132, -1.3863],
        [-0.9900, -1.0263, -1.0217, -1.4750]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0294, -1.0143, -1.0119, -1.3999],
        [-1.0377, -0.9950, -0.9656, -1.4462]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9816, -1.0318, -1.0613, -1.3546],
        [-0.9656, -1.0186, -0.9926, -1.4217]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9879, -0.9621, -0.9525, -1.3056],
        [-1.0258, -0.9973, -0.9961, -1.4352]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0071, -0.9271, -0.9473, -1.4243],
        [-1.0119, -1.0233, -0.9761, -1.4675]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 8, Time: 0.03826899826526642
Total Reward: [ 0.7 -8. ], Discounted: [ 0.6459213 -7.6482754]
Scalarized Reward: -8.0, Discounted: -7.648275375366211
scalar_q_values:tensor([[-1.0126, -1.0143, -1.0264, -1.3746],
        [-0.9917, -1.0182, -0.9964, -1.4081]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9890, -1.0013, -1.0295, -1.4429],
        [-0.9964, -0.9827, -1.0284, -1.3692]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.008062000386416912
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-0.9995, -1.0163, -1.0018, -1.4010],
        [-1.0100, -0.9510, -1.0367, -1.4015]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002440999960526824
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0018, 0.6989, 0.0042, 0.3127],
        [0.0292, 0.6712, 0.0307, 0.2773]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020649998914450407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0043, -1.0072, -0.9876, -1.3378],
        [-0.9649, -1.0060, -0.9987, -1.3940]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0088, -1.0241, -0.9986, -1.3807],
        [-1.0004, -1.0351, -0.9675, -1.3953]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0304, -1.0111, -0.9219, -1.3607],
        [-1.0542, -1.0145, -0.9721, -1.4231]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0436, -0.9922, -1.0172, -1.3815],
        [-1.0160, -0.9928, -1.0211, -1.4000]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.01779700070619583
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.9903, -1.0120, -0.9513, -1.4067],
        [-0.9967, -0.9693, -0.9939, -1.4017]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9945, -0.9772, -0.9961, -1.4178],
        [-0.9974, -0.9854, -0.9741, -1.4258]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9757, -1.0218, -1.0195, -1.4039],
        [-0.9663, -1.0099, -1.0228, -1.4167]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9817, -0.9904, -0.9717, -1.3881],
        [-0.9953, -1.0088, -1.0087, -1.4555]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0224, -0.9897, -1.0110, -1.3761],
        [-0.9985, -0.9946, -1.0114, -1.4670]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.023562999442219734
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-1.0163, -0.9923, -1.0306, -1.3862],
        [-1.0387, -1.0002, -1.0010, -1.4773]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022950000129640102
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0509, -1.0135, -0.9848, -1.3263],
        [-1.0000, -1.0021, -1.0282, -1.4211]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0213, -0.9893, -1.0169, -1.3576],
        [-1.0250, -1.0144, -0.9856, -1.4348]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9703, -1.0121, -0.9873, -1.3625],
        [-0.9960, -1.0204, -0.9538, -1.3864]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0030, -0.9867, -1.0224, -1.3760],
        [-0.9813, -1.0049, -1.0115, -1.4272]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9778, -0.9665, -1.0029, -1.4086],
        [-0.9976, -0.9888, -0.9759, -1.4072]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.023756999522447586
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: -5.0, Discounted: -4.851984977722168
scalar_q_values:tensor([[-1.0281, -0.9900, -1.0454, -1.3205],
        [-1.0101, -0.9953, -0.9940, -1.4194]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024339999072253704
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 2.1108e-02,  6.3251e-01, -9.2837e-03,  3.1838e-01],
        [-9.1435e-03,  6.9509e-01, -2.7309e-04,  2.4008e-01]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020959998946636915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0106,  0.6890,  0.0367,  0.3028],
        [ 0.0009,  0.6958, -0.0045,  0.2315]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0247, -0.9841, -1.0120, -1.3862],
        [-1.0104, -0.9925, -0.9913, -1.4087]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002032000105828047
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.0243, -0.9728, -1.0155, -1.3705],
        [-0.9658, -0.9255, -1.0510, -1.4647]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020349998958408833
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0096,  0.7159, -0.0286,  0.2979],
        [-0.0008,  0.8214,  0.0267,  0.2560]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0025400000158697367
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9610, -0.9765, -0.9762, -1.3449],
        [-1.0248, -1.0154, -0.9452, -1.4125]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0104, -1.0104, -1.0193, -1.3634],
        [-1.0069, -1.0282, -0.9941, -1.4029]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9823, -1.0062, -0.9882, -1.3627],
        [-1.0273, -1.0580, -1.0221, -1.4331]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9972, -1.0011, -0.9961, -1.3169],
        [-1.0132, -0.9556, -1.0465, -1.4010]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.018081000074744225
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[ 0.0223,  0.6800, -0.0377,  0.3042],
        [-0.0281,  0.7026, -0.0156,  0.2692]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002245000097900629
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0060,  0.6734, -0.0266,  0.2999],
        [-0.0064,  0.7112,  0.0009,  0.2649]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002102999947965145
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.0248, -0.9771, -0.9302, -1.2889],
        [-0.9546, -1.0015, -0.9945, -1.4068]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-0.9426, -1.0006, -0.9798, -1.3262],
        [-0.9817, -1.0011, -0.9730, -1.4037]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0257, -1.0365, -1.0115, -1.2982],
        [-0.9825, -1.0139, -1.0004, -1.4116]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9815, -1.0160, -1.0130, -1.3511],
        [-0.9994, -0.9620, -1.0156, -1.4375]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.01746699959039688
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: -4.0, Discounted: -3.9009950160980225
scalar_q_values:tensor([[-0.0162,  0.7036,  0.0093,  0.3008],
        [-0.0107,  0.6873,  0.0174,  0.2812]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022869999520480633
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.9385, -0.9966, -0.9763, -1.3093],
        [-1.0053, -0.9800, -0.9713, -1.4377]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0006, -0.9906, -0.9931, -1.4098],
        [-0.9843, -1.0476, -1.0297, -1.3704]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.9656, -1.0105, -0.9761, -1.4183],
        [-0.9760, -1.0227, -0.9997, -1.3934]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9776, -0.9972, -1.0078, -1.3649],
        [-1.0052, -1.0019, -1.0008, -1.4079]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-0.9961, -0.9831, -0.9825, -1.3628],
        [-1.0275, -1.0321, -0.9642, -1.4133]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.9524, -0.9992, -0.9822, -1.3253],
        [-1.0046, -1.0102, -1.0073, -1.3977]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[-1.0114, -1.0063, -0.9906, -1.3672],
        [-1.0237, -1.0260, -0.9730, -1.4505]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.0064, -0.9952, -1.0002, -1.4110],
        [-0.9873, -1.0115, -1.0079, -1.4367]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0016, -0.9903, -0.9881, -1.4020],
        [-1.0078, -0.9975, -1.0027, -1.4313]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.0019, -0.9890, -1.0138, -1.3572],
        [-0.9266, -0.9420, -0.9949, -1.3776]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-1.0043, -0.9585, -1.0396, -1.3137],
        [-1.0213, -1.0074, -0.9794, -1.4242]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 11, Time: 0.06068500131368637
Total Reward: [  0.7 -11. ], Discounted: [  0.62673676 -10.361512  ]
Scalarized Reward: -11.0, Discounted: -10.361512184143066
scalar_q_values:tensor([[-1.0617, -1.0091, -1.0885, -1.4272],
        [-1.0446, -0.9677, -1.0977, -1.3533]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002182000083848834
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.1565, -1.0049, -1.1594, -1.4229],
        [-1.1705, -0.9790, -1.1170, -1.4469]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022249999456107616
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0740, 0.6996, 0.0826, 0.3291],
        [0.0586, 0.6352, 0.0456, 0.2434]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022319999989122152
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0290, 0.6550, 0.0402, 0.3009],
        [0.0543, 0.6802, 0.1734, 0.2444]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023089998867362738
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0968, 0.6716, 0.1474, 0.2884],
        [0.1434, 0.7135, 0.2129, 0.1815]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002104999963194132
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.1581, 0.7310, 0.2681, 0.1787],
        [0.1792, 0.7137, 0.2375, 0.1829]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020749999675899744
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.8025, -1.0122, -1.8575, -1.5958],
        [-1.9071, -1.0115, -1.8813, -1.6398]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002059000078588724
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.2890, 0.8050, 0.4728, 0.2028],
        [0.3349, 0.7563, 0.4323, 0.1005]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002013999968767166
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.2294, -1.0177, -2.2395, -1.6028],
        [-2.1854, -0.9960, -2.2261, -1.6440]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020000000949949026
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.4637, 0.6538, 0.6260, 0.1416],
        [0.4590, 0.6256, 0.6350, 0.0860]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020240000449121
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.3793, -0.9969, -2.3181, -1.7876],
        [-2.3918, -1.0024, -2.3237, -1.8132]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020129999611526728
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.3768, -0.9868, -2.3297, -1.7681],
        [-2.4048, -1.0079, -2.3041, -1.9340]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022189998999238014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.3799, -1.0419, -2.2987, -1.8530],
        [-2.3696, -1.0310, -2.2870, -1.8928]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002125999890267849
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6539, 0.6363, 0.7158, 0.1489],
        [0.6499, 0.6651, 0.7431, 0.1140]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6800, 0.6142, 0.6351, 0.1046],
        [0.7143, 0.6116, 0.6682, 0.1520]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.6530, 0.6544, 0.6164, 0.1103],
        [0.7050, 0.7023, 0.5986, 0.0518]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.6972, 0.8375, 0.6445, 0.1410],
        [0.6714, 0.7778, 0.6398, 0.0358]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.018673000857234
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6724172234535217
scalar_q_values:tensor([[0.7315, 0.8153, 0.5775, 0.1127],
        [0.7174, 0.7338, 0.5861, 0.0980]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021840000990778208
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.7994, -1.0027, -1.8629, -2.0727],
        [-1.7719, -0.9936, -1.8449, -2.1500]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.7343, -0.9833, -1.8503, -2.0541],
        [-1.7136, -0.9705, -1.8047, -2.1421]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002380999969318509
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.7374, -0.9864, -1.8244, -2.1284],
        [-1.7208, -1.0592, -1.7795, -2.1432]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.5893, 0.6429, 0.4994, 0.1125],
        [0.5770, 0.6930, 0.5244, 0.0650]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020759999752044678
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.7796, -1.0289, -1.8928, -2.1952],
        [-1.7347, -1.0161, -1.9046, -2.2293]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021979999728500843
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.8562, -1.0150, -1.9576, -2.2397],
        [-1.9070, -0.9670, -1.9127, -2.2403]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020310000982135534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.5423, 0.7162, 0.5818, 0.1231],
        [0.5434, 0.7317, 0.5550, 0.0537]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020649998914450407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0165, -1.0169, -2.0668, -2.2424],
        [-2.0539, -1.0271, -2.0432, -2.2670]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00203399988822639
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.1316, -1.0239, -2.1068, -2.2487],
        [-2.0863, -0.9689, -2.0486, -2.1831]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.5574, 0.7332, 0.5957, 0.0818],
        [0.5505, 0.6653, 0.5885, 0.0728]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021860001143068075
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.1310, -1.0189, -2.0163, -2.1847],
        [-2.1515, -0.9864, -2.0519, -2.2658]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020800000056624413
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.1374, -0.9936, -2.0820, -2.1776],
        [-2.1177, -1.0145, -1.9999, -2.1935]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6329, 0.7281, 0.6126, 0.0394],
        [0.6440, 0.7439, 0.6339, 0.0453]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020800000056624413
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0034, -0.9959, -1.9508, -2.1115],
        [-2.0341, -1.0556, -1.9219, -2.1886]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002056000055745244
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6426,  0.7070,  0.6944,  0.0789],
        [ 0.6762,  0.7040,  0.6909, -0.0288]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002088000066578388
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.8337, -1.0216, -1.9524, -2.1297],
        [-1.9139, -1.0282, -1.9235, -2.1343]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021069999784231186
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9014, -1.0112, -1.9654, -2.0932],
        [-1.8268, -0.9901, -1.9485, -2.0577]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0024770000018179417
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6908,  0.7212,  0.6585,  0.0147],
        [ 0.7118,  0.6664,  0.6421, -0.0359]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00228299992159009
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9458, -0.9730, -2.0412, -1.9844],
        [-1.8609, -1.0465, -2.0426, -2.0643]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022040000185370445
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9269, -1.0213, -2.0359, -2.0774],
        [-1.9529, -1.0014, -2.0302, -2.0705]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002104999963194132
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0091, -0.9909, -1.9656, -2.1019],
        [-2.0123, -0.9490, -2.0226, -2.0294]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002239000052213669
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.5801,  0.6521,  0.5363,  0.0188],
        [ 0.6247,  0.6909,  0.5795, -0.0073]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002177000045776367
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.5848, 0.7269, 0.5049, 0.0119],
        [0.6338, 0.6850, 0.5677, 0.0193]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6161, 0.6647, 0.5353, 0.0640],
        [0.6231, 0.6792, 0.5888, 0.0176]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021689999848604202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.6508,  0.7140,  0.5449,  0.0502],
        [ 0.6626,  0.6861,  0.6583, -0.0201]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020630001090466976
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6774, 0.7407, 0.7138, 0.0479],
        [0.6992, 0.7133, 0.7249, 0.0255]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002409999957308173
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.6857,  0.7201,  0.7123,  0.0777],
        [ 0.6775,  0.7246,  0.6879, -0.0207]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021039999555796385
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.6373,  0.6905,  0.6578,  0.0501],
        [ 0.7077,  0.6689,  0.7303, -0.0059]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.6729,  0.6783,  0.6226, -0.0374],
        [ 0.6914,  0.6642,  0.7118, -0.0045]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.6514,  0.6106,  0.6424,  0.0740],
        [ 0.6546,  0.6525,  0.6524, -0.0066]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.6123,  0.7409,  0.5780,  0.0391],
        [ 0.6833,  0.7010,  0.5878, -0.0069]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.017890000715851784
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6724172234535217
scalar_q_values:tensor([[0.6259, 0.7071, 0.5806, 0.0278],
        [0.6721, 0.6888, 0.6500, 0.0299]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002124000107869506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9757, -1.0196, -1.9681, -2.2086],
        [-1.9264, -1.0374, -1.9749, -2.0650]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020989999175071716
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0440, -0.9951, -1.9370, -2.0232],
        [-2.0252, -1.0475, -1.9835, -2.1286]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020570000633597374
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.5647,  0.7286,  0.5304,  0.0300],
        [ 0.6230,  0.6788,  0.5965, -0.0955]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021009999327361584
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.5912, 0.8005, 0.5875, 0.0509],
        [0.6322, 0.6932, 0.6110, 0.0137]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021250001154839993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.6450,  0.6599,  0.5821,  0.0985],
        [ 0.6769,  0.6599,  0.7144, -0.0348]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6590, 0.6446, 0.5981, 0.0866],
        [0.6391, 0.6844, 0.7221, 0.1259]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6419, 0.6760, 0.6858, 0.1424],
        [0.6955, 0.7195, 0.6680, 0.0361]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012822000309824944
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[0.6541, 0.7726, 0.6491, 0.0816],
        [0.7117, 0.8092, 0.7545, 0.0258]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002185000106692314
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.7054, 0.6229, 0.5801, 0.0932],
        [0.6866, 0.8194, 0.6600, 0.0322]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020840000361204147
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6052, 0.6448, 0.5745, 0.0265],
        [0.6183, 0.6390, 0.7302, 0.0841]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6492, 0.6513, 0.5591, 0.1127],
        [0.6857, 0.6709, 0.7125, 0.0467]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6170, 0.7231, 0.5239, 0.1299],
        [0.5813, 0.7867, 0.6714, 0.1196]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012392999604344368
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[0.6286, 0.7125, 0.5673, 0.1134],
        [0.6086, 0.7524, 0.5575, 0.0044]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020989999175071716
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6403, 0.6403, 0.5629, 0.1174],
        [0.6430, 0.7309, 0.6647, 0.0577]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002107999986037612
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.6262,  0.7128,  0.5804,  0.0998],
        [ 0.5948,  0.7369,  0.6175, -0.0045]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020290000829845667
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9701, -1.0473, -1.9896, -1.9984],
        [-1.9788, -1.0149, -1.9545, -2.0063]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002193999942392111
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9271, -1.0174, -1.9537, -1.9752],
        [-1.9770, -1.0141, -1.9939, -2.0678]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00203300011344254
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6272, 0.7666, 0.5320, 0.1040],
        [0.6135, 0.7106, 0.6651, 0.0452]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002486000070348382
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6567, 0.7550, 0.6058, 0.0507],
        [0.6692, 0.6594, 0.5856, 0.0022]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025390000082552433
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0203, -1.0162, -2.0102, -2.0485],
        [-2.0062, -1.0084, -1.9871, -2.1107]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002157999901100993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9920, -0.9831, -2.0089, -2.0590],
        [-2.0249, -0.9768, -1.9814, -2.0394]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021059999708086252
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9881, -1.0083, -2.0028, -2.0311],
        [-1.9704, -1.0356, -1.9769, -2.0700]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002245000097900629
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6126, 0.7147, 0.5158, 0.1237],
        [0.6370, 0.7770, 0.5600, 0.0218]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021120000164955854
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.6065,  0.7099,  0.4915,  0.0351],
        [ 0.6428,  0.6614,  0.5908, -0.0612]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021069999784231186
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6376, 0.7260, 0.4554, 0.0578],
        [0.5905, 0.6799, 0.6106, 0.0580]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002096999902278185
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0331, -0.9803, -1.9772, -2.0077],
        [-2.0050, -1.0075, -2.0076, -2.1139]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002171000000089407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6595, 0.7199, 0.5713, 0.0583],
        [0.6989, 0.7730, 0.6896, 0.0188]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020729999523609877
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.6430,  0.7145,  0.6249,  0.0528],
        [ 0.6735,  0.8017,  0.6784, -0.0971]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022869999520480633
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0144, -0.9894, -1.9797, -1.9614],
        [-1.9586, -1.0145, -1.9760, -2.0731]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020830000285059214
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6684, 0.7363, 0.5224, 0.0588],
        [0.7032, 0.6588, 0.6529, 0.0106]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021550001110881567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9662, -1.0011, -2.0130, -1.9944],
        [-2.0027, -1.0008, -1.9693, -2.0136]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002094999887049198
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6387,  0.6997,  0.5235,  0.0386],
        [ 0.6658,  0.6708,  0.6341, -0.0124]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020379999186843634
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6331, 0.7679, 0.5359, 0.0182],
        [0.6725, 0.6935, 0.5887, 0.0128]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020540000405162573
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9962, -1.0064, -2.0055, -1.9715],
        [-1.9609, -1.0509, -2.0132, -2.0766]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6344,  0.6935,  0.5130,  0.1132],
        [ 0.6337,  0.6801,  0.6458, -0.0054]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021359999664127827
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9860, -0.9607, -2.0044, -1.9736],
        [-1.9743, -0.9528, -1.9422, -2.0531]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020920000970363617
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6084,  0.6772,  0.5151,  0.0747],
        [ 0.6226,  0.6848,  0.6339, -0.0645]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002065999899059534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.5908,  0.7070,  0.6137,  0.0866],
        [ 0.6496,  0.7956,  0.6621, -0.0677]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002142000012099743
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9850, -0.9934, -1.9975, -1.9878],
        [-2.0000, -1.0906, -1.9850, -2.0794]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020959998946636915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0099, -0.9803, -2.0103, -2.0260],
        [-1.9853, -0.9753, -2.0182, -2.0709]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020669999066740274
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9967, -0.9642, -2.0225, -2.0304],
        [-1.9928, -0.9831, -2.0124, -2.0882]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6349,  0.6938,  0.4550,  0.1260],
        [ 0.6458,  0.6794,  0.6211, -0.0161]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020779999904334545
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0143, -1.0258, -1.9705, -2.0471],
        [-1.9733, -0.9979, -1.9324, -2.0616]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020409999415278435
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6419,  0.6895,  0.3593,  0.1255],
        [ 0.6508,  0.7368,  0.6067, -0.0107]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020759999752044678
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
Episode infos:
Steps: 1, Time: 0.0008500000112690032
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9776, -1.0305, -1.9354, -2.0094],
        [-2.0018, -0.9929, -1.9565, -2.1252]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020979999098926783
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0113, -1.0096, -2.0055, -2.0224],
        [-2.0050, -1.0179, -1.9791, -2.0351]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020979999098926783
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.5235,  0.6755,  0.4566,  0.0454],
        [ 0.6618,  0.6917,  0.6435, -0.1116]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002065999899059534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.5697,  0.6928,  0.4598,  0.0630],
        [ 0.6177,  0.7214,  0.7444, -0.0707]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.5657,  0.6847,  0.4509,  0.0924],
        [ 0.6708,  0.7489,  0.6666, -0.0172]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007581000216305256
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0124, -1.0006, -2.0132, -1.9701],
        [-1.9850, -1.0129, -2.0184, -2.0586]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020570000633597374
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6013,  0.7625,  0.4887,  0.1033],
        [ 0.7131,  0.7361,  0.6687, -0.1137]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024709999561309814
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.6583,  0.7474,  0.4543,  0.0421],
        [ 0.7240,  0.6920,  0.6577, -0.0778]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021929999347776175
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.6117,  0.6795,  0.3137, -0.0239],
        [ 0.7109,  0.7314,  0.6192, -0.0743]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.5697, 0.7087, 0.3702, 0.0325],
        [0.5619, 0.7389, 0.6287, 0.0145]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002247000113129616
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.5266,  0.6916,  0.4059,  0.0539],
        [ 0.6509,  0.6583,  0.6181, -0.0019]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021170000545680523
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.5410,  0.7220,  0.3754,  0.0927],
        [ 0.6047,  0.7563,  0.7466, -0.1138]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022479998879134655
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0195, -0.9741, -1.9643, -1.9786],
        [-1.9864, -1.0063, -1.9678, -2.0636]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002088000066578388
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.4776,  0.6884,  0.3661,  0.0568],
        [ 0.6394,  0.6786,  0.7126, -0.0429]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.4866,  0.7519,  0.4897,  0.0804],
        [ 0.6421,  0.5963,  0.6656, -0.0521]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007325999904423952
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9255, -1.0376, -1.9640, -1.9107],
        [-2.0114, -1.0113, -2.0216, -2.0623]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021039999555796385
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.5390,  0.6613,  0.2791,  0.0221],
        [ 0.6264,  0.8342,  0.6108, -0.0009]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020840000361204147
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9877, -0.9785, -2.0103, -1.9527],
        [-1.9306, -0.9261, -1.9805, -2.0091]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9758, -0.9978, -2.0171, -1.9597],
        [-1.9402, -0.9824, -1.9802, -2.0331]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020439999643713236
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0169, -1.0389, -1.9556, -1.9586],
        [-2.0233, -0.9936, -2.0326, -2.0568]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020959998946636915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9958, -0.9940, -1.9335, -1.9548],
        [-2.0071, -1.0121, -2.0564, -2.1056]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020810000132769346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.4887,  0.6553,  0.2411,  0.1056],
        [ 0.6255,  0.7470,  0.5653, -0.1600]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020469999872148037
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9514, -0.9986, -1.9972, -1.9254],
        [-2.0074, -0.9908, -2.0118, -2.0861]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020570000633597374
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.4654,  0.6989,  0.2451,  0.1282],
        [ 0.6520,  0.7657,  0.7151, -0.0218]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020779999904334545
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.4423,  0.6883,  0.3302,  0.0507],
        [ 0.6169,  0.6631,  0.6710, -0.0399]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021430000197142363
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9796, -1.0063, -1.9933, -1.9727],
        [-1.9688, -0.9774, -1.9814, -2.1255]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002107999986037612
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.4237,  0.8004,  0.2957,  0.1413],
        [ 0.6487,  0.8076,  0.6984, -0.0268]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0121, -1.0370, -1.9683, -1.9624],
        [-1.9935, -0.9934, -1.9501, -2.1429]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021200000774115324
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0108, -1.0272, -2.0102, -1.9917],
        [-1.9893, -0.9931, -1.9740, -2.1410]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00203000009059906
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.3911,  0.6257,  0.1251,  0.1314],
        [ 0.5714,  0.8139,  0.5995, -0.0160]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002059000078588724
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.4142, 0.6898, 0.0693, 0.1386],
        [0.6561, 0.6861, 0.5357, 0.0578]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021100000012665987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.3691,  0.7688,  0.0966,  0.1036],
        [ 0.5940,  0.7057,  0.6378, -0.0556]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002004999900236726
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0249, -0.9893, -2.0339, -1.9147],
        [-2.0023, -0.9928, -1.9880, -2.0939]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022799998987466097
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9337, -0.9669, -2.0001, -1.9224],
        [-1.9994, -1.0010, -1.9051, -2.1219]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021520000882446766
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9844, -0.9891, -2.0046, -1.9710],
        [-1.9835, -1.0277, -2.0044, -2.1139]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021800000686198473
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.3112,  0.6955,  0.0625,  0.1101],
        [ 0.6461,  0.6682,  0.7182, -0.0905]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.3026,  0.7462, -0.0491,  0.0699],
        [ 0.6461,  0.7086,  0.6014, -0.0606]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007428999990224838
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0093, -1.0245, -1.9591, -1.9717],
        [-1.9648, -0.9545, -1.9825, -2.0981]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.2682,  0.6732,  0.0223,  0.0804],
        [ 0.6627,  0.6461,  0.6604, -0.0731]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020580000709742308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.2669,  0.6896,  0.0776,  0.0223],
        [ 0.6539,  0.7465,  0.6514, -0.0524]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020759999752044678
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9186, -1.0137, -1.9006, -1.9450],
        [-2.0212, -1.0178, -1.9772, -2.1373]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002380999969318509
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9512, -1.0080, -1.9004, -1.9710],
        [-1.9785, -0.9793, -2.0204, -2.1527]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002374999923631549
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.2353,  0.7477,  0.1076,  0.0738],
        [ 0.6513,  0.8480,  0.8895, -0.1068]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.1782,  0.6828,  0.0600,  0.0632],
        [ 0.6192,  0.7487,  0.6809, -0.1069]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007563000079244375
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[ 0.1988,  0.6697,  0.0472,  0.0496],
        [ 0.5997,  0.7344,  0.6752, -0.1025]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021550001110881567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.1523,  0.6445,  0.0532,  0.0754],
        [ 0.6589,  0.7140,  0.6219, -0.0666]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002133999951183796
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0003, -0.9549, -2.0066, -2.0628],
        [-1.9962, -0.9491, -1.9871, -2.1495]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002070999937132001
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.2357,  0.6856,  0.0188,  0.0727],
        [ 0.7345,  0.7165,  0.7288, -0.0943]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.2245,  0.7333,  0.0315,  0.0283],
        [ 0.6979,  0.7412,  0.6449, -0.0875]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007600999902933836
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0117, -1.0421, -1.9858, -1.9786],
        [-2.0270, -1.0346, -1.9964, -2.1694]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002065999899059534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9936, -1.0208, -2.0035, -2.0025],
        [-1.9587, -0.9615, -1.8988, -2.0924]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002078999998047948
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.1508,  0.7364, -0.0125, -0.0054],
        [ 0.6312,  0.7159,  0.6606, -0.0649]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002019000006839633
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.1244,  0.6869,  0.0356,  0.0066],
        [ 0.6830,  0.7715,  0.6625, -0.0815]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002067999914288521
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9778, -1.0037, -1.9988, -1.9571],
        [-1.9813, -0.9984, -1.9869, -2.0809]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002090000081807375
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.1326,  0.7541,  0.1399, -0.0027],
        [ 0.7107,  0.7033,  0.7630, -0.1367]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0980,  0.6378,  0.0071, -0.0209],
        [ 0.6123,  0.7744,  0.6986, -0.1001]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007368999999016523
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[ 0.0914,  0.6540, -0.0070,  0.0106],
        [ 0.6182,  0.6800,  0.6212, -0.0501]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002041999949142337
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0942,  0.6945,  0.0252, -0.0069],
        [ 0.7444,  0.7748,  0.7336, -0.0410]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002309999894350767
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.1235,  0.7295,  0.0327, -0.0753],
        [ 0.7224,  0.7908,  0.6754, -0.1477]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002205000026151538
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0577,  0.8093, -0.0207,  0.0714],
        [ 0.6903,  0.7467,  0.6990, -0.1027]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020149999763816595
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 5.3345e-02,  6.4441e-01, -1.9927e-02, -4.0623e-05],
        [ 6.7060e-01,  7.7997e-01,  6.8328e-01, -8.4522e-02]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.001959000015631318
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.8970, -0.9692, -1.8900, -1.9417],
        [-1.9893, -0.9780, -1.9890, -2.1374]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021550001110881567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9860, -1.0327, -1.9592, -1.9965],
        [-1.9983, -0.9765, -1.9810, -2.1331]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002007999923080206
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0209, -1.0200, -1.9877, -1.9691],
        [-1.9892, -1.0234, -2.0057, -2.1588]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002319999970495701
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0024,  0.7383,  0.0101,  0.0185],
        [ 0.6670,  0.6630,  0.6570, -0.0740]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0026130001060664654
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0216,  0.6838,  0.0531, -0.0235],
        [ 0.7306,  0.6646,  0.7329, -0.0779]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0232, 0.6434, 0.0617, 0.0302],
        [0.6587, 0.6521, 0.6872, 0.0259]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0692,  0.6552,  0.0738,  0.0721],
        [ 0.7201,  0.6203,  0.6126, -0.0282]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.0319,  0.6494,  0.0069, -0.0062],
        [ 0.5950,  0.7748,  0.6640,  0.0190]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.02370299957692623
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6724172234535217
scalar_q_values:tensor([[-2.0163, -0.9995, -2.0168, -1.8999],
        [-1.9299, -0.9956, -1.9460, -2.0997]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002572000026702881
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0428,  0.7069,  0.0146, -0.0073],
        [ 0.6749,  0.6400,  0.6721, -0.0306]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002887000096961856
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0531,  0.7414,  0.0879, -0.0091],
        [ 0.6753,  0.6774,  0.6368,  0.0149]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0026569999754428864
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9915, -0.9955, -1.9643, -1.9472],
        [-1.9359, -1.0105, -1.9878, -2.1258]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0028750000055879354
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9863, -0.9928, -1.9175, -1.9600],
        [-1.9929, -1.0454, -2.0052, -2.1174]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002652999944984913
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9789, -1.0486, -2.0023, -1.8592],
        [-1.9486, -1.0180, -1.9360, -2.1476]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0027989998925477266
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0239,  0.6720,  0.0112,  0.0611],
        [ 0.7385,  0.6680,  0.6972, -0.0246]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0157, 0.6899, 0.0072, 0.0747],
        [0.6825, 0.8086, 0.6622, 0.0721]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.009371000342071056
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9779, -0.9450, -1.9719, -1.9461],
        [-2.0050, -1.0121, -2.0005, -2.1261]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002581000095233321
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0048,  0.7574, -0.0329,  0.1212],
        [ 0.6905,  0.6900,  0.6693,  0.0242]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0031789999920874834
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9688, -0.9792, -2.0088, -1.9325],
        [-1.9876, -1.0351, -2.0046, -2.1410]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002469999948516488
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9809, -0.9941, -1.9725, -1.9292],
        [-1.9976, -1.0213, -1.9978, -2.1363]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002368000103160739
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9606, -1.0440, -1.9745, -1.9220],
        [-1.9846, -0.9832, -2.0081, -2.1213]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0030739998910576105
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9328, -0.9680, -2.0187, -1.9329],
        [-2.0158, -0.9791, -1.9770, -2.0879]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002662000013515353
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9703, -0.9765, -1.9468, -1.9158],
        [-2.0185, -0.9498, -1.9635, -2.1442]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002730000065639615
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0023, -1.0138, -1.9708, -1.9223],
        [-2.0088, -0.9981, -2.0390, -2.1046]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0026579999830573797
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0094, -1.0371, -1.9572, -1.9085],
        [-2.0009, -0.9735, -2.0608, -2.0848]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023590000346302986
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0016, -0.9950, -2.0044, -1.8785],
        [-1.9754, -0.9671, -1.9775, -1.9499]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022750000935047865
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9836, -1.0032, -2.0074, -1.9281],
        [-1.9932, -0.9607, -1.9872, -2.1046]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002348999958485365
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9606, -0.9888, -1.9571, -1.9418],
        [-2.0168, -0.9999, -1.8711, -2.1202]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002830999903380871
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0050,  0.7271, -0.0128,  0.0111],
        [ 0.6739,  0.6794,  0.6718,  0.0143]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025530001148581505
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9669, -1.0132, -2.0295, -1.9080],
        [-1.9549, -1.0089, -2.0176, -2.1512]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002698000054806471
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0495, 0.6799, 0.0262, 0.0355],
        [0.7201, 0.6919, 0.7200, 0.0037]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 4.2140e-02,  6.6684e-01,  2.5731e-02,  2.5805e-02],
        [ 6.7454e-01,  6.7700e-01,  6.8092e-01, -6.3417e-04]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0156,  0.6589, -0.0125, -0.0464],
        [ 0.6464,  0.6837,  0.6519, -0.0053]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.016009999439120293
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-1.9523, -1.0086, -2.0203, -1.9553],
        [-1.9439, -1.0207, -1.9875, -2.1058]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002899999963119626
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9856, -1.0122, -2.0013, -1.9399],
        [-1.9825, -1.0242, -2.0100, -2.1537]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002718999981880188
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0269,  0.6923, -0.0112, -0.0536],
        [ 0.6616,  0.7159,  0.6615, -0.0387]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002529999939724803
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0210, 0.6918, 0.0171, 0.0376],
        [0.6913, 0.6803, 0.6726, 0.0070]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0028540000785142183
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9396, -0.9713, -2.0230, -1.9639],
        [-1.9798, -0.9987, -1.9811, -2.1184]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002606000052765012
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9571, -1.0158, -1.9580, -1.9468],
        [-1.9226, -1.0053, -1.9382, -2.0848]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.003278000047430396
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9982, -1.0005, -2.0012, -1.9683],
        [-1.9629, -1.0056, -2.0116, -2.1060]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0026030000299215317
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9711, -1.0310, -2.0114, -1.8918],
        [-2.0396, -1.0287, -1.9943, -2.0661]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0031580000650137663
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0860, -2.0980, -2.0032, -2.2406],
        [-2.1115, -2.0905, -1.9763, -2.4372]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-1.9712, -1.0274, -1.9908, -1.9727],
        [-2.0010, -0.9827, -2.0087, -2.0907]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.015752000734210014
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-0.0165,  0.7113,  0.0375,  0.0647],
        [ 0.6495,  0.6156,  0.6418,  0.0271]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0030330000445246696
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0205,  0.6955,  0.0020,  0.0274],
        [ 0.6590,  0.7561,  0.6869, -0.0159]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0025400000158697367
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9578, -1.0338, -1.9927, -1.9624],
        [-1.9623, -0.9684, -1.9469, -2.1255]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0028909998945891857
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0405, 0.6989, 0.0740, 0.0266],
        [0.7495, 0.6677, 0.7199, 0.0078]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0628,  0.7037, -0.0139,  0.0349],
        [ 0.6501,  0.7271,  0.8017,  0.1137]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0106,  0.7423, -0.0318,  0.0534],
        [ 0.6715,  0.6878,  0.6669,  0.0108]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01592399924993515
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[ 0.0604,  0.7343, -0.0015, -0.0193],
        [ 0.6632,  0.6914,  0.6621,  0.0170]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024300001095980406
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0368,  0.6559,  0.0066,  0.0374],
        [ 0.6950,  0.7638,  0.6906, -0.0438]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.003025999991223216
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0018, 0.7323, 0.0390, 0.0345],
        [0.6870, 0.7923, 0.7228, 0.0734]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0024770000018179417
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0291,  0.6547,  0.0024, -0.0071],
        [ 0.7466,  0.6495,  0.7268, -0.0689]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0570,  0.6352, -0.0087,  0.0110],
        [ 0.6661,  0.7155,  0.6514, -0.0260]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.009236999787390232
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0044, -0.9853, -1.9818, -1.9959],
        [-1.9736, -0.9487, -1.9342, -2.0445]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002624999964609742
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9983, -0.9943, -2.0008, -1.9358],
        [-2.0227, -0.9938, -1.9638, -2.0825]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0027840000111609697
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0273,  0.7107,  0.0009,  0.0255],
        [ 0.7145,  0.6834,  0.5892, -0.0040]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0831,  0.7312,  0.0327,  0.0163],
        [ 0.6967,  0.6816,  0.6066, -0.0323]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.00825599953532219
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0150, -0.9490, -2.0179, -1.9665],
        [-2.0024, -1.0090, -2.0034, -2.0967]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024139999877661467
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0444, 0.6758, 0.0419, 0.0093],
        [0.6960, 0.7043, 0.6567, 0.0393]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022780001163482666
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9961, -1.0110, -1.9966, -1.9600],
        [-1.9941, -0.9930, -1.9696, -2.1100]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0025339999701827765
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9688, -0.9918, -1.9886, -1.9711],
        [-1.9419, -1.0026, -1.9722, -2.1313]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021959999576210976
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0505, 0.7965, 0.0699, 0.0925],
        [0.5980, 0.6690, 0.6681, 0.0357]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025919999461621046
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0367, 0.6443, 0.0761, 0.0679],
        [0.6232, 0.6422, 0.7253, 0.0488]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0293, 0.6959, 0.0950, 0.0437],
        [0.6778, 0.7609, 0.7063, 0.0620]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.0074640000239014626
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0378, 0.6979, 0.0293, 0.1393],
        [0.6947, 0.7311, 0.7810, 0.0411]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0155, 0.7384, 0.0178, 0.0625],
        [0.6975, 0.6975, 0.7170, 0.0406]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.00724400021135807
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0610, 0.6551, 0.0336, 0.0762],
        [0.6913, 0.8020, 0.6547, 0.0318]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022780001163482666
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0044, -1.0022, -1.9653, -1.9958],
        [-1.9915, -1.0004, -2.0221, -2.1769]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002355000004172325
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9590, -1.0163, -2.0070, -1.9645],
        [-1.9984, -1.0202, -1.9815, -2.1160]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020779999904334545
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0086,  0.7679,  0.0375,  0.0203],
        [ 0.6636,  0.6927,  0.6513,  0.0022]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020490000024437904
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0004, -0.9262, -1.9656, -2.0112],
        [-1.9701, -0.9733, -1.9074, -2.0573]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002102999947965145
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9664, -0.9609, -2.0156, -1.9188],
        [-1.9573, -0.9761, -1.9914, -1.9836]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002070999937132001
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0043, -1.0014, -1.9880, -1.9608],
        [-2.0070, -1.0219, -1.9893, -2.0993]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021510000806301832
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0149, -1.0593, -1.9920, -2.0196],
        [-1.9624, -1.0193, -1.9868, -2.0826]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022539999336004257
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0047, -0.9705, -1.9615, -1.9313],
        [-1.9896, -1.0028, -1.9885, -2.0462]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002365000080317259
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9771, -0.9939, -1.9963, -1.9078],
        [-1.9869, -0.9863, -2.0288, -2.0729]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023610000498592854
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9749, -0.9263, -2.0501, -1.9159],
        [-1.9597, -1.0247, -1.9923, -2.0813]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002182000083848834
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9414, -1.0014, -2.0564, -1.9172],
        [-1.9876, -1.0404, -1.9916, -2.0256]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021619999315589666
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9850, -1.0156, -1.9879, -1.9789],
        [-1.9642, -1.0138, -1.9938, -2.0748]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020550000481307507
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9266, -1.0472, -1.9595, -1.8940],
        [-2.0124, -1.0109, -1.9459, -2.0866]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002022000029683113
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9705, -0.9730, -1.9986, -1.9641],
        [-1.9725, -0.9611, -1.8946, -1.9713]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002804999938234687
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0022, -1.0000, -1.9797, -1.9778],
        [-2.0058, -1.0022, -1.9948, -2.0896]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023590000346302986
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0080, -1.0110, -1.9868, -1.9941],
        [-2.0010, -1.0192, -1.9834, -2.1574]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021510000806301832
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0810, 0.7166, 0.0126, 0.0415],
        [0.6753, 0.7273, 0.6432, 0.0012]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022009999956935644
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0444,  0.6462,  0.0756, -0.0174],
        [ 0.6626,  0.6814,  0.6573, -0.0167]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021170000545680523
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0603,  0.7018,  0.0208, -0.0112],
        [ 0.6575,  0.6503,  0.7075,  0.0255]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0595, 0.6987, 0.0419, 0.0283],
        [0.6884, 0.7307, 0.7058, 0.0495]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.0077320002019405365
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[ 2.0493e-02,  7.1732e-01, -2.2773e-04,  3.5686e-02],
        [ 7.1479e-01,  7.0805e-01,  7.2811e-01,  2.4475e-02]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0486, 0.6599, 0.0233, 0.0377],
        [0.7280, 0.6834, 0.6712, 0.0127]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0129,  0.7706,  0.0096,  0.0450],
        [ 0.6744,  0.7085,  0.7387, -0.0052]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013861999846994877
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-1.9964, -0.9925, -1.9891, -1.9533],
        [-2.0394, -0.9868, -2.0069, -2.1207]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002125999890267849
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0168,  0.7683,  0.0022, -0.0099],
        [ 0.6681,  0.7133,  0.6569,  0.0188]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002088000066578388
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0078,  0.6862, -0.0200, -0.0479],
        [ 0.6967,  0.7129,  0.6114,  0.0246]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002090000081807375
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9956, -1.0056, -1.9835, -1.9765],
        [-2.0146, -0.9755, -1.9505, -1.9892]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020520000252872705
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0365,  0.7152,  0.0859, -0.0483],
        [ 0.6563,  0.6498,  0.6866, -0.0048]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002096999902278185
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0302,  0.7474,  0.0156, -0.0206],
        [ 0.6939,  0.7111,  0.5933,  0.0724]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020240000449121
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
Episode infos:
Steps: 1, Time: 0.0008820000221021473
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0051,  0.7814,  0.0031, -0.0254],
        [ 0.6656,  0.7860,  0.6744,  0.0215]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002102999947965145
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[5.2540e-05, 7.6687e-01, 1.3790e-02, 6.4859e-03],
        [6.6478e-01, 6.8470e-01, 6.5267e-01, 8.2865e-02]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021349999587982893
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0240, 0.6588, 0.0269, 0.0581],
        [0.6497, 0.6716, 0.6831, 0.0822]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0606, 0.6715, 0.0737, 0.0414],
        [0.6209, 0.7214, 0.7045, 0.0933]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.008119000121951103
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0391, 0.6787, 0.0394, 0.0356],
        [0.7018, 0.6651, 0.6973, 0.0502]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0434,  0.7350, -0.0032,  0.0279],
        [ 0.6869,  0.7492,  0.6697,  0.0480]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007656999863684177
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[ 0.0277,  0.7009,  0.0529, -0.0058],
        [ 0.6292,  0.7031,  0.6834,  0.0093]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021250001154839993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9807, -0.9937, -1.9930, -1.9473],
        [-1.9683, -0.9809, -1.9973, -2.0823]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020860000513494015
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0287,  0.6937,  0.0111, -0.0309],
        [ 0.6566,  0.7043,  0.6822,  0.0597]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020630001090466976
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0303, 0.6932, 0.0213, 0.0440],
        [0.7078, 0.6186, 0.7036, 0.0102]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.0056,  0.6880, -0.0039, -0.0063],
        [ 0.6642,  0.7161,  0.6708,  0.0240]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.0073460000567138195
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0339, 0.6686, 0.0377, 0.0120],
        [0.6686, 0.6636, 0.6983, 0.0433]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0216, 0.6965, 0.0535, 0.0067],
        [0.6689, 0.6716, 0.6415, 0.0424]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007228999864310026
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9723, -0.9942, -1.9875, -1.9609],
        [-1.9661, -0.9998, -1.9832, -2.0958]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002116000046953559
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0239,  0.7064,  0.0203,  0.0062],
        [ 0.6958,  0.7089,  0.7109, -0.0317]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.1059,  0.6913,  0.0825, -0.0302],
        [ 0.6172,  0.7189,  0.7014,  0.0111]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007430000230669975
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-0.0073,  0.7267, -0.0026, -0.0167],
        [ 0.6679,  0.6967,  0.6575,  0.0487]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023900000378489494
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0365,  0.6630,  0.0196, -0.0313],
        [ 0.6503,  0.7566,  0.7025, -0.0420]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002191999927163124
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0665, 0.6684, 0.0481, 0.0062],
        [0.7117, 0.6892, 0.7228, 0.0377]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0417,  0.7398,  0.0670, -0.0503],
        [ 0.6830,  0.6512,  0.6260,  0.0860]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007337000221014023
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0088, -1.0085, -2.0016, -1.9761],
        [-2.0008, -1.0141, -2.0093, -2.1280]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020469999872148037
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0585, 0.6674, 0.0395, 0.0102],
        [0.6644, 0.7126, 0.6557, 0.0355]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00203399988822639
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0231,  0.6688, -0.0017, -0.0186],
        [ 0.6764,  0.6507,  0.6637,  0.0200]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0115, 0.6980, 0.0171, 0.0097],
        [0.6554, 0.7337, 0.6060, 0.0111]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.0073819998651742935
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9987, -0.9824, -1.9841, -1.9983],
        [-2.0027, -0.9840, -1.9897, -2.1138]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002067999914288521
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9933, -1.0069, -1.9664, -2.0322],
        [-1.9664, -0.9842, -1.9705, -2.0969]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002213000087067485
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0434,  0.6882,  0.0448,  0.1052],
        [ 0.6906,  0.6569,  0.6823, -0.0114]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.0185,  0.6497, -0.0034, -0.0011],
        [ 0.7081,  0.6865,  0.6729, -0.0266]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0349,  0.6845,  0.0030, -0.0218],
        [ 0.6814,  0.7063,  0.6581, -0.0045]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012810000218451023
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[ 0.0066,  0.7284,  0.0115, -0.0156],
        [ 0.6987,  0.6932,  0.6753,  0.0099]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021470000501722097
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0002, -0.9890, -1.9919, -1.9536],
        [-1.9824, -1.0237, -1.9639, -2.0587]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002268000040203333
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0154, -0.9955, -2.0132, -1.9638],
        [-2.0327, -1.0324, -1.9939, -2.0387]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022150001022964716
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0261,  0.7337, -0.0155, -0.0195],
        [ 0.6384,  0.6834,  0.6627,  0.0763]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002107999986037612
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0055,  0.7032,  0.0444,  0.0451],
        [ 0.6777,  0.6165,  0.7425, -0.0260]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0131, 0.7172, 0.0429, 0.0161],
        [0.6880, 0.7005, 0.6743, 0.0493]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007400999777019024
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9887, -1.0084, -2.0289, -2.0080],
        [-1.9607, -1.0262, -2.0141, -2.0874]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002131999935954809
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0016, 0.7644, 0.0275, 0.0505],
        [0.6431, 0.7067, 0.6580, 0.1052]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022950000129640102
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0512, 0.6469, 0.0040, 0.0132],
        [0.6898, 0.6817, 0.6751, 0.0186]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0273,  0.7382, -0.0130,  0.0339],
        [ 0.6897,  0.6856,  0.6933,  0.0299]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007544999942183495
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9911, -0.9446, -1.9849, -1.9644],
        [-1.9654, -0.9905, -1.9908, -2.0828]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00216299993917346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0689, 0.6866, 0.0999, 0.0300],
        [0.7083, 0.7205, 0.7366, 0.0182]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0011,  0.7371,  0.0758, -0.0101],
        [ 0.6492,  0.7754,  0.6519,  0.0014]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007495000027120113
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9732, -1.0016, -2.0193, -1.9524],
        [-1.9925, -0.9936, -1.9808, -2.0860]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002070999937132001
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9719, -1.0206, -1.9498, -1.9576],
        [-1.9288, -1.0011, -1.9443, -2.1208]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022460001055151224
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0296,  0.7041,  0.0263,  0.0444],
        [ 0.6709,  0.7106,  0.6518,  0.0751]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002254999941214919
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0107, -1.0100, -1.9707, -1.9930],
        [-1.9972, -0.9772, -2.0036, -2.0431]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020810000132769346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0340, 0.6996, 0.0647, 0.0298],
        [0.6355, 0.6967, 0.6749, 0.0462]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021720000077039003
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0199, -1.0093, -2.0082, -1.9510],
        [-1.9438, -1.0162, -1.9867, -2.0529]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020699999295175076
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0013, -1.0202, -1.9672, -1.9156],
        [-1.9874, -0.9993, -2.0137, -2.0569]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020729999523609877
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0369, -0.9833, -2.0027, -1.9553],
        [-1.9613, -0.9765, -1.9855, -2.0045]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002472999971359968
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9142, -1.0136, -1.9132, -1.9989],
        [-1.9434, -0.9719, -2.0289, -2.0606]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020979999098926783
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9945, -1.0112, -1.9732, -1.9692],
        [-1.9628, -0.9881, -1.9780, -2.1217]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00216299993917346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9924, -1.0233, -1.9729, -1.9775],
        [-1.9303, -1.0155, -1.9966, -2.0564]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021200000774115324
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0191, 0.6844, 0.0112, 0.0754],
        [0.7089, 0.7270, 0.7023, 0.0730]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020570000633597374
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0022,  0.7207,  0.0320,  0.0813],
        [ 0.6372,  0.7479,  0.6921,  0.1276]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021379999816417694
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9867, -0.9706, -2.0123, -1.9568],
        [-1.9783, -1.0056, -2.0198, -2.0905]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020779999904334545
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9410, -0.9979, -1.9154, -1.9963],
        [-2.0178, -0.9850, -2.0129, -2.0259]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020359999034553766
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0302, 0.6752, 0.0704, 0.0370],
        [0.6546, 0.7427, 0.7262, 0.0279]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0249,  0.6848,  0.0437, -0.0181],
        [ 0.6684,  0.7932,  0.7194,  0.0075]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002070999937132001
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9891, -1.0100, -1.9693, -1.9839],
        [-1.9636, -1.0014, -2.0065, -2.0903]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021120000164955854
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9516, -1.0093, -2.0054, -1.9608],
        [-2.0019, -0.9845, -2.0000, -2.0497]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0619,  0.7055,  0.0305, -0.0059],
        [ 0.7157,  0.7163,  0.7111,  0.0086]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021279999054968357
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9754, -1.0338, -1.9701, -1.8990],
        [-2.0018, -1.0051, -2.0117, -2.0861]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.006206999998539686
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[0.0346, 0.6876, 0.0706, 0.0072],
        [0.6567, 0.7111, 0.6539, 0.0372]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021669999696314335
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0186, 0.7450, 0.0723, 0.0120],
        [0.6794, 0.6856, 0.7157, 0.0721]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020439999643713236
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0155, 0.6730, 0.0460, 0.0222],
        [0.6968, 0.6699, 0.7094, 0.0324]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0106, 0.7650, 0.0125, 0.0055],
        [0.6369, 0.6673, 0.6470, 0.0670]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007275000214576721
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[ 0.0492,  0.6802,  0.0532, -0.0226],
        [ 0.6774,  0.6758,  0.6789,  0.0448]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002061000093817711
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9847, -1.0287, -1.9925, -1.9430],
        [-1.8985, -1.0347, -2.0036, -2.0961]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002492000116035342
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9591, -1.0328, -1.9763, -1.9494],
        [-1.9042, -0.9140, -1.9868, -2.0176]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021649999544024467
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0593, 0.6992, 0.0679, 0.0169],
        [0.6798, 0.7207, 0.6559, 0.0740]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021899999119341373
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-4.1859e-02,  7.0312e-01,  7.0897e-02, -5.9581e-04],
        [ 6.1255e-01,  6.8559e-01,  6.1362e-01,  6.1551e-02]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020230000372976065
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0393, 0.6658, 0.0608, 0.0401],
        [0.6448, 0.6393, 0.6066, 0.0733]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002217999892309308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9554, -0.9725, -2.0126, -2.0206],
        [-1.9787, -0.9995, -1.9920, -2.0818]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022799998987466097
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0157,  0.7073,  0.1371, -0.0127],
        [ 0.6996,  0.7026,  0.7433,  0.0189]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0388,  0.7484,  0.0293,  0.0135],
        [ 0.6941,  0.6966,  0.6863,  0.0508]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.00737199978902936
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9794, -0.9980, -1.9951, -2.0231],
        [-1.9077, -0.9856, -1.9663, -2.1380]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021860001143068075
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9878, -0.9755, -1.9577, -2.0013],
        [-1.9957, -0.9643, -1.9749, -2.1550]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021490000654011965
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0644,  0.7002, -0.0476,  0.0060],
        [ 0.6865,  0.7054,  0.6686,  0.0693]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020830000285059214
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9932, -1.0068, -2.0081, -2.0454],
        [-1.9976, -0.9982, -2.0114, -2.1341]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020230000372976065
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9604, -0.9919, -1.9699, -1.9828],
        [-2.0001, -1.0017, -1.9941, -2.1220]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020089999306946993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9755, -0.9995, -2.0014, -1.9591],
        [-1.9830, -1.0417, -1.9887, -2.1151]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020469999872148037
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9741, -1.0102, -1.9905, -1.9669],
        [-2.0045, -0.9924, -2.0036, -2.1171]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002051000017672777
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0802, 0.6840, 0.0572, 0.0140],
        [0.6882, 0.6615, 0.7160, 0.0425]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0206,  0.7419,  0.0662,  0.0630],
        [ 0.6150,  0.6327,  0.6907,  0.0111]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007648000027984381
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[ 0.0383,  0.6921, -0.0150,  0.0038],
        [ 0.6821,  0.6972,  0.7106,  0.0538]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0271, 0.6771, 0.0068, 0.0019],
        [0.5742, 0.7248, 0.6093, 0.0307]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007532000076025724
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0053, -1.0105, -1.9511, -2.0426],
        [-1.9901, -0.9838, -1.9988, -2.1062]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002104999963194132
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0336, 0.6700, 0.0152, 0.0181],
        [0.7378, 0.6731, 0.6854, 0.0905]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0609, 0.6955, 0.0615, 0.0358],
        [0.6856, 0.7187, 0.6779, 0.1016]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007437999825924635
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0082, 0.6927, 0.0404, 0.0193],
        [0.6973, 0.6859, 0.6714, 0.0609]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0296, 0.7389, 0.0787, 0.0010],
        [0.6700, 0.6485, 0.6404, 0.0195]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007540000136941671
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9843, -0.9988, -1.9564, -1.9796],
        [-1.9563, -1.0304, -1.9282, -2.1342]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021750000305473804
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0321,  0.6222, -0.0248,  0.0428],
        [ 0.6615,  0.6799,  0.6754,  0.0734]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021200000774115324
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0074, 0.6545, 0.0141, 0.0289],
        [0.6758, 0.7455, 0.7051, 0.0516]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002128999913111329
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9980, -0.9907, -1.9963, -1.9929],
        [-1.9578, -0.9938, -1.9913, -2.1178]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021619999315589666
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0474,  0.6885, -0.0327, -0.0117],
        [ 0.7008,  0.7340,  0.6864,  0.0220]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9477, -1.0132, -1.9966, -1.9980],
        [-1.9909, -1.0015, -2.0076, -2.0988]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002426000079140067
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0597, 0.6840, 0.0156, 0.0373],
        [0.6868, 0.7199, 0.6481, 0.0156]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002179000061005354
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9949, -1.0135, -1.9734, -1.9982],
        [-1.9927, -0.9872, -1.9441, -2.1304]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0024679999332875013
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9656, -1.0152, -1.9872, -1.9811],
        [-1.9700, -0.9858, -1.9691, -2.1041]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002217999892309308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0207,  0.6748, -0.0376, -0.0050],
        [ 0.6714,  0.7301,  0.6887,  0.0345]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002391000045463443
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0113,  0.6654, -0.0067, -0.0290],
        [ 0.6705,  0.6982,  0.6999,  0.0597]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0152,  0.6964, -0.0018, -0.0148],
        [ 0.6726,  0.6603,  0.6493,  0.0584]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.008016999810934067
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9587, -0.9891, -1.9955, -1.9591],
        [-1.9690, -0.9978, -1.9944, -2.0936]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022479998879134655
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9899, -1.0022, -2.0257, -1.9988],
        [-1.9835, -0.9886, -2.0121, -2.0371]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022499999031424522
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9690, -1.0135, -1.9281, -1.9431],
        [-1.9652, -0.9862, -2.0251, -2.0140]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002334000077098608
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9678, -1.0099, -1.9836, -2.0530],
        [-1.9484, -0.9947, -1.9048, -2.0797]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021879998967051506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0127,  0.7121,  0.0280,  0.0430],
        [ 0.6856,  0.7024,  0.7029,  0.0535]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021299999207258224
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0691, 0.6857, 0.0011, 0.0110],
        [0.6449, 0.7246, 0.6733, 0.0549]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021069999784231186
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0206, -0.9708, -1.9920, -1.8105],
        [-1.9749, -0.9932, -1.9620, -2.1610]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002142000012099743
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0040, -1.0025, -1.9831, -1.9664],
        [-1.9387, -0.9850, -1.9165, -2.0834]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002104999963194132
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9628, -0.9602, -2.0166, -1.9845],
        [-2.0187, -1.0045, -1.9992, -2.0874]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002199999988079071
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0264,  0.7012,  0.0337, -0.0092],
        [ 0.6947,  0.6922,  0.6738,  0.0029]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002114000031724572
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9981, -0.9819, -2.0077, -2.0032],
        [-2.0159, -1.0023, -1.9992, -2.1182]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002102999947965145
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0706,  0.6904,  0.0784, -0.0136],
        [ 0.7044,  0.6852,  0.7034,  0.0134]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0434, 0.7067, 0.0721, 0.0391],
        [0.6906, 0.6858, 0.6728, 0.0348]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007600999902933836
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9776, -0.9867, -1.9567, -1.9889],
        [-1.9682, -1.0172, -1.9373, -2.0892]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002174000022932887
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0251, -0.9884, -1.9924, -1.9973],
        [-1.9765, -1.0146, -1.9454, -2.0517]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020860000513494015
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.1227,  0.7753,  0.1204, -0.0182],
        [ 0.6748,  0.6872,  0.6405,  0.0225]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021059999708086252
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9734, -1.0258, -1.9982, -2.0172],
        [-1.9961, -1.0135, -2.0012, -2.0664]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002090000081807375
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0045, 0.7202, 0.0317, 0.0112],
        [0.6676, 0.6995, 0.6692, 0.0529]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002237000036984682
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0399, 0.6665, 0.0419, 0.0102],
        [0.6705, 0.6859, 0.6586, 0.0488]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002111000008881092
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0719, 0.6720, 0.0255, 0.0348],
        [0.7011, 0.7575, 0.7417, 0.0535]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021730000153183937
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0049, -0.9711, -1.9873, -1.9348],
        [-1.9721, -0.9863, -2.0174, -1.9916]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022120000794529915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0515,  0.7565,  0.0734, -0.0244],
        [ 0.6369,  0.6589,  0.6519,  0.0491]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002082000020891428
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0159,  0.7077,  0.0067,  0.0308],
        [ 0.5636,  0.7701,  0.6220,  0.0572]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002145000034943223
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9763, -0.9922, -1.9955, -1.9766],
        [-1.9712, -0.9510, -1.9740, -2.1439]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002102999947965145
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0220, 0.7650, 0.0170, 0.0113],
        [0.7058, 0.7563, 0.6664, 0.0423]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9361, -0.9902, -1.9775, -2.0268],
        [-1.9589, -1.0030, -2.0447, -2.0700]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002345999935641885
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0142, 0.6994, 0.0357, 0.0186],
        [0.7311, 0.7236, 0.7266, 0.0485]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0097,  0.7559,  0.0371, -0.0365],
        [ 0.6797,  0.6838,  0.7089,  0.1003]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.00737799983471632
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0134, 0.6680, 0.0231, 0.0339],
        [0.6570, 0.7259, 0.7270, 0.0474]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0010, 0.6767, 0.0061, 0.0299],
        [0.6354, 0.6577, 0.6681, 0.0282]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007685000076889992
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0204, 0.7238, 0.0226, 0.0509],
        [0.6515, 0.6680, 0.6672, 0.0264]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023360000923275948
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9766, -0.9764, -2.0020, -1.9913],
        [-2.0329, -0.9959, -1.9908, -2.0238]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002191999927163124
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9672, -0.9596, -1.9859, -1.9453],
        [-2.0056, -0.9713, -2.0006, -2.0195]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021550001110881567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0369, 0.6837, 0.0632, 0.0035],
        [0.6659, 0.7130, 0.7141, 0.0998]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0486, 0.8026, 0.0768, 0.0100],
        [0.6635, 0.6863, 0.7034, 0.1027]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007608000189065933
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9846, -1.0096, -1.9928, -1.9784],
        [-2.0111, -0.9867, -1.9964, -2.0969]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021009999327361584
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0646,  0.6945, -0.0023,  0.0338],
        [ 0.7147,  0.6681,  0.6695,  0.1207]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0802,  0.6938, -0.0166,  0.0647],
        [ 0.5396,  0.6970,  0.7304,  0.0921]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0198,  0.6619, -0.0199,  0.0384],
        [ 0.6600,  0.6992,  0.5780,  0.1017]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.0134680001065135
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-1.9925, -1.0022, -1.9857, -1.9420],
        [-1.9874, -1.0126, -1.9961, -2.1058]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0232, 0.6804, 0.0269, 0.0486],
        [0.6906, 0.7398, 0.6864, 0.0944]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020860000513494015
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0138,  0.7794,  0.0267,  0.0806],
        [ 0.6137,  0.7488,  0.7795,  0.0363]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0185,  0.7110, -0.0150, -0.0035],
        [ 0.6629,  0.7137,  0.7070,  0.0959]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007511999923735857
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-0.0209,  0.7079,  0.0364,  0.0152],
        [ 0.6943,  0.6959,  0.6904,  0.0845]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002078999998047948
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0291,  0.7214,  0.0125, -0.0140],
        [ 0.6500,  0.7070,  0.6211,  0.0497]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002050000010058284
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9767, -0.9867, -1.9872, -1.9910],
        [-1.9827, -1.0179, -1.9957, -2.0575]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9553, -1.0170, -1.9851, -1.9943],
        [-1.9855, -1.0292, -1.9809, -2.0626]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002061000093817711
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9934, -0.9854, -2.0054, -1.9614],
        [-2.0030, -0.9820, -1.9957, -2.0684]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020649998914450407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9708, -0.9834, -2.0288, -1.9617],
        [-2.0243, -0.9942, -1.9731, -2.0409]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00204299995675683
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0118,  0.7216,  0.0156,  0.0792],
        [ 0.6815,  0.7298,  0.6209,  0.1267]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002073999959975481
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9857, -1.0102, -1.9648, -2.0071],
        [-2.0055, -0.9883, -1.9843, -2.0655]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.006118000019341707
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: -2.0, Discounted: -1.9701000452041626
scalar_q_values:tensor([[-1.9707, -0.9831, -1.9745, -1.9795],
        [-1.9735, -0.9948, -1.9889, -2.0827]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020620001014322042
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0102, 0.6711, 0.0178, 0.0413],
        [0.6660, 0.6673, 0.6319, 0.0675]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002125999890267849
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0076, 0.6926, 0.0133, 0.0228],
        [0.6771, 0.6887, 0.6582, 0.0682]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020439999643713236
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0340, -1.0443, -1.9711, -1.9808],
        [-1.9893, -1.0643, -1.9887, -2.0669]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021299999207258224
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0493,  0.7302, -0.0166,  0.0085],
        [ 0.6844,  0.7318,  0.6674,  0.0284]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020169999916106462
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0342,  0.6691,  0.0184, -0.0249],
        [ 0.7059,  0.7240,  0.6709,  0.0703]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002047999994829297
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9903, -1.0001, -1.9763, -2.0179],
        [-1.9824, -0.9361, -1.9921, -2.0648]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0441, -2.0601, -1.9439, -2.2169],
        [-2.1223, -2.1294, -1.9579, -2.4646]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.9815, -0.9946, -1.9650, -1.9814],
        [-1.9881, -0.9933, -1.9800, -2.0692]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.011943000368773937
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-1.9656, -1.0167, -1.9859, -1.9570],
        [-1.9974, -1.0125, -1.9522, -2.0723]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021750000305473804
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9801, -0.9721, -1.9538, -2.0234],
        [-1.9841, -0.9540, -1.9372, -2.0961]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002472999971359968
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0170,  0.7768,  0.0341, -0.0085],
        [ 0.6860,  0.7486,  0.7196,  0.0322]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002389000030234456
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9867, -1.0027, -2.0006, -1.9823],
        [-2.0192, -0.9882, -1.9732, -2.0733]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002182000083848834
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.8993, -0.9661, -2.0009, -1.8788],
        [-2.0017, -0.9841, -1.9937, -2.0900]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021520000882446766
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9960, -1.0341, -2.0006, -2.0158],
        [-1.9494, -0.9849, -1.9580, -2.0425]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002111000008881092
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0312,  0.6914, -0.0292, -0.0299],
        [ 0.6737,  0.6855,  0.6567,  0.0446]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002102999947965145
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0221, -0.9914, -2.0094, -1.9927],
        [-1.9715, -0.9878, -1.9988, -2.0542]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022430000826716423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9554, -1.0101, -1.9746, -2.0012],
        [-1.9326, -0.9656, -1.9693, -2.0785]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021269998978823423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0288, 0.7456, 0.0316, 0.0313],
        [0.6897, 0.6653, 0.6883, 0.0515]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021830000914633274
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0061, -1.0057, -1.9804, -2.0007],
        [-1.9945, -0.9905, -1.9573, -2.0669]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021609999239444733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0163, -1.0007, -1.9597, -2.0485],
        [-1.9884, -0.9960, -2.0196, -2.0437]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021440000273287296
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0099, 0.7140, 0.0588, 0.0122],
        [0.6538, 0.7203, 0.6595, 0.0515]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002211000071838498
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9557, -0.9675, -1.9860, -2.0359],
        [-2.0092, -0.9932, -1.9890, -2.0625]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002251999918371439
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[1.6302e-02, 6.6783e-01, 2.6403e-04, 7.5848e-02],
        [6.0335e-01, 7.8087e-01, 6.4504e-01, 7.2166e-02]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020459999796003103
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0312, 0.6770, 0.0452, 0.0326],
        [0.6370, 0.7382, 0.6414, 0.0752]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021720000077039003
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0072,  0.7608,  0.0352,  0.0157],
        [ 0.6971,  0.6687,  0.6503,  0.0625]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020830000285059214
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9951, -0.9991, -2.0070, -2.0040],
        [-1.9898, -1.0381, -2.0051, -2.0545]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002124000107869506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0098, -1.0170, -1.9555, -2.0834],
        [-1.9992, -1.0087, -1.9887, -2.0843]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022269999608397484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9849, -1.0226, -1.9751, -1.9715],
        [-2.0105, -0.9954, -1.9469, -2.1132]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0126, -1.0248, -1.9786, -2.0312],
        [-1.9881, -0.9992, -1.9760, -2.0914]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0487, 0.7420, 0.0341, 0.0565],
        [0.7005, 0.6670, 0.6857, 0.0663]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021689999848604202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9706, -0.9692, -1.9696, -2.0208],
        [-1.9953, -0.9948, -1.9762, -2.1077]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023330000694841146
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9499, -0.9841, -1.9820, -2.0119],
        [-2.0009, -1.0056, -1.9951, -2.1167]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002266000024974346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0165,  0.6903,  0.0299, -0.0069],
        [ 0.6560,  0.6618,  0.6727,  0.0538]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9693, -0.9864, -2.0528, -1.9261],
        [-1.9944, -0.9938, -2.0302, -2.0596]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021250001154839993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9743, -1.0047, -2.0207, -2.0102],
        [-1.9749, -0.9771, -1.9873, -2.0926]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002219999907538295
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9933, -0.9577, -1.9992, -2.0042],
        [-1.9729, -1.0029, -2.0185, -2.0353]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021470000501722097
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0543, 0.7302, 0.0429, 0.0315],
        [0.7059, 0.7480, 0.7396, 0.0689]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022819999139755964
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0243, -0.9833, -1.9707, -2.0348],
        [-1.9970, -0.9953, -2.0016, -2.1502]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002096999902278185
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9819, -0.9376, -1.9689, -2.0067],
        [-1.9947, -1.0108, -1.9734, -2.0861]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002128999913111329
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0525,  0.7305, -0.0051,  0.0164],
        [ 0.6406,  0.7419,  0.6241,  0.0275]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022090000566095114
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0452,  0.7091,  0.0273,  0.0136],
        [ 0.5990,  0.6863,  0.5573, -0.0327]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021120000164955854
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0049, -1.0102, -1.9927, -2.0081],
        [-1.9959, -0.9866, -2.0014, -2.1015]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021100000012665987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9806, -1.0063, -2.0050, -2.0008],
        [-1.9730, -0.9926, -2.0128, -2.0941]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002365000080317259
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0309, 0.6842, 0.0572, 0.0268],
        [0.6655, 0.6852, 0.6782, 0.0156]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021470000501722097
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9926, -1.0209, -1.9334, -1.9733],
        [-2.0147, -1.0132, -1.9898, -2.1430]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023159999400377274
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9993, -1.0347, -1.9436, -1.9902],
        [-1.9818, -1.0015, -1.9689, -2.0447]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002145000034943223
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9509, -0.9840, -1.9723, -1.9923],
        [-1.9883, -0.9356, -1.9610, -2.0861]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021089999936521053
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0077, 0.6868, 0.0237, 0.0503],
        [0.6829, 0.6811, 0.6923, 0.0431]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0175,  0.7550, -0.0096,  0.0324],
        [ 0.7231,  0.7020,  0.6773,  0.0365]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.0077309999614953995
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-0.0247,  0.7211, -0.0114, -0.0319],
        [ 0.6932,  0.7222,  0.6797,  0.0282]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021389999892562628
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0078,  0.7796, -0.0040,  0.0282],
        [ 0.7080,  0.7103,  0.6748,  0.0260]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020860000513494015
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0147, -0.9741, -1.9632, -1.9937],
        [-2.0183, -0.9818, -1.9739, -2.1042]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0032,  0.6956, -0.0097, -0.0243],
        [ 0.6867,  0.6673,  0.7026,  0.0554]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0238,  0.6408, -0.0128, -0.0077],
        [ 0.6622,  0.7150,  0.6125,  0.0071]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007449000142514706
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-0.0021,  0.7482, -0.0072, -0.0144],
        [ 0.6681,  0.8078,  0.7318,  0.0319]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00216599996201694
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0167,  0.6956,  0.0274,  0.0054],
        [ 0.6883,  0.7166,  0.7369,  0.0370]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0204,  0.6990,  0.0424,  0.0204],
        [ 0.6247,  0.7138,  0.6752,  0.0559]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007511000148952007
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0180, 0.7066, 0.0146, 0.0810],
        [0.6672, 0.6905, 0.6693, 0.0611]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021230001002550125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0026, -0.9948, -2.0063, -1.9844],
        [-1.9671, -0.9749, -2.0019, -2.0690]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002179000061005354
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0715, 0.6845, 0.0503, 0.0377],
        [0.6642, 0.7243, 0.6549, 0.0612]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002242000075057149
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0222,  0.7032,  0.0075, -0.0252],
        [ 0.6771,  0.7858,  0.6388,  0.0789]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021250001154839993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9975, -0.9600, -1.9619, -1.9700],
        [-1.9635, -0.9932, -1.9808, -2.0843]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9933, -0.9913, -1.9633, -1.9767],
        [-1.9679, -0.9474, -1.9827, -2.0471]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021689999848604202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9900, -0.9815, -2.0408, -1.9708],
        [-1.9975, -0.9754, -1.9981, -2.0886]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002148000057786703
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9855, -1.0140, -1.9935, -1.9485],
        [-1.9980, -0.9954, -1.9927, -2.0809]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002348999958485365
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9973, -1.0114, -1.9850, -1.9643],
        [-1.9557, -0.9730, -1.9887, -2.1050]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002368000103160739
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0538,  0.6770,  0.0099, -0.0242],
        [ 0.6337,  0.6547,  0.7257,  0.0316]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0576,  0.6932,  0.0196, -0.0288],
        [ 0.6584,  0.6935,  0.6989,  0.0254]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0337,  0.7200, -0.0176,  0.0166],
        [ 0.6195,  0.7203,  0.6022,  0.0555]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01286999974399805
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[ 0.0149,  0.8350,  0.0269, -0.0378],
        [ 0.7475,  0.6628,  0.6962, -0.0651]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022040000185370445
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0103, -0.9814, -1.9775, -1.9377],
        [-2.0073, -1.0241, -2.0050, -2.1244]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021299999207258224
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9956, -0.9881, -1.9656, -1.9735],
        [-1.9684, -0.9913, -2.0151, -2.0650]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002205000026151538
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0089, -0.9836, -2.0234, -1.9720],
        [-2.0294, -0.9827, -1.9837, -1.9768]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022539999336004257
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9398, -0.9684, -1.9997, -1.9705],
        [-1.9997, -1.0081, -1.9673, -2.1340]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002125999890267849
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9869, -0.9267, -1.9355, -1.9054],
        [-2.0036, -1.0396, -1.9875, -2.1267]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002420000033453107
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9817, -0.9939, -1.9729, -1.9787],
        [-2.0160, -1.0429, -1.9435, -2.0745]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021989999804645777
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9948, -0.9919, -2.0039, -1.9705],
        [-1.9698, -1.0149, -2.0071, -2.0242]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021589999087154865
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9598, -0.9862, -1.9790, -2.0080],
        [-1.9769, -0.9958, -1.9807, -2.0686]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023819999769330025
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0140,  0.6782,  0.0385, -0.0357],
        [ 0.7278,  0.6504,  0.6745,  0.0315]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0277, 0.6738, 0.0791, 0.0326],
        [0.6396, 0.7638, 0.7655, 0.0230]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0349, 0.6732, 0.0624, 0.0281],
        [0.7228, 0.7372, 0.6493, 0.0513]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013406000100076199
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-2.0071, -1.0147, -2.0402, -1.8499],
        [-1.9870, -1.0295, -2.0079, -2.0583]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021899999119341373
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0119, 0.6396, 0.0485, 0.0785],
        [0.6444, 0.6812, 0.6588, 0.0220]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0298, 0.7003, 0.0474, 0.0293],
        [0.6651, 0.6945, 0.6596, 0.0559]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0241,  0.7365, -0.0267,  0.0427],
        [ 0.6703,  0.7135,  0.6247,  0.0508]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021810000762343407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0206, 0.7026, 0.0359, 0.0371],
        [0.6845, 0.6979, 0.6686, 0.0532]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021440000273287296
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9973, -1.0273, -1.9789, -1.9993],
        [-1.9661, -1.0091, -1.9919, -2.0562]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0058, 0.6763, 0.0534, 0.0307],
        [0.6616, 0.7130, 0.7215, 0.0235]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0492,  0.6362, -0.0291, -0.0691],
        [ 0.6815,  0.6590,  0.6976,  0.0057]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0398,  0.6770,  0.0130, -0.0115],
        [ 0.6997,  0.6584,  0.7235,  0.1037]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0347, 0.6907, 0.0462, 0.0361],
        [0.6532, 0.7682, 0.6756, 0.0132]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.01828799955546856
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6724172234535217
scalar_q_values:tensor([[-2.0020, -1.0045, -2.0034, -2.0154],
        [-1.9925, -0.9932, -2.0067, -2.1009]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021359999664127827
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9839, -0.9704, -1.9611, -1.9275],
        [-1.9812, -0.9981, -2.0416, -2.0686]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021510000806301832
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9746, -0.9709, -1.9676, -2.0051],
        [-1.9647, -0.9989, -1.9558, -2.1032]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023640000727027655
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0025, 0.7452, 0.0495, 0.0297],
        [0.6849, 0.6603, 0.6636, 0.0823]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023620000574737787
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9884, -0.9983, -1.9906, -1.9967],
        [-1.9909, -0.9876, -1.9984, -2.1115]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002500999951735139
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0003, -0.9975, -1.9894, -2.0296],
        [-1.9678, -0.9585, -1.9942, -2.1059]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002234000014141202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0304, 0.6902, 0.0245, 0.0386],
        [0.6062, 0.7052, 0.7266, 0.0977]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0566,  0.6810,  0.0040, -0.0066],
        [ 0.6954,  0.7931,  0.6671,  0.1013]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007954999804496765
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[ 0.0407,  0.6952,  0.0587,  0.0425],
        [ 0.6794,  0.7568,  0.5777, -0.0073]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021609999239444733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0020, -1.0092, -2.0026, -1.9955],
        [-1.9929, -1.0462, -1.9121, -2.0815]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002136999974027276
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0041, 0.7722, 0.0633, 0.0476],
        [0.6999, 0.6963, 0.6663, 0.0213]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022239999379962683
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9647, -0.9644, -1.9769, -1.9817],
        [-1.9620, -0.9758, -2.0185, -2.0847]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002242000075057149
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0116, -0.9833, -1.9628, -2.0176],
        [-1.9541, -0.9965, -2.0167, -2.0966]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023129999171942472
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0392,  0.7400,  0.0177, -0.0151],
        [ 0.6347,  0.7087,  0.6890,  0.0095]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021879998967051506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9941, -1.0124, -1.9978, -1.9850],
        [-1.9870, -1.0373, -1.9983, -2.0874]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023089998867362738
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0403,  0.6906,  0.0342,  0.0052],
        [ 0.6660,  0.6916,  0.6743, -0.0037]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002409999957308173
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0322, -0.9923, -1.9891, -2.0007],
        [-1.9694, -0.9780, -2.0307, -2.0665]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022189998999238014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0362,  0.6800,  0.0259, -0.0243],
        [ 0.6878,  0.6677,  0.7145,  0.0523]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0422,  0.6813, -0.0690, -0.0352],
        [ 0.6906,  0.6860,  0.6831,  0.0171]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0343,  0.6775,  0.0476, -0.0323],
        [ 0.6835,  0.6924,  0.6572,  0.0340]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013903999701142311
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[ 4.2381e-04,  6.8378e-01, -1.5049e-02, -3.7853e-02],
        [ 6.8599e-01,  6.9055e-01,  6.9141e-01,  4.7116e-02]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0181,  0.7352, -0.0333, -0.0746],
        [ 0.5887,  0.6624,  0.7202,  0.0280]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.00776899978518486
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0007, -1.0015, -2.0102, -2.0326],
        [-1.9998, -0.9691, -1.9667, -2.0862]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021460000425577164
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9757, -1.0018, -2.0070, -1.9880],
        [-1.9832, -1.0063, -2.0053, -2.0820]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022140000946819782
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0509,  0.7033, -0.0066, -0.0803],
        [ 0.6246,  0.7748,  0.6827,  0.0642]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021520000882446766
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0254,  0.6927,  0.0496,  0.0326],
        [ 0.6956,  0.6952,  0.6903, -0.0040]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0118,  0.6938,  0.0119, -0.0039],
        [ 0.6479,  0.6850,  0.7077,  0.0649]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0424,  0.6963,  0.0432, -0.0404],
        [ 0.6350,  0.7520,  0.6744,  0.0291]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01343700010329485
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-2.0058, -1.0122, -2.0031, -1.9808],
        [-2.0147, -0.9893, -2.0022, -2.0797]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002136999974027276
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0315, -1.0163, -1.9759, -1.9498],
        [-1.9887, -1.0073, -1.9944, -2.0526]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021589999087154865
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0312,  0.6878,  0.0106, -0.0587],
        [ 0.6607,  0.7091,  0.6234,  0.0693]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0068,  0.6952, -0.0019, -0.0289],
        [ 0.6725,  0.6397,  0.5963,  0.0172]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00215999991632998
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0279,  0.6487, -0.0104, -0.0200],
        [ 0.7100,  0.7045,  0.6697,  0.0578]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0492,  0.7487,  0.0782, -0.0513],
        [ 0.7132,  0.6788,  0.6936,  0.0726]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007344000041484833
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9237, -0.9974, -2.0142, -1.9379],
        [-1.9759, -1.0138, -1.9593, -2.0350]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021520000882446766
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9636, -1.0005, -1.9793, -1.9540],
        [-2.0044, -1.0090, -2.0004, -2.0695]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022009999956935644
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0318,  0.7097,  0.0553, -0.0165],
        [ 0.6710,  0.7364,  0.6703,  0.0517]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0113,  0.7557,  0.0407,  0.0321],
        [ 0.6742,  0.6736,  0.6390,  0.0030]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002174000022932887
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9965, -0.9853, -1.9857, -1.9810],
        [-1.9804, -0.9472, -1.9488, -2.0642]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020630001090466976
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9962, -1.0006, -1.9842, -1.9601],
        [-2.0341, -1.0140, -1.9544, -2.0418]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025839998852461576
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9800, -0.9932, -1.9958, -1.9674],
        [-1.9807, -1.0140, -1.9972, -2.0687]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021720000077039003
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9985, -0.9800, -2.0022, -1.9986],
        [-1.9692, -0.9930, -1.9915, -2.0818]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002125999890267849
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9829, -1.0074, -2.0100, -1.9790],
        [-1.9973, -0.9938, -1.9928, -2.0904]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021800000686198473
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9820, -0.9827, -1.9869, -2.0062],
        [-1.9926, -0.9698, -1.9857, -2.0293]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021899999119341373
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0445,  0.6646,  0.0048, -0.0605],
        [ 0.6294,  0.6633,  0.6524, -0.0017]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021609999239444733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0191,  0.7358,  0.0899, -0.0190],
        [ 0.6809,  0.6872,  0.6840,  0.0593]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021649999544024467
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0290, 0.6820, 0.0671, 0.0133],
        [0.6803, 0.7252, 0.7360, 0.0550]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0221, 0.6880, 0.0207, 0.0008],
        [0.7147, 0.7091, 0.7416, 0.0882]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0166, 0.7022, 0.0240, 0.0075],
        [0.6686, 0.7614, 0.6635, 0.0427]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013171999715268612
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-1.9913, -0.9945, -1.9805, -1.9938],
        [-1.9984, -0.9591, -1.9696, -2.0247]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022070000413805246
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0629,  0.6765,  0.0017,  0.0697],
        [ 0.6335,  0.7021,  0.6774,  0.0516]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023670000955462456
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0172,  0.7174,  0.0089,  0.0077],
        [ 0.6388,  0.6849,  0.6273,  0.0722]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021569998934865
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9827, -1.0378, -2.0178, -1.9773],
        [-1.9762, -1.0015, -1.9949, -2.0731]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002174000022932887
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0033, -1.0085, -2.0070, -1.9709],
        [-1.9753, -1.0209, -1.9843, -2.1182]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002219999907538295
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0386, 0.7127, 0.0666, 0.0149],
        [0.7207, 0.6868, 0.7117, 0.0848]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0545, 0.7231, 0.1066, 0.0493],
        [0.6995, 0.6321, 0.6801, 0.0470]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007646999787539244
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0059, -1.0139, -2.0001, -2.0223],
        [-1.9977, -0.9998, -1.9359, -2.0629]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 4.4159e-02,  7.9492e-01, -2.2417e-04,  5.9832e-02],
        [ 6.9373e-01,  7.4576e-01,  6.7544e-01,  9.7035e-02]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024949999060481787
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0125, 0.7099, 0.0163, 0.0547],
        [0.6551, 0.7081, 0.6449, 0.0759]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002191999927163124
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0068, -1.0094, -1.9660, -2.0645],
        [-1.9914, -0.9934, -2.0111, -2.0950]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0412, 0.7256, 0.0105, 0.0273],
        [0.6618, 0.6719, 0.6455, 0.0558]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002463999902829528
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9858, -1.0211, -1.9977, -2.0000],
        [-1.9643, -1.0376, -2.0222, -2.1254]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022929999977350235
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0140, 0.6816, 0.0215, 0.0281],
        [0.6645, 0.6824, 0.6423, 0.1086]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022350000217556953
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0098, -0.9804, -2.0188, -2.0105],
        [-2.0083, -1.0045, -1.9375, -2.1016]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0026859999634325504
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9602, -0.9831, -1.9231, -2.1162],
        [-1.9584, -1.0176, -1.9618, -2.1048]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021440000273287296
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0288, 0.6680, 0.1103, 0.0441],
        [0.7237, 0.7044, 0.7018, 0.0254]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.0027,  0.7243,  0.0456, -0.0022],
        [ 0.7033,  0.7469,  0.6756,  0.0637]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007639000192284584
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9947, -0.9893, -2.0126, -2.0594],
        [-1.9997, -0.9928, -1.9802, -2.1273]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021210000850260258
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9770, -1.0079, -1.9986, -1.9761],
        [-2.0000, -1.0072, -1.9924, -2.0749]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021379999816417694
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9342, -0.9230, -2.0059, -1.9397],
        [-1.9569, -0.9922, -2.0092, -2.0205]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021269998978823423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0182,  0.6405,  0.0305, -0.0647],
        [ 0.6888,  0.7094,  0.7052,  0.0593]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002191999927163124
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0255,  0.7183,  0.0065, -0.0177],
        [ 0.6832,  0.7354,  0.7223,  0.0148]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002196999965235591
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0135, 0.6652, 0.0221, 0.0291],
        [0.6630, 0.7342, 0.7180, 0.0554]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021879998967051506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0408, 0.6985, 0.0147, 0.0027],
        [0.6629, 0.7400, 0.6886, 0.0783]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021550001110881567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0661,  0.6606, -0.0092,  0.0114],
        [ 0.6776,  0.6682,  0.6665,  0.0618]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0361,  0.6916,  0.0154, -0.0032],
        [ 0.6562,  0.6964,  0.6526,  0.0622]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.00785099994391203
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9849, -1.0026, -2.0249, -1.9894],
        [-1.9967, -0.9867, -1.9792, -2.0369]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021269998978823423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9937, -0.9902, -1.9894, -2.0154],
        [-2.0207, -0.9843, -2.0151, -2.0795]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021669999696314335
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0127, 0.6786, 0.0216, 0.0219],
        [0.6908, 0.7663, 0.6685, 0.0066]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021379999816417694
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9792, -1.0060, -1.9842, -1.9902],
        [-1.9985, -1.0016, -2.0088, -2.0924]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021250001154839993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9796, -0.9991, -1.9981, -1.9940],
        [-1.9786, -0.9970, -1.9987, -2.1011]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9868, -1.0066, -1.9859, -1.9916],
        [-1.9826, -0.9904, -1.9806, -2.0612]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022140000946819782
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9657, -0.9985, -1.9600, -1.9577],
        [-1.9908, -1.0071, -1.9529, -2.0398]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002406999934464693
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0100, -1.0021, -1.9694, -2.0168],
        [-1.9989, -1.0045, -1.9894, -2.0954]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021440000273287296
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9890, -0.9838, -1.9557, -1.9923],
        [-1.9810, -0.9553, -2.0019, -2.0762]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021879998967051506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0151,  0.7542,  0.0469,  0.0262],
        [ 0.7346,  0.6302,  0.6705,  0.0369]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002239000052213669
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9900, -0.9962, -1.9926, -2.0060],
        [-1.9800, -0.9890, -1.9958, -2.1051]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021269998978823423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0614,  0.6586,  0.0343, -0.0083],
        [ 0.6880,  0.7276,  0.6793,  0.1071]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022720000706613064
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9489, -0.9928, -1.9933, -1.9760],
        [-2.0017, -1.0210, -2.0114, -2.1084]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002107999986037612
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0394,  0.7048,  0.0312,  0.0479],
        [ 0.6664,  0.6984,  0.7227,  0.0785]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0177,  0.6999, -0.0119,  0.0220],
        [ 0.6616,  0.6969,  0.7169,  0.0677]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0168,  0.7053, -0.0123, -0.0475],
        [ 0.6576,  0.6872,  0.6925,  0.0482]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01398099958896637
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-2.0000, -1.0310, -1.9935, -1.9708],
        [-1.9991, -1.0047, -1.9661, -2.0922]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022849999368190765
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0004, -1.0023, -2.0028, -1.9931],
        [-1.9926, -1.0138, -2.0015, -2.0835]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00203699991106987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0092, -1.0131, -2.0245, -1.9926],
        [-1.9924, -0.9888, -2.0056, -2.0848]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002078999998047948
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9859, -1.0022, -2.0057, -1.9916],
        [-1.9812, -0.9933, -2.0339, -2.0893]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021780000533908606
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9888, -0.9992, -2.0131, -1.9888],
        [-1.9748, -0.9983, -1.9766, -2.0921]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021860001143068075
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0182,  0.6645, -0.0089,  0.0370],
        [ 0.6951,  0.6879,  0.6165,  0.0103]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0161,  0.6827, -0.0131, -0.0396],
        [ 0.6467,  0.7073,  0.6641,  0.0242]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.008080000057816505
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0196, 0.6051, 0.0109, 0.0252],
        [0.6963, 0.6281, 0.6660, 0.0294]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0174, 0.6486, 0.0264, 0.0165],
        [0.6920, 0.6810, 0.6802, 0.0207]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0011, 0.6589, 0.0226, 0.0842],
        [0.6971, 0.7316, 0.6908, 0.0537]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013698999769985676
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-1.9935, -0.9725, -2.0131, -1.9915],
        [-1.9945, -1.0014, -1.9744, -2.1359]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021510000806301832
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0199, 0.7593, 0.0338, 0.0182],
        [0.6995, 0.7129, 0.6872, 0.0556]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021879998967051506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0436, 0.7478, 0.0267, 0.0088],
        [0.6180, 0.7024, 0.6448, 0.1245]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021800000686198473
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9676, -0.9594, -1.9480, -2.0167],
        [-1.9774, -0.9976, -1.9926, -2.0577]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00215000007301569
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9758, -1.0001, -2.0028, -1.9911],
        [-1.9910, -1.0033, -1.9540, -1.9795]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002174000022932887
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0055, 0.6939, 0.0602, 0.0256],
        [0.7127, 0.7752, 0.6858, 0.1044]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0103, 0.7440, 0.0863, 0.0323],
        [0.6666, 0.7009, 0.6980, 0.0778]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002194999950006604
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0142, 0.7667, 0.0775, 0.0293],
        [0.6738, 0.7183, 0.6632, 0.0604]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0286, 0.7345, 0.0375, 0.0237],
        [0.6669, 0.6989, 0.6294, 0.0878]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020469999872148037
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9959, -1.0125, -1.9873, -1.9651],
        [-2.0002, -1.0006, -1.9649, -2.0603]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9895, -0.9736, -1.9933, -1.9750],
        [-2.0225, -0.9949, -1.9818, -2.0608]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020540000405162573
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0054,  0.6462, -0.0080,  0.0215],
        [ 0.6785,  0.6477,  0.6593,  0.0197]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0695, 0.6731, 0.0632, 0.0564],
        [0.7079, 0.6226, 0.6094, 0.0447]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0155, 0.7780, 0.0543, 0.0699],
        [0.6794, 0.7131, 0.7092, 0.0376]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013252000324428082
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-1.9792, -0.9806, -2.0014, -2.0100],
        [-2.0077, -0.9900, -1.9892, -2.0647]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021860001143068075
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0595,  0.7103,  0.0825, -0.0555],
        [ 0.6317,  0.8134,  0.7400,  0.0725]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002219999907538295
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0065,  0.6896,  0.0233,  0.0011],
        [ 0.6559,  0.7124,  0.7029, -0.0302]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002266000024974346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 4.0585e-02,  6.4295e-01,  6.2139e-04, -2.4384e-03],
        [ 6.7927e-01,  6.6003e-01,  6.8942e-01,  4.3857e-02]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0418,  0.6881, -0.0277, -0.0558],
        [ 0.6547,  0.6135,  0.6689,  0.0267]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007594999857246876
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9815, -1.0040, -1.9834, -1.9481],
        [-1.9716, -0.9884, -1.9691, -2.0790]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021689999848604202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0757,  0.7778,  0.0487, -0.0575],
        [ 0.7343,  0.7235,  0.6630,  0.0508]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002203000010922551
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.1149, 0.8307, 0.0116, 0.0417],
        [0.7231, 0.7826, 0.6874, 0.0091]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00216299993917346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0684,  0.6875, -0.0142,  0.0524],
        [ 0.7445,  0.7772,  0.7264,  0.0175]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023169999476522207
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9585, -0.9693, -1.9523, -1.9166],
        [-1.9898, -0.9850, -2.0225, -2.0772]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022239999379962683
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0042, -0.9750, -1.9710, -1.9632],
        [-1.9933, -0.9753, -1.9914, -2.0553]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022140000946819782
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9752, -1.0059, -1.9949, -1.9812],
        [-1.9975, -1.0010, -1.9907, -2.0222]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023040000814944506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0016, -1.0243, -1.9632, -2.0461],
        [-1.9903, -1.0271, -2.0214, -2.0141]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00215000007301569
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0166, 0.7106, 0.0108, 0.0328],
        [0.5734, 0.7249, 0.6684, 0.0677]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020639998838305473
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0118, -1.0287, -1.9648, -1.9763],
        [-1.9850, -0.9808, -2.0016, -2.0873]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0137, 0.7135, 0.0376, 0.0356],
        [0.5962, 0.6832, 0.6406, 0.0813]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020959998946636915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0190, -1.0248, -2.0029, -2.0123],
        [-1.9475, -1.0070, -2.0307, -2.0827]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0818,  0.7163,  0.0472, -0.0077],
        [ 0.7010,  0.6632,  0.7025,  0.0762]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021689999848604202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9325, -0.9888, -1.9635, -1.9691],
        [-1.9888, -0.9950, -1.9584, -2.0688]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021879998967051506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0027, -0.9408, -1.9179, -1.9781],
        [-1.9760, -0.9876, -1.9712, -2.0521]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021689999848604202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0827,  0.7193, -0.0124,  0.0589],
        [ 0.7195,  0.6944,  0.6683,  0.1082]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0972,  0.6550, -0.0231,  0.0905],
        [ 0.6661,  0.6976,  0.6719,  0.0870]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007501999847590923
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9931, -0.9889, -1.9368, -1.9719],
        [-1.9806, -0.9892, -1.9802, -2.0237]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002234000014141202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9750, -0.9663, -1.9563, -2.0271],
        [-2.0116, -0.9841, -2.0067, -2.0048]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9988, -1.0084, -1.9941, -2.0015],
        [-2.0006, -1.0250, -2.0132, -2.0488]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0017,  0.7058,  0.0549,  0.0297],
        [ 0.6285,  0.7337,  0.7003,  0.0386]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002268000040203333
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9817, -1.0102, -2.0005, -1.9694],
        [-2.0088, -0.9968, -2.0368, -2.0322]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002076999982818961
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9889, -0.9993, -1.9856, -1.9661],
        [-1.9661, -0.9781, -2.0013, -2.0508]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9769, -1.0091, -1.9726, -1.9588],
        [-1.9818, -1.0048, -1.9971, -2.0774]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9883, -1.0409, -1.9747, -1.9679],
        [-1.9856, -1.0002, -1.9523, -2.0801]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020729999523609877
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0199, 0.6981, 0.0270, 0.0103],
        [0.6018, 0.7678, 0.6710, 0.0162]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022509999107569456
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0015, -0.9992, -1.9877, -2.0066],
        [-2.0148, -1.0024, -1.9960, -2.0370]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020729999523609877
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0970,  0.6840,  0.0438, -0.0056],
        [ 0.6974,  0.6704,  0.6724,  0.0056]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0660,  0.6831, -0.0125,  0.0280],
        [ 0.7082,  0.6603,  0.6624, -0.0443]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.1066, 0.6728, 0.0303, 0.0146],
        [0.7028, 0.6973, 0.6606, 0.0071]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.1293,  0.7481, -0.0052, -0.0531],
        [ 0.6963,  0.7313,  0.6815, -0.0425]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.01866300031542778
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6724172234535217
scalar_q_values:tensor([[-1.9779, -0.9934, -2.0007, -2.0012],
        [-1.9828, -1.0178, -1.9941, -2.0582]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002211000071838498
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0824,  0.7928,  0.0507, -0.0053],
        [ 0.6571,  0.6697,  0.6965,  0.0270]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0452, 0.6921, 0.0342, 0.0053],
        [0.6833, 0.7115, 0.6418, 0.0639]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00215000007301569
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 4.1792e-03,  6.6830e-01,  6.9158e-02,  3.7895e-02],
        [ 6.5766e-01,  6.7010e-01,  6.2361e-01, -5.2068e-04]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020310000982135534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9977, -0.9821, -1.9710, -1.9933],
        [-1.9878, -1.0120, -1.9926, -2.0826]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9818, -0.9864, -1.9561, -2.0123],
        [-2.0206, -0.9953, -1.9576, -2.0848]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002131999935954809
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0401,  0.7171, -0.0073,  0.0748],
        [ 0.6615,  0.7206,  0.6819,  0.0426]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021379999816417694
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9793, -0.9957, -2.0004, -2.0020],
        [-2.0289, -1.0008, -1.9678, -2.1150]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002131999935954809
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9918, -0.9967, -1.9857, -2.0172],
        [-2.0119, -0.9888, -1.9283, -2.0655]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0578,  0.7005, -0.0218,  0.0627],
        [ 0.6815,  0.6912,  0.6795,  0.0752]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020920000970363617
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0370, 0.6801, 0.0254, 0.0311],
        [0.7018, 0.6687, 0.7144, 0.0259]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0111, 0.7675, 0.0180, 0.0022],
        [0.7038, 0.7821, 0.6924, 0.0169]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007486000191420317
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[3.3990e-04, 6.9360e-01, 2.7246e-04, 1.9783e-02],
        [6.9493e-01, 6.7813e-01, 6.7957e-01, 2.1603e-02]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0108,  0.7257,  0.0059,  0.0156],
        [ 0.6326,  0.6818,  0.5706, -0.0136]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.0074880002066493034
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9983, -1.0024, -1.9778, -1.9957],
        [-1.9641, -1.0124, -1.9817, -2.0503]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002191999927163124
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0416,  0.7317, -0.0042, -0.0088],
        [ 0.6786,  0.7623,  0.7185,  0.0713]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0059,  0.6846,  0.0354,  0.0039],
        [ 0.6659,  0.6873,  0.6932,  0.0823]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0277, 0.7297, 0.0191, 0.0170],
        [0.6883, 0.7027, 0.7068, 0.0398]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007431000005453825
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0022, 0.6576, 0.0356, 0.0354],
        [0.6421, 0.6680, 0.6984, 0.0090]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0497, 0.7185, 0.0382, 0.0115],
        [0.7371, 0.7026, 0.6950, 0.0262]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0318, 0.6827, 0.0406, 0.0194],
        [0.6898, 0.7014, 0.6913, 0.0489]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012799999676644802
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[ 0.0689,  0.6617,  0.0210, -0.0188],
        [ 0.6984,  0.6796,  0.6890,  0.0769]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0672, 0.6910, 0.0540, 0.0244],
        [0.6771, 0.6877, 0.6859, 0.0362]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007267000153660774
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[ 0.0103,  0.8144,  0.0739, -0.0379],
        [ 0.6850,  0.7410,  0.6683,  0.0629]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002217999892309308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0285, 0.7096, 0.0437, 0.0013],
        [0.6588, 0.7669, 0.7579, 0.0883]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021669999696314335
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0492, 0.7455, 0.1030, 0.0086],
        [0.6547, 0.7187, 0.6690, 0.0392]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021269998978823423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0057, -0.9937, -1.9893, -1.9860],
        [-1.9568, -0.9988, -2.0016, -2.0232]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.004141000099480152
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0502, 0.6660, 0.0385, 0.0011],
        [0.6327, 0.7046, 0.6846, 0.0304]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020810000132769346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9903, -1.0097, -1.9774, -1.9752],
        [-1.9854, -0.9722, -1.9776, -2.0557]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023350000847131014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9591, -1.0218, -1.9954, -1.9458],
        [-1.9543, -1.0008, -1.9862, -2.0593]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0027580000460147858
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9772, -0.9664, -1.9324, -1.9812],
        [-1.9752, -0.9796, -2.0045, -2.0319]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002248999895527959
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9933, -1.0061, -2.0092, -1.9789],
        [-1.9764, -0.9936, -2.0200, -2.0521]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023670000955462456
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0170,  0.7141, -0.0765, -0.0891],
        [ 0.6473,  0.6945,  0.6669,  0.0797]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002248999895527959
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9179, -1.0306, -1.9896, -1.9913],
        [-1.9944, -0.9565, -2.0004, -2.1129]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00216299993917346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-6.1119e-04,  6.8359e-01, -3.0014e-03, -3.0704e-02],
        [ 6.7631e-01,  7.0377e-01,  6.8847e-01,  6.4828e-02]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022720000706613064
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9818, -0.9549, -1.9771, -1.9497],
        [-2.0066, -0.9944, -1.9641, -2.0976]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[1.6764e-02, 7.0765e-01, 1.6194e-02, 2.8918e-06],
        [7.0653e-01, 7.0815e-01, 6.7717e-01, 1.2758e-01]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00203300011344254
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9880, -1.0021, -1.9870, -2.0054],
        [-1.9830, -1.0009, -2.0000, -2.0795]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002225999953225255
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9763, -0.9843, -1.9965, -2.0350],
        [-1.9530, -1.0021, -1.9532, -2.0536]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002437999937683344
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0411,  0.7338, -0.0466,  0.0018],
        [ 0.6813,  0.6936,  0.6928,  0.0546]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002311999909579754
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0579, 0.6962, 0.0060, 0.0738],
        [0.6397, 0.7457, 0.6941, 0.0463]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002176000038161874
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9808, -0.9162, -1.9791, -1.9936],
        [-2.0479, -0.9682, -2.0199, -1.9727]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00203699991106987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9444, -0.9718, -1.9750, -1.9648],
        [-1.9860, -1.0318, -2.0187, -2.0632]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020620001014322042
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0192, 0.7548, 0.0426, 0.0168],
        [0.6961, 0.6769, 0.6521, 0.0360]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00203300011344254
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0653,  0.6586,  0.0384, -0.0905],
        [ 0.6770,  0.7193,  0.7083,  0.0187]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002067999914288521
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9734, -1.0008, -1.9741, -1.9877],
        [-2.0056, -0.9998, -1.9892, -2.0676]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021689999848604202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9857, -0.9771, -1.9478, -1.9737],
        [-1.9664, -1.0043, -1.9751, -2.1045]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00203699991106987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9897, -0.9921, -1.9931, -1.9837],
        [-2.0001, -1.0007, -1.9893, -2.1489]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020959998946636915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0045,  0.6958,  0.0834, -0.0603],
        [ 0.6821,  0.7197,  0.6400,  0.0751]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002194999950006604
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0213, 0.6601, 0.0393, 0.0017],
        [0.6756, 0.6966, 0.6636, 0.0624]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002070999937132001
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0012,  0.7452,  0.0209, -0.0026],
        [ 0.6575,  0.7540,  0.6266,  0.0748]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020620001014322042
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9528, -1.0106, -1.9715, -2.0321],
        [-1.9900, -0.9967, -2.0000, -2.1261]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020520000252872705
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0075, 0.6985, 0.0191, 0.0142],
        [0.6865, 0.7462, 0.6505, 0.0235]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020600000862032175
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9881, -1.0048, -1.9719, -1.9829],
        [-2.0084, -1.0084, -1.9741, -2.0908]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002131999935954809
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0365, 0.7182, 0.0411, 0.0474],
        [0.6908, 0.6920, 0.6702, 0.0647]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020409999415278435
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0266, 0.6724, 0.0365, 0.0354],
        [0.7085, 0.6888, 0.7162, 0.0735]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0306, 0.6758, 0.0426, 0.0318],
        [0.6699, 0.7785, 0.6934, 0.0690]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.011641000397503376
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[0.0319, 0.6823, 0.0284, 0.0175],
        [0.6491, 0.7910, 0.6347, 0.0907]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020830000285059214
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0060,  0.6945, -0.0129,  0.0440],
        [ 0.6381,  0.6803,  0.6819,  0.0683]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002274000085890293
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0159, 0.7249, 0.0089, 0.0760],
        [0.6485, 0.7309, 0.6831, 0.0352]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00216299993917346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0052, -0.9956, -1.9725, -2.0104],
        [-2.0093, -1.0020, -1.9622, -2.0445]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021170000545680523
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0554, 0.6907, 0.0307, 0.0424],
        [0.6586, 0.7463, 0.6807, 0.0743]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023399998899549246
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0057, -1.0018, -2.0051, -1.9378],
        [-1.9696, -0.9969, -1.9554, -2.0095]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002133999951183796
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9843, -0.9895, -2.0029, -1.9932],
        [-1.9518, -0.9501, -1.9561, -2.0100]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022720000706613064
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9889, -0.9864, -1.9989, -1.9717],
        [-1.9650, -1.0176, -2.0089, -2.0463]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0224,  0.6816,  0.0145, -0.0329],
        [ 0.6966,  0.6625,  0.6412,  0.0616]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0027,  0.6726, -0.0268,  0.0108],
        [ 0.7249,  0.7058,  0.6633,  0.0035]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0413,  0.6904, -0.0376,  0.0459],
        [ 0.6696,  0.7002,  0.6476,  0.0189]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012791000306606293
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[ 0.0099,  0.7374, -0.0342, -0.0025],
        [ 0.6732,  0.7704,  0.6922,  0.0426]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002076999982818961
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9961, -0.9893, -1.9970, -1.9781],
        [-2.0026, -0.9941, -2.0271, -2.0232]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002171000000089407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0204, 0.6603, 0.0065, 0.0139],
        [0.6767, 0.6979, 0.6543, 0.0109]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020959998946636915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0403, 0.7155, 0.1025, 0.0221],
        [0.6510, 0.7366, 0.7001, 0.0336]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002056000055745244
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0175, 0.6875, 0.0108, 0.0013],
        [0.6560, 0.7014, 0.7050, 0.0482]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-0.0036,  0.7444, -0.0347,  0.0203],
        [ 0.7187,  0.7132,  0.6648,  0.0436]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007348999846726656
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9971, -0.9913, -1.9663, -1.9635],
        [-1.9932, -0.9991, -1.9905, -2.0478]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020749999675899744
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9823, -1.0394, -1.9460, -1.9626],
        [-1.9854, -0.9958, -1.9648, -2.0156]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9515, -1.0047, -1.9770, -1.9660],
        [-1.9933, -0.9943, -1.9854, -2.0441]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002047999994829297
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9911, -1.0105, -1.9986, -1.9769],
        [-1.9962, -1.0046, -2.0002, -2.0637]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002078999998047948
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0333,  0.6990, -0.0110,  0.0042],
        [ 0.6731,  0.6954,  0.6488,  0.0370]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0545, 0.7448, 0.0484, 0.0201],
        [0.6410, 0.7056, 0.7067, 0.0245]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0487, 0.7150, 0.0431, 0.0073],
        [0.6412, 0.7152, 0.6290, 0.0132]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0251,  0.7049,  0.0542, -0.0117],
        [ 0.6323,  0.7596,  0.6493, -0.0238]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021170000545680523
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9917, -1.0007, -2.0071, -1.9963],
        [-1.9735, -1.0010, -1.9906, -2.0309]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025289999321103096
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0155, 0.7074, 0.0608, 0.0084],
        [0.6828, 0.7213, 0.7464, 0.0320]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0213,  0.7192, -0.0142, -0.0085],
        [ 0.6825,  0.7018,  0.7038,  0.0451]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007298000156879425
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9762, -1.0040, -1.9940, -1.9683],
        [-2.0035, -1.0014, -2.0023, -2.0552]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002067999914288521
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0193, 0.7054, 0.0014, 0.0796],
        [0.6789, 0.6738, 0.6512, 0.0459]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002090000081807375
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9825, -0.9782, -1.9885, -2.0166],
        [-2.0086, -0.9939, -1.9844, -2.0924]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002050000010058284
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0385,  0.6896, -0.0515, -0.0233],
        [ 0.6825,  0.7022,  0.6761,  0.0480]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021039999555796385
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9937, -1.0131, -1.9775, -2.0030],
        [-1.9525, -1.0045, -1.9946, -2.0894]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020409999415278435
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0098, -0.9829, -1.9994, -1.9133],
        [-1.9731, -1.0114, -1.9949, -2.0236]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020979999098926783
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0024, -0.9864, -2.0096, -2.0141],
        [-1.9909, -1.0014, -1.9850, -2.0867]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002107999986037612
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0403, -1.0066, -1.9528, -2.0190],
        [-1.9748, -0.9855, -1.9207, -2.0742]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020600000862032175
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[3.8153e-05, 6.6803e-01, 6.1279e-02, 1.5097e-02],
        [7.2002e-01, 7.2685e-01, 7.1579e-01, 9.2187e-02]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0019779999274760485
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0360, 0.6701, 0.0670, 0.0356],
        [0.6605, 0.6831, 0.6948, 0.0539]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0579,  0.7011,  0.0935, -0.0224],
        [ 0.6615,  0.7384,  0.6599,  0.0096]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.008458999916911125
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9804, -0.9957, -1.9483, -1.9857],
        [-1.9825, -1.0149, -2.0133, -2.1047]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0027600000612437725
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0423,  0.6784,  0.0331, -0.0079],
        [ 0.6123,  0.7074,  0.6227, -0.0033]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0025790000800043344
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0533,  0.7084,  0.0416, -0.0364],
        [ 0.6900,  0.7094,  0.6546,  0.0310]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002205000026151538
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0392, 0.7477, 0.0110, 0.0118],
        [0.6815, 0.6754, 0.6452, 0.0320]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024759999942034483
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0823, 0.6646, 0.0463, 0.0207],
        [0.6720, 0.6939, 0.6643, 0.0121]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002471999963745475
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9782, -0.9956, -1.9809, -2.0167],
        [-1.9828, -1.0002, -1.9864, -2.0962]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002239000052213669
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9703, -0.9776, -1.9931, -1.9711],
        [-2.0040, -0.9792, -1.9834, -2.1094]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021379999816417694
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0063, -1.0051, -2.0127, -1.9725],
        [-2.0160, -1.0116, -1.9811, -2.0680]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002070999937132001
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0224,  0.7604, -0.0234,  0.0030],
        [ 0.6378,  0.6376,  0.6535,  0.0498]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002085000043734908
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9838, -1.0008, -1.9876, -1.9757],
        [-1.9855, -0.9917, -1.9774, -2.0916]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0214,  0.6721,  0.0260,  0.0576],
        [ 0.6176,  0.6448,  0.6095, -0.0753]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00204299995675683
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9848, -1.0061, -1.9864, -2.0083],
        [-1.9936, -1.0045, -1.9975, -2.0889]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002067999914288521
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0052, 0.6617, 0.0048, 0.0722],
        [0.6349, 0.6902, 0.7066, 0.1093]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0367, 0.7192, 0.0440, 0.0354],
        [0.6959, 0.6965, 0.6408, 0.0541]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007728999946266413
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0098, -1.0182, -1.9586, -2.0396],
        [-1.9474, -0.9987, -1.9605, -2.0325]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021510000806301832
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0407, 0.7087, 0.0310, 0.0664],
        [0.7003, 0.6883, 0.6527, 0.0939]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002085000043734908
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0568, 0.7001, 0.0188, 0.0473],
        [0.6262, 0.7680, 0.6899, 0.0943]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9784, -1.0021, -1.9646, -2.0022],
        [-1.9810, -0.9908, -1.9963, -2.0440]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002076999982818961
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0442, 0.6985, 0.0362, 0.0033],
        [0.6576, 0.7221, 0.6788, 0.0391]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020349998958408833
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0360, 0.6998, 0.0307, 0.0193],
        [0.6499, 0.7648, 0.6412, 0.0377]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00204299995675683
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0281, 0.6687, 0.0145, 0.0077],
        [0.6407, 0.6829, 0.6335, 0.0506]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020179999992251396
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0202,  0.7167,  0.0139, -0.0106],
        [ 0.6845,  0.6833,  0.6706,  0.0203]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021550001110881567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0247, 0.7393, 0.0577, 0.0217],
        [0.6529, 0.7046, 0.6887, 0.0790]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00204299995675683
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0065, -1.0107, -1.9684, -2.0009],
        [-2.0077, -0.9845, -1.9998, -2.0501]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002022000029683113
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9750, -1.0037, -1.9760, -1.9120],
        [-1.9986, -0.9822, -1.9676, -2.0175]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020389999262988567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0012,  0.7066, -0.0234, -0.0344],
        [ 0.6604,  0.6944,  0.6918,  0.0367]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020749999675899744
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9949, -0.9828, -1.9621, -1.9718],
        [-1.9954, -0.9968, -1.9627, -2.0456]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002065999899059534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9250, -1.0238, -1.9950, -1.9881],
        [-1.9830, -0.9996, -1.9772, -2.0762]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022070000413805246
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9869, -1.0033, -2.0009, -2.0123],
        [-1.9899, -0.9808, -1.9999, -2.0410]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00228299992159009
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0323, 0.7004, 0.0296, 0.0024],
        [0.6886, 0.6870, 0.6707, 0.0806]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023330000694841146
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0139, -1.0313, -1.9927, -1.9817],
        [-1.9984, -0.9929, -1.9838, -2.0053]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021720000077039003
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 6.9276e-04,  8.4576e-01,  1.2109e-01, -5.8803e-03],
        [ 6.8782e-01,  7.1259e-01,  6.9990e-01,  5.2918e-02]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021250001154839993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0633, 0.6847, 0.0216, 0.0205],
        [0.6652, 0.6921, 0.6851, 0.0195]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002463999902829528
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0150,  0.7430, -0.0131,  0.0008],
        [ 0.6569,  0.7862,  0.6403,  0.0991]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021860001143068075
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0059, -1.0113, -1.9722, -2.0338],
        [-1.9736, -1.0323, -1.9527, -2.1355]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9640, -0.9844, -1.9639, -1.9000],
        [-1.9510, -1.0246, -2.0016, -2.0233]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00215000007301569
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9538, -0.9919, -1.9918, -2.0066],
        [-1.9414, -1.0161, -1.9976, -2.0822]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002065999899059534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9580, -1.0318, -1.9295, -1.9380],
        [-1.9714, -0.9934, -1.9768, -2.0542]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0472,  0.7033,  0.0463,  0.0271],
        [ 0.6820,  0.7014,  0.6043, -0.0028]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002065999899059534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0651,  0.6796,  0.0052, -0.0144],
        [ 0.7058,  0.7239,  0.7334,  0.0752]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0458, 0.7045, 0.0151, 0.0426],
        [0.6863, 0.6889, 0.6756, 0.0667]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007306999992579222
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0176, -0.9987, -1.9834, -1.9787],
        [-2.0149, -0.9901, -1.9972, -2.0925]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002128999913111329
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0019,  0.6632, -0.0048,  0.0059],
        [ 0.6721,  0.6744,  0.6638,  0.0727]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9609, -0.9953, -1.9569, -2.0151],
        [-2.0348, -1.0474, -1.9835, -2.0840]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020830000285059214
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9625, -0.9581, -1.9639, -2.0351],
        [-1.9769, -1.0010, -2.0098, -2.0597]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020280000753700733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0138, 0.7192, 0.0296, 0.0404],
        [0.6567, 0.7261, 0.6607, 0.0458]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9782, -1.0138, -2.0029, -2.0091],
        [-1.9751, -0.9881, -1.9975, -2.0757]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020520000252872705
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0538,  0.7764, -0.0586, -0.0301],
        [ 0.7105,  0.7173,  0.6131,  0.0673]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00203300011344254
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-3.3421e-04,  6.8012e-01,  9.0866e-02,  2.9405e-02],
        [ 6.1856e-01,  6.5025e-01,  6.7803e-01,  7.3718e-02]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020749999675899744
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0418,  0.6665,  0.0143, -0.0177],
        [ 0.6988,  0.6735,  0.6899,  0.0801]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0331, 0.6868, 0.0093, 0.0418],
        [0.6878, 0.7182, 0.6707, 0.0834]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.00724400021135807
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0085, 0.6976, 0.0014, 0.0291],
        [0.6984, 0.7540, 0.6789, 0.0759]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020600000862032175
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[5.0985e-03, 7.0307e-01, 5.0244e-04, 3.1929e-02],
        [6.4000e-01, 7.5961e-01, 7.1193e-01, 4.9919e-02]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020069999154657125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9314, -1.0005, -1.8935, -1.9834],
        [-1.9939, -0.9959, -1.9964, -2.0748]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020540000405162573
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9649, -0.9738, -1.9596, -2.0035],
        [-2.0080, -1.0274, -1.9877, -2.1028]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020409999415278435
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0042,  0.7048,  0.0442, -0.0839],
        [ 0.6342,  0.6631,  0.6777,  0.1012]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002211000071838498
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0468, 0.7372, 0.0483, 0.0407],
        [0.6525, 0.7019, 0.7100, 0.0250]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021609999239444733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9855, -0.9979, -2.0254, -1.9886],
        [-1.9619, -1.0056, -1.9960, -2.0616]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020979999098926783
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0193, -0.9437, -1.9622, -1.9452],
        [-1.9759, -1.0298, -1.9650, -2.0170]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021039999555796385
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0848,  0.6696, -0.0296,  0.0094],
        [ 0.6504,  0.6776,  0.6780,  0.1782]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0084, 0.7242, 0.0177, 0.0356],
        [0.6936, 0.7038, 0.6339, 0.0517]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007286000065505505
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0439, 0.6488, 0.0078, 0.0762],
        [0.6591, 0.7065, 0.6364, 0.0489]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 3.0688e-02,  6.7800e-01,  8.8613e-05, -4.4953e-03],
        [ 6.7174e-01,  7.1557e-01,  6.2616e-01,  4.5669e-02]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020240000449121
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9738, -0.9908, -1.9529, -1.9028],
        [-1.9913, -0.9976, -1.9674, -2.0648]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002128999913111329
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9881, -0.9981, -1.9997, -2.0050],
        [-1.9760, -1.0001, -1.9992, -2.0451]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021780000533908606
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9934, -1.0239, -2.0119, -1.9923],
        [-1.9785, -0.9790, -1.9987, -2.0406]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0025100000202655792
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0134, -1.0020, -2.0008, -2.0162],
        [-1.9604, -1.0134, -2.0413, -2.0426]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021230001002550125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9897, -1.0118, -1.9965, -1.9653],
        [-1.9591, -0.9786, -2.0041, -2.0610]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0388,  0.6912, -0.0179,  0.0245],
        [ 0.6819,  0.6982,  0.6966,  0.0549]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002182000083848834
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9674, -0.9944, -2.0216, -1.9527],
        [-1.9989, -1.0070, -2.0069, -2.0656]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002111000008881092
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0065, -1.0075, -1.9794, -2.0359],
        [-2.0114, -0.9875, -1.9800, -2.0732]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020520000252872705
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0218,  0.6766,  0.0108, -0.0368],
        [ 0.6399,  0.7701,  0.6222,  0.0332]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9849, -0.9765, -1.9730, -2.0450],
        [-1.9945, -1.0111, -1.9806, -2.1093]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0297,  0.7153,  0.0103, -0.0098],
        [ 0.7029,  0.7112,  0.6589,  0.0199]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022559999488294125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0322,  0.7276,  0.1069, -0.0102],
        [ 0.6606,  0.6760,  0.6914,  0.0409]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002090000081807375
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0141,  0.7029,  0.0411, -0.0092],
        [ 0.6956,  0.6736,  0.6937,  0.0501]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002070999937132001
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9997, -1.0100, -1.9883, -1.9983],
        [-1.9841, -0.9828, -2.0168, -2.0923]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020540000405162573
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9952, -0.9929, -2.0047, -2.0040],
        [-2.0215, -0.9987, -1.9828, -2.0648]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020729999523609877
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0735, 0.7113, 0.0133, 0.0668],
        [0.6737, 0.7123, 0.6900, 0.0489]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0610, 0.6894, 0.0271, 0.0074],
        [0.6370, 0.7332, 0.6781, 0.1064]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020830000285059214
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0332,  0.7342,  0.0035, -0.0135],
        [ 0.6781,  0.7591,  0.6841,  0.0754]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021609999239444733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0204,  0.6564,  0.0533,  0.0602],
        [ 0.6533,  0.6964,  0.6405,  0.0943]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002644000109285116
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9881, -1.0057, -1.9738, -2.0242],
        [-1.9949, -0.9952, -1.9576, -2.0453]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0025369999930262566
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0256, -1.0095, -1.9590, -2.0087],
        [-1.9847, -1.0131, -1.9827, -2.1373]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002351999981328845
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9652, -0.9825, -1.9783, -1.9813],
        [-1.9793, -0.9942, -1.9861, -2.0576]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023419999051839113
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9898, -1.0072, -1.9805, -2.0127],
        [-2.0019, -0.9952, -1.9840, -2.0724]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00215300009585917
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0120,  0.7695,  0.0197, -0.0334],
        [ 0.6917,  0.7253,  0.7169, -0.0030]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024949999060481787
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9939, -1.0008, -1.9878, -2.0358],
        [-2.0031, -1.0218, -2.0267, -2.0764]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002131999935954809
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9774, -0.9981, -1.9999, -2.0186],
        [-1.9885, -1.0171, -1.9902, -2.0196]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002085000043734908
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9487, -0.9767, -1.9672, -2.0911],
        [-1.9909, -0.9944, -1.9845, -2.0890]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002128999913111329
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0287,  0.7378,  0.0233, -0.0118],
        [ 0.7118,  0.7026,  0.7273,  0.0018]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021230001002550125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0147,  0.7670, -0.0353, -0.0054],
        [ 0.6521,  0.7053,  0.6588,  0.0448]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002217999892309308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0215,  0.7056, -0.0018, -0.0012],
        [ 0.6934,  0.7324,  0.6702,  0.0399]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021550001110881567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0250, -1.0107, -2.0287, -2.0671],
        [-1.9887, -1.0010, -1.9975, -2.0688]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022509999107569456
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9630, -0.9699, -1.9652, -1.9802],
        [-2.0461, -0.9597, -2.0100, -2.0733]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021550001110881567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0346, 0.7252, 0.0297, 0.0088],
        [0.6936, 0.6932, 0.6952, 0.0710]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020830000285059214
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0080,  0.6990, -0.0074,  0.0275],
        [ 0.7134,  0.6656,  0.7099,  0.1023]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[-0.0283,  0.6728, -0.0411,  0.0150],
        [ 0.6884,  0.7351,  0.7033,  0.0663]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.008662999607622623
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9823, -1.0053, -1.9808, -2.0114],
        [-1.9895, -0.9995, -2.0041, -2.0700]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021100000012665987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0136,  0.7060, -0.0041,  0.0263],
        [ 0.6866,  0.6846,  0.6484, -0.0369]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022140000946819782
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[2.9180e-02, 6.6274e-01, 3.0433e-04, 8.8312e-03],
        [6.7819e-01, 7.4283e-01, 6.5664e-01, 7.9621e-02]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020389999262988567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0015,  0.6908, -0.0229,  0.0066],
        [ 0.7345,  0.6537,  0.6776, -0.0037]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0285,  0.6734,  0.0072, -0.0017],
        [ 0.6597,  0.7201,  0.6575,  0.0486]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.00761799979954958
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0440, 0.7516, 0.0398, 0.0027],
        [0.7295, 0.7032, 0.6670, 0.0404]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9101, -1.0258, -1.9249, -1.9759],
        [-2.0064, -1.0041, -1.9716, -2.0621]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9542, -0.9815, -1.9906, -2.0144],
        [-1.9813, -1.0017, -1.9919, -2.0912]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002085000043734908
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0034,  0.7125, -0.0036, -0.0192],
        [ 0.6287,  0.6875,  0.6688,  0.0676]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021210000850260258
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9859, -1.0195, -2.0034, -2.0014],
        [-2.0043, -1.0086, -1.9902, -2.0835]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0024759999942034483
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9924, -0.9694, -2.0084, -1.9785],
        [-2.0094, -1.0088, -2.0071, -2.1008]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022849999368190765
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9870, -1.0106, -1.9739, -1.9846],
        [-2.0018, -0.9904, -1.9648, -2.0970]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002394000068306923
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0519,  0.6524,  0.0610,  0.0494],
        [ 0.6972,  0.6913,  0.7281, -0.0030]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0607, 0.6857, 0.0564, 0.0224],
        [0.6776, 0.7320, 0.6716, 0.0534]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.0073449998162686825
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9920, -1.0121, -1.9844, -1.9637],
        [-1.9884, -1.0023, -1.9827, -2.0471]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021059999708086252
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9842, -0.9827, -1.9666, -1.9861],
        [-1.9700, -1.0103, -1.9564, -2.0257]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021780000533908606
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9785, -1.0054, -1.9757, -2.0501],
        [-1.9865, -0.9922, -1.9864, -2.0518]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021929999347776175
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0000, -1.0100, -1.9769, -1.9714],
        [-1.9766, -0.9955, -1.9780, -1.9549]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0128, -1.0146, -1.9924, -1.9619],
        [-1.9782, -0.9997, -1.9929, -2.0594]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002090000081807375
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0023, 0.6694, 0.0376, 0.0405],
        [0.6618, 0.6449, 0.6360, 0.1068]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020580000709742308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9615, -0.9782, -2.0053, -1.9222],
        [-2.0299, -0.9845, -2.0114, -2.0612]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021440000273287296
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0879,  0.7130,  0.0625, -0.0113],
        [ 0.6698,  0.6986,  0.6510,  0.1167]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021100000012665987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0596, 0.6662, 0.0266, 0.0843],
        [0.6399, 0.7461, 0.6653, 0.1349]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020749999675899744
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9931, -0.9922, -1.9896, -1.9769],
        [-1.9869, -1.0014, -1.9689, -2.0583]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021210000850260258
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0226, -1.0005, -1.9992, -2.0191],
        [-1.9800, -1.0028, -1.9865, -2.0751]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002050000010058284
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9545, -1.0233, -2.0039, -1.9296],
        [-1.9701, -0.9814, -1.9668, -2.0476]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002245000097900629
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0244,  0.7200,  0.0274, -0.0058],
        [ 0.6967,  0.6700,  0.6586,  0.0986]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021170000545680523
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0144, 0.6968, 0.0180, 0.0197],
        [0.6851, 0.6934, 0.6592, 0.0422]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020959998946636915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0234,  0.7115,  0.0183,  0.0470],
        [ 0.6723,  0.7053,  0.6711,  0.0782]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021250001154839993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9883, -1.0042, -2.0122, -1.9813],
        [-2.0333, -1.0039, -2.0020, -2.0878]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020639998838305473
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9906, -1.0171, -1.9860, -2.0382],
        [-2.0025, -0.9706, -1.9736, -2.1213]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021210000850260258
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0432,  0.6720, -0.0042,  0.0043],
        [ 0.6647,  0.6632,  0.6563,  0.0259]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002219999907538295
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9832, -1.0064, -1.9555, -2.0121],
        [-1.9843, -0.9923, -1.9960, -2.1234]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002400999888777733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0005, -0.9949, -1.9669, -2.0115],
        [-1.9974, -0.9912, -2.0003, -2.1158]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022789998911321163
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9267, -1.0325, -1.9952, -2.0207],
        [-2.0014, -0.9713, -1.9436, -2.1012]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002182000083848834
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9820, -0.9886, -1.9442, -2.0064],
        [-1.9362, -1.0138, -1.9614, -2.0794]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002329000039026141
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0275,  0.6840,  0.0207, -0.0017],
        [ 0.6739,  0.6714,  0.7440,  0.0754]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0334,  0.7399, -0.0087,  0.0008],
        [ 0.6882,  0.6879,  0.7098,  0.0108]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.0074269999749958515
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0064, -1.0055, -1.9951, -2.0126],
        [-1.9969, -1.0055, -2.0042, -2.1240]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024530000519007444
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9940, -1.0074, -2.0046, -2.0108],
        [-1.9985, -1.0140, -2.0136, -2.1338]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002171000000089407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-9.8736e-05,  7.1712e-01, -1.8706e-02, -1.9540e-02],
        [ 6.8865e-01,  7.2487e-01,  6.9670e-01,  3.5063e-02]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0412, 0.6782, 0.0141, 0.0157],
        [0.6801, 0.7364, 0.6728, 0.0281]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9865, -0.9815, -1.9878, -2.0272],
        [-1.9937, -0.9959, -1.9984, -2.1057]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0004, -0.9947, -2.0031, -2.0583],
        [-2.0012, -1.0084, -1.9762, -2.0142]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020920000970363617
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9581, -0.9901, -1.9827, -2.0317],
        [-2.0050, -1.0141, -1.9969, -2.1054]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002076999982818961
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0371, 0.7211, 0.0366, 0.0055],
        [0.7132, 0.7351, 0.7134, 0.0540]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021039999555796385
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9565, -0.9984, -2.0099, -1.9813],
        [-1.9914, -0.9882, -2.0239, -2.0997]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020280000753700733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9948, -0.9619, -1.9653, -2.0500],
        [-1.9622, -0.9696, -1.9586, -2.0677]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0574,  0.6940,  0.0708, -0.0080],
        [ 0.6780,  0.7147,  0.6809,  0.0053]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002078999998047948
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9379, -1.0020, -1.9627, -2.0543],
        [-1.9943, -0.9973, -1.9991, -2.1042]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002056000055745244
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0030, -1.0331, -1.9925, -2.0184],
        [-1.9858, -1.0324, -2.0041, -2.1002]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020260000601410866
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9961, -0.9862, -1.9990, -1.9642],
        [-1.9767, -0.9932, -1.9869, -2.0648]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020600000862032175
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9509, -1.0028, -2.0078, -1.9852],
        [-2.0079, -0.9700, -1.9898, -2.0823]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002076999982818961
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0091, -0.9527, -2.0053, -2.0015],
        [-1.9891, -0.9880, -1.9768, -2.0819]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002050000010058284
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9924, -0.9882, -1.9876, -1.9743],
        [-2.0007, -0.9874, -1.9845, -2.0732]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002056000055745244
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0049,  0.7426, -0.0294, -0.0113],
        [ 0.6057,  0.7294,  0.6093, -0.0056]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020749999675899744
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0593, 0.7512, 0.0509, 0.0201],
        [0.7227, 0.7126, 0.6289, 0.0053]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002085000043734908
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9835, -0.9880, -2.0023, -2.0182],
        [-1.9954, -0.9730, -1.9664, -2.0652]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[1.6527e-02, 6.7533e-01, 3.3870e-02, 8.3097e-05],
        [6.4443e-01, 7.2867e-01, 6.9417e-01, 8.0580e-02]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0019890000112354755
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0383,  0.6780,  0.0421, -0.0129],
        [ 0.6692,  0.6789,  0.6913,  0.0520]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-7.9817e-03,  7.1516e-01, -1.6594e-04,  1.9380e-02],
        [ 6.8537e-01,  6.8926e-01,  7.0955e-01,  6.0144e-02]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.0073739998042583466
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-0.0355,  0.7540,  0.0324,  0.0339],
        [ 0.6772,  0.6957,  0.6547,  0.0177]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020810000132769346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0042,  0.7149,  0.0066,  0.0512],
        [ 0.6831,  0.7349,  0.6890,  0.0948]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020469999872148037
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0402, 0.7356, 0.1118, 0.0083],
        [0.6848, 0.6771, 0.6561, 0.0784]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021359999664127827
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0496, 0.7124, 0.0471, 0.0382],
        [0.7008, 0.7814, 0.6393, 0.0650]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021269998978823423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0548,  0.6851, -0.0059,  0.0860],
        [ 0.6721,  0.7128,  0.6257,  0.1031]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0027469999622553587
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0439, 0.7119, 0.0082, 0.0392],
        [0.7026, 0.6871, 0.6298, 0.0960]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021359999664127827
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9381, -1.0155, -1.9174, -2.0305],
        [-1.9735, -0.9695, -2.0173, -2.0218]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020580000709742308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0724, 0.7435, 0.0469, 0.0147],
        [0.6844, 0.7219, 0.7126, 0.1130]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025939999613910913
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9910, -0.9867, -1.9900, -1.9966],
        [-1.9838, -0.9538, -1.9626, -2.0719]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022279999684542418
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0668, 0.6878, 0.0702, 0.0637],
        [0.6680, 0.6865, 0.7021, 0.1019]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.0239, 0.7036, 0.0513, 0.0658],
        [0.6619, 0.7110, 0.6283, 0.1206]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007590999826788902
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9874, -1.0379, -1.9908, -1.9886],
        [-1.9555, -1.0249, -1.9886, -2.0331]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020729999523609877
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9998, -0.9954, -2.0099, -2.0006],
        [-1.9773, -0.9680, -1.9524, -2.0339]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9940, -0.9845, -1.9715, -1.9770],
        [-1.9908, -0.9814, -1.9926, -2.0679]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021510000806301832
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0193, -0.9892, -1.9953, -2.0218],
        [-2.0082, -0.9890, -1.9953, -2.0756]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002248999895527959
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0246,  0.7096,  0.0113, -0.0348],
        [ 0.6727,  0.6896,  0.6614,  0.0430]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002216000109910965
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0049, 0.8057, 0.0780, 0.0798],
        [0.6759, 0.7159, 0.6744, 0.0410]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020810000132769346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0077, 0.7419, 0.0787, 0.0249],
        [0.7027, 0.6896, 0.6890, 0.0362]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021840000990778208
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-4.0038e-04,  6.7629e-01,  1.4095e-02, -1.8415e-03],
        [ 6.6391e-01,  6.5178e-01,  6.2312e-01,  2.9032e-02]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0019890000112354755
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0204,  0.6912, -0.0223, -0.0487],
        [ 0.6626,  0.6807,  0.6874,  0.0439]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024129999801516533
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
Episode infos:
Steps: 1, Time: 0.0009190000128000975
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0124, -1.0082, -1.9743, -1.9870],
        [-1.9904, -1.0255, -1.9705, -1.9930]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021440000273287296
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[1.0800e-02, 7.4079e-01, 6.1186e-03, 5.8203e-04],
        [6.4438e-01, 6.8520e-01, 6.6136e-01, 2.9577e-02]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002044999971985817
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9789, -0.9776, -1.9795, -1.9914],
        [-2.0303, -1.0013, -1.9469, -2.0690]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021359999664127827
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9786, -1.0067, -1.9958, -1.9290],
        [-2.0049, -0.9961, -2.0068, -2.0719]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020620001014322042
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0526, 0.6604, 0.0188, 0.0060],
        [0.6723, 0.6591, 0.6706, 0.0576]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0206,  0.6874, -0.0113,  0.0211],
        [ 0.7048,  0.6683,  0.6947,  0.1160]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0342, 0.7555, 0.0048, 0.0048],
        [0.6787, 0.6913, 0.6592, 0.0437]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012446999549865723
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[ 0.0244,  0.6774, -0.0324,  0.0377],
        [ 0.6679,  0.6874,  0.6636,  0.0645]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021039999555796385
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9871, -0.9915, -1.9918, -1.9699],
        [-1.9808, -0.9829, -1.9943, -2.0648]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002047999994829297
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0129, 0.6986, 0.0112, 0.0384],
        [0.6587, 0.7194, 0.6856, 0.1072]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002196999965235591
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9941, -1.0157, -2.0000, -2.0001],
        [-2.0070, -1.0059, -1.9902, -2.0501]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022219999227672815
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9895, -1.0006, -2.0150, -1.9811],
        [-1.9119, -1.0040, -1.9967, -2.0508]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020639998838305473
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9804, -0.9862, -2.0294, -1.9838],
        [-1.9931, -0.9887, -2.0203, -2.0580]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020580000709742308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9735, -0.9568, -1.9663, -1.9985],
        [-1.9956, -1.0157, -1.9867, -2.0865]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021669999696314335
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0106, 0.7037, 0.0338, 0.0730],
        [0.6746, 0.6900, 0.6836, 0.0589]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0095, -1.0093, -2.0000, -2.0118],
        [-1.9847, -0.9688, -1.9994, -2.0378]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0025160000659525394
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9947, -1.0010, -1.9857, -2.0020],
        [-1.9778, -1.0198, -1.9549, -1.9539]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022559999488294125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9943, -0.9944, -1.9670, -1.9936],
        [-1.9375, -0.9856, -2.0175, -2.0451]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021689999848604202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9797, -0.9778, -1.9901, -2.0102],
        [-1.9905, -0.9937, -1.9984, -2.0895]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024109999649226665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0652,  0.6903,  0.0491, -0.0081],
        [ 0.7197,  0.7016,  0.5854, -0.0459]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0825, 0.6985, 0.0936, 0.0626],
        [0.6894, 0.7095, 0.6716, 0.0369]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007906000129878521
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0084, -0.9888, -2.0044, -2.0067],
        [-1.9985, -0.9942, -2.0081, -2.0903]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021860001143068075
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0084, -0.9685, -1.9816, -2.0015],
        [-1.9905, -0.9852, -2.0094, -2.0747]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002076999982818961
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0158, 0.7171, 0.1066, 0.0182],
        [0.6209, 0.7014, 0.6875, 0.0253]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0007, -0.9951, -1.9771, -2.0324],
        [-1.9932, -1.0013, -1.9922, -2.0998]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020290000829845667
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9536, -1.0144, -1.9738, -2.0185],
        [-1.9432, -1.0018, -2.0168, -2.1570]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002148000057786703
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0331,  0.7374,  0.0574,  0.0151],
        [ 0.6665,  0.6801,  0.6797,  0.0422]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021780000533908606
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0088,  0.7132, -0.0018, -0.0093],
        [ 0.6805,  0.6799,  0.7062,  0.0432]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020099999383091927
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9956, -1.0048, -1.9749, -2.0046],
        [-1.9356, -1.0034, -2.0017, -1.9655]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021440000273287296
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0215, 0.7064, 0.0270, 0.0275],
        [0.6991, 0.7190, 0.6987, 0.0627]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021780000533908606
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9955, -1.0073, -2.0009, -1.9907],
        [-2.0045, -1.0153, -2.0416, -2.1019]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021100000012665987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9926, -1.0066, -1.9990, -1.9817],
        [-1.9945, -1.0143, -1.9645, -2.1503]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021609999239444733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0270, -1.0014, -2.0070, -1.9933],
        [-2.0072, -0.9947, -2.0028, -2.0797]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002139999996870756
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0293, 0.6556, 0.0187, 0.0063],
        [0.7027, 0.6980, 0.6728, 0.0655]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0426, 0.7006, 0.0637, 0.0396],
        [0.7250, 0.7018, 0.6368, 0.0447]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.0121, 0.6791, 0.0398, 0.0273],
        [0.6506, 0.7053, 0.6675, 0.0489]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01309799961745739
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-1.9802, -0.9994, -2.0009, -2.0137],
        [-2.0085, -0.9949, -1.9981, -2.0721]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021250001154839993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9909, -1.0128, -1.9975, -2.0317],
        [-1.9905, -0.9990, -1.9942, -2.0687]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021379999816417694
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9610, -0.9615, -1.9793, -1.9592],
        [-1.9809, -1.0139, -1.9829, -2.0723]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020920000970363617
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0215,  0.7739,  0.0122, -0.0177],
        [ 0.6304,  0.6883,  0.6173,  0.1261]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9980, -1.0008, -2.0163, -2.0050],
        [-1.9902, -1.0028, -2.0215, -2.0540]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022559999488294125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9938, -1.0104, -1.9652, -1.9858],
        [-1.9872, -0.9142, -1.9969, -2.0005]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9718, -1.0012, -2.0041, -2.0134],
        [-1.9705, -1.0196, -1.9243, -2.1000]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020580000709742308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0006, -1.0121, -1.9385, -1.9459],
        [-1.9925, -1.0011, -1.9803, -2.1338]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021100000012665987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.0548, 0.6863, 0.0041, 0.0462],
        [0.7616, 0.7407, 0.6828, 0.0598]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[ 0.0217,  0.6999, -0.0018,  0.0431],
        [ 0.6729,  0.7030,  0.7006,  0.0915]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007474999874830246
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.0876, 0.6869, 0.0724, 0.0952],
        [0.6325, 0.7063, 0.6031, 0.0049]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0017, -0.9656, -1.9471, -2.0215],
        [-2.0017, -1.0033, -1.9997, -2.0765]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002171000000089407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0110, -0.9973, -1.9537, -1.9447],
        [-1.9832, -1.0048, -1.9863, -2.0927]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00254100002348423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9933, -1.0072, -1.9923, -1.9968],
        [-1.9971, -1.0024, -1.9962, -2.0668]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023709998931735754
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9722, -0.9953, -1.9881, -1.9992],
        [-1.9878, -0.9904, -1.9955, -2.0519]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021269998978823423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0268,  0.7412,  0.0031, -0.0339],
        [ 0.6609,  0.6544,  0.7401,  0.1078]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002191999927163124
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.0210, 0.6967, 0.0263, 0.0435],
        [0.6694, 0.6929, 0.7373, 0.0571]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0476,  0.6985, -0.0032,  0.0044],
        [ 0.6894,  0.6854,  0.6991,  0.0841]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[ 0.0678,  0.7522, -0.0306,  0.0062],
        [ 0.6689,  0.7502,  0.6826,  0.0635]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012698999606072903
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[ 0.0187,  0.6869, -0.0432,  0.0250],
        [ 0.7026,  0.7189,  0.6612,  0.0755]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002139999996870756
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0403,  0.7018, -0.0379, -0.0172],
        [ 0.6623,  0.7024,  0.6662,  0.0223]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021170000545680523
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0008, -1.0072, -1.9589, -1.9711],
        [-1.9996, -1.0093, -1.9662, -2.0950]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002078999998047948
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.0467,  0.7086, -0.0021, -0.0120],
        [ 0.6526,  0.7416,  0.6368,  0.0544]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9539, -0.9857, -1.9774, -1.9717],
        [-2.0022, -1.0046, -1.9668, -2.0646]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9855, -0.9995, -1.9835, -1.9901],
        [-1.9563, -0.9558, -1.9665, -2.0299]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020540000405162573
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9987, -0.9920, -2.0018, -1.9920],
        [-1.9693, -1.0003, -1.9629, -2.0521]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002056000055745244
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9871, -1.0266, -1.9576, -1.9337],
        [-2.0129, -1.0158, -2.0034, -2.0545]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020749999675899744
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-0.0049,  0.6809,  0.0890, -0.0485],
        [ 0.6111,  0.7221,  0.6849,  0.0492]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022299999836832285
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-0.0293,  0.7249,  0.0394, -0.0346],
        [ 0.6371,  0.6190,  0.6891,  0.0296]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00203300011344254
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[ 0.0105,  0.6894,  0.0229, -0.0324],
        [ 0.6503,  0.7231,  0.6652, -0.0455]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020810000132769346
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9934, -1.0078, -2.0229, -2.0091],
        [-2.0060, -1.0146, -2.0052, -2.1014]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002078999998047948
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9197, -0.9850, -1.9892, -2.0252],
        [-1.9743, -1.0088, -1.9580, -2.1507]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020920000970363617
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0219, -0.9955, -2.0312, -2.0423],
        [-2.0051, -1.0057, -2.0434, -2.0844]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020580000709742308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0260, -0.9892, -2.0081, -2.0495],
        [-1.9877, -0.9952, -2.0150, -2.1460]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0347, -1.0079, -2.0192, -2.1374],
        [-2.0263, -1.0039, -2.0325, -2.2014]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021929999347776175
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.3468, 0.6937, 0.2432, 0.0068],
        [0.8556, 0.7076, 0.7363, 0.0524]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.4264, 0.6929, 0.3208, 0.0319],
        [0.8921, 0.7178, 0.7865, 0.0741]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.4269, 0.6985, 0.4001, 0.0597],
        [0.8779, 0.6661, 0.7858, 0.1001]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.4787, 0.6705, 0.3784, 0.0786],
        [0.8419, 0.6854, 0.7456, 0.0902]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.5017, 0.7317, 0.3336, 0.0938],
        [0.8136, 0.7441, 0.7057, 0.1306]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.5147, 0.7499, 0.4058, 0.1315],
        [0.7484, 0.7312, 0.6402, 0.0990]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 6, Time: 0.03274499997496605
Total Reward: [ 0.7 -6. ], Discounted: [ 0.6590361 -5.793465 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6590360999107361
scalar_q_values:tensor([[-1.9533, -1.0028, -1.9332, -2.5098],
        [-1.9686, -0.9957, -1.9897, -2.5794]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023670000955462456
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.4755, 0.6916, 0.4738, 0.1636],
        [0.6989, 0.6884, 0.6512, 0.1342]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.4711, 0.7398, 0.4858, 0.1421],
        [0.6790, 0.6824, 0.6559, 0.1368]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.00737199978902936
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.4948, 0.6422, 0.5040, 0.1717],
        [0.6512, 0.7298, 0.7046, 0.1860]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002213000087067485
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0418, -0.9431, -2.0284, -2.7411],
        [-2.0303, -1.0147, -2.0350, -2.7902]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002257999964058399
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.5431, 0.7224, 0.6700, 0.2680],
        [0.7042, 0.7390, 0.7662, 0.1994]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.5907, 0.6811, 0.6552, 0.2586],
        [0.7169, 0.7340, 0.8023, 0.2578]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.5429, 0.6574, 0.7004, 0.3077],
        [0.7034, 0.7441, 0.8199, 0.2409]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6134, 0.7141, 0.6939, 0.2737],
        [0.7291, 0.7116, 0.8123, 0.2850]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.5889, 0.7179, 0.7786, 0.3637],
        [0.7542, 0.6901, 0.7818, 0.2962]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6244, 0.7238, 0.6538, 0.3183],
        [0.7172, 0.6835, 0.7451, 0.3257]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6089, 0.6839, 0.6124, 0.3475],
        [0.6749, 0.6968, 0.6909, 0.3298]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 7, Time: 0.03615200147032738
Total Reward: [ 0.7 -7. ], Discounted: [ 0.65244573 -6.7255306 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6524457335472107
scalar_q_values:tensor([[0.5948, 0.7017, 0.6087, 0.3762],
        [0.7079, 0.8074, 0.7593, 0.3577]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021230001002550125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.5930, 0.7258, 0.5723, 0.4209],
        [0.6815, 0.6983, 0.6221, 0.4009]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0319, -0.9990, -1.9802, -3.0475],
        [-2.0131, -0.9886, -2.0148, -3.1053]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022539999336004257
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9916, -1.0172, -2.0053, -3.0482],
        [-1.9962, -0.9958, -1.9990, -3.0892]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9958, -0.9971, -1.9856, -3.0007],
        [-1.9508, -1.0093, -1.9015, -3.0516]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002076999982818961
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9669, -1.0234, -1.9431, -3.0134],
        [-1.9880, -0.9779, -1.9902, -3.0285]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002191999927163124
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6662, 0.7118, 0.7237, 0.4274],
        [0.7785, 0.7065, 0.8125, 0.4692]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6884, 0.6639, 0.7177, 0.4407],
        [0.7594, 0.6705, 0.7592, 0.4342]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.6741, 0.7218, 0.7520, 0.4553],
        [0.7330, 0.7388, 0.7597, 0.4515]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6787, 0.7149, 0.7387, 0.4390],
        [0.7235, 0.7314, 0.7192, 0.4518]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6579, 0.7057, 0.6722, 0.4440],
        [0.7150, 0.6793, 0.7304, 0.4299]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6261, 0.6856, 0.6729, 0.4894],
        [0.6682, 0.7317, 0.6439, 0.4938]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 6, Time: 0.028082000091671944
Total Reward: [ 0.7 -6. ], Discounted: [ 0.6590361 -5.793465 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6590360999107361
scalar_q_values:tensor([[0.6007, 0.6919, 0.5946, 0.4475],
        [0.6648, 0.6885, 0.6397, 0.4533]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6118, 0.6893, 0.6483, 0.4644],
        [0.6475, 0.6738, 0.6327, 0.4761]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6070, 0.7700, 0.6629, 0.4790],
        [0.6887, 0.6662, 0.7906, 0.5229]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6136, 0.7443, 0.6854, 0.4786],
        [0.7156, 0.7465, 0.6858, 0.4600]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007389000151306391
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.6547, 0.7088, 0.6967, 0.4711],
        [0.6958, 0.7006, 0.7127, 0.5027]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6856, 0.6852, 0.7146, 0.4613],
        [0.7267, 0.6716, 0.7693, 0.4902]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6813, 0.7359, 0.7144, 0.4734],
        [0.7205, 0.6872, 0.6691, 0.5105]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012950999662280083
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-1.9701, -0.9723, -1.9867, -2.9575],
        [-2.0063, -1.0015, -1.9997, -3.0047]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002247000113129616
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9881, -0.9944, -1.9944, -2.9628],
        [-2.0192, -1.0047, -1.9769, -3.0031]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020729999523609877
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0187, -0.9990, -1.9838, -3.0489],
        [-2.0070, -1.0089, -1.9804, -3.0186]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021210000850260258
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0205, -1.0018, -2.0081, -3.0442],
        [-2.0286, -1.0175, -2.0102, -3.0575]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0105, -0.9841, -1.9758, -2.9786],
        [-2.0193, -1.0026, -2.0117, -3.0930]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9986, -0.9760, -2.0166, -3.0710],
        [-1.9726, -0.9972, -1.9736, -3.0061]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021550001110881567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0026, -0.9976, -1.9867, -3.0120],
        [-2.0051, -0.9951, -2.0170, -3.0791]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021780000533908606
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6805, 0.7162, 0.6237, 0.4708],
        [0.7096, 0.7106, 0.7081, 0.4466]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002268000040203333
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6421, 0.7082, 0.7177, 0.4589],
        [0.7009, 0.7206, 0.6969, 0.4760]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023990001063793898
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6361, 0.7304, 0.7253, 0.4976],
        [0.6800, 0.7074, 0.7161, 0.5175]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002756000030785799
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6548, 0.7353, 0.6771, 0.4664],
        [0.5955, 0.6809, 0.7260, 0.5663]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002148000057786703
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6480, 0.7336, 0.6610, 0.4720],
        [0.6646, 0.7350, 0.6610, 0.4815]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00228299992159009
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0044, -1.0072, -2.0100, -2.9298],
        [-2.0240, -1.0085, -1.9957, -3.0311]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002311999909579754
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0135, -0.9755, -2.0012, -2.9522],
        [-2.0309, -0.9818, -1.9846, -2.9908]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021359999664127827
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0115, -0.9974, -1.9906, -3.0198],
        [-2.0540, -0.9621, -1.9809, -3.0630]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021230001002550125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7101, 0.6779, 0.6996, 0.4865],
        [0.7326, 0.6831, 0.6796, 0.5052]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.7180, 0.6723, 0.6924, 0.4803],
        [0.7495, 0.6618, 0.6792, 0.4960]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.7253, 0.6761, 0.7413, 0.4400],
        [0.7445, 0.6813, 0.6921, 0.4617]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.6787, 0.6537, 0.6447, 0.4738],
        [0.7108, 0.7007, 0.6779, 0.4604]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.6693, 0.7164, 0.6491, 0.4059],
        [0.6981, 0.7210, 0.6794, 0.4574]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 5, Time: 0.023111000657081604
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6656930446624756
scalar_q_values:tensor([[-1.9703, -0.9939, -2.0145, -3.0120],
        [-1.9810, -0.9977, -2.0182, -3.0512]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023270000237971544
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9788, -0.9926, -2.0187, -3.0470],
        [-1.9813, -1.0148, -2.0085, -3.0846]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002065999899059534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9981, -1.0053, -2.0059, -3.0428],
        [-1.9990, -1.0079, -2.0224, -3.0470]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022430000826716423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6665, 0.7074, 0.7267, 0.4426],
        [0.6530, 0.6959, 0.6925, 0.4519]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6715, 0.6984, 0.6934, 0.4482],
        [0.7072, 0.7347, 0.6949, 0.4450]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007584000006318092
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.6512, 0.7119, 0.7542, 0.4596],
        [0.7377, 0.7116, 0.6946, 0.4130]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6856, 0.6727, 0.6650, 0.4415],
        [0.7195, 0.6926, 0.6956, 0.4698]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.7014, 0.6871, 0.6976, 0.4646],
        [0.6685, 0.7621, 0.7191, 0.3855]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01256600022315979
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[0.7075, 0.6897, 0.6535, 0.4277],
        [0.7128, 0.6918, 0.6506, 0.4656]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.6161, 0.7356, 0.6208, 0.4652],
        [0.6795, 0.7246, 0.6696, 0.4716]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007586999796330929
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.6571, 0.6742, 0.6847, 0.4678],
        [0.6761, 0.6901, 0.6593, 0.4558]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020310000982135534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9771, -0.9915, -1.9542, -2.8518],
        [-1.9854, -1.0075, -2.0175, -3.0117]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021550001110881567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9872, -1.0009, -2.0023, -2.9739],
        [-1.9869, -1.0110, -1.9986, -3.0174]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002259999979287386
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9913, -1.0026, -2.0172, -2.9703],
        [-1.9866, -1.0291, -2.0298, -2.9784]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020580000709742308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9910, -0.9816, -1.9898, -2.9919],
        [-2.0014, -1.0185, -2.0046, -2.9450]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002213000087067485
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6854, 0.7362, 0.6881, 0.4970],
        [0.7248, 0.7519, 0.6862, 0.5126]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022140000946819782
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.7095, 0.6992, 0.6767, 0.5017],
        [0.7115, 0.7053, 0.6517, 0.5123]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.7036, 0.7291, 0.6710, 0.4629],
        [0.7084, 0.6872, 0.6493, 0.4968]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007317999843508005
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.6965, 0.7391, 0.6957, 0.4701],
        [0.7322, 0.7279, 0.6255, 0.5345]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6705, 0.7139, 0.7128, 0.4532],
        [0.6981, 0.7065, 0.7054, 0.4694]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023960000835359097
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9936, -1.0056, -2.0154, -3.0776],
        [-2.0202, -1.0021, -1.9911, -3.1400]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020959998946636915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6869, 0.6908, 0.6872, 0.4253],
        [0.6467, 0.6590, 0.7093, 0.4542]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6544, 0.7195, 0.7280, 0.3909],
        [0.6836, 0.7174, 0.7121, 0.4383]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6754, 0.6988, 0.7001, 0.4593],
        [0.6915, 0.6909, 0.7042, 0.4435]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.7034, 0.7100, 0.6994, 0.4264],
        [0.6858, 0.6890, 0.6610, 0.4394]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 4, Time: 0.018570000305771828
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6724172234535217
scalar_q_values:tensor([[-1.9697, -0.9679, -1.9842, -3.0382],
        [-1.9745, -1.0105, -1.9638, -3.0365]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002211000071838498
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0301, -0.9841, -1.9968, -3.0259],
        [-2.0169, -0.9990, -2.0209, -3.0958]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0093, -0.9926, -1.9900, -3.0271],
        [-2.0016, -0.9914, -1.9931, -3.0418]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6830, 0.6688, 0.6765, 0.4515],
        [0.7016, 0.7186, 0.6625, 0.4638]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002148000057786703
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6903, 0.6981, 0.6918, 0.4612],
        [0.6891, 0.7139, 0.6559, 0.4902]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021510000806301832
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6346, 0.7387, 0.7041, 0.4324],
        [0.6692, 0.6950, 0.6882, 0.5198]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025369999930262566
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6665, 0.6666, 0.6935, 0.4442],
        [0.6798, 0.6826, 0.7059, 0.4876]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6418, 0.6973, 0.7078, 0.4678],
        [0.6536, 0.7018, 0.7220, 0.4665]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6778, 0.6888, 0.6983, 0.4812],
        [0.6620, 0.7195, 0.6470, 0.4622]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013009999878704548
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-1.9936, -0.9929, -1.9926, -2.9662],
        [-2.0108, -1.0157, -1.9968, -3.0334]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002111000008881092
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9991, -0.9848, -1.9793, -3.0161],
        [-2.0226, -0.9808, -2.0501, -3.0686]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002070999937132001
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7331, 0.6977, 0.6948, 0.4570],
        [0.7226, 0.7259, 0.6808, 0.4932]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[0.7494, 0.6824, 0.7157, 0.4492],
        [0.6652, 0.7527, 0.6361, 0.4960]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007507000118494034
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.7134, 0.6611, 0.6765, 0.4452],
        [0.7106, 0.6938, 0.7071, 0.5111]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[0.6700, 0.6735, 0.6914, 0.4923],
        [0.6926, 0.7003, 0.6917, 0.5071]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007511000148952007
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9902, -1.0036, -1.9683, -3.0007],
        [-1.9699, -0.9734, -1.9572, -3.0562]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0130, -1.0083, -1.9873, -2.9943],
        [-2.0176, -0.9869, -2.0122, -3.0313]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020979999098926783
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6610, 0.7117, 0.6812, 0.4997],
        [0.6438, 0.6809, 0.6851, 0.4822]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002111000008881092
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9871, -1.0149, -2.0178, -2.9765],
        [-2.0115, -1.0282, -1.9955, -3.0306]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020729999523609877
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0067, -0.9969, -1.9885, -2.9824],
        [-1.9969, -0.9945, -2.0075, -3.0056]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021860001143068075
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0101, -1.0172, -1.9837, -2.9471],
        [-1.9967, -0.9904, -1.9763, -3.0051]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021269998978823423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0297, -1.0017, -1.9665, -2.9156],
        [-1.9776, -1.0048, -1.9948, -3.0072]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7460, 0.7324, 0.6723, 0.4441],
        [0.7081, 0.7439, 0.6824, 0.4885]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[0.7148, 0.6839, 0.6853, 0.4922],
        [0.7248, 0.6952, 0.6242, 0.5708]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.6682, 0.6661, 0.6414, 0.5994],
        [0.6851, 0.7585, 0.7510, 0.5651]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012498999945819378
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[-1.9963, -1.0035, -2.0214, -2.9470],
        [-1.9625, -0.9768, -1.9647, -3.0480]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020830000285059214
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6992, 0.6704, 0.6892, 0.5568],
        [0.7071, 0.7007, 0.6617, 0.5368]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.7008, 0.6797, 0.7549, 0.5727],
        [0.6753, 0.7212, 0.6833, 0.5243]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6749, 0.6985, 0.6948, 0.5156],
        [0.6851, 0.7228, 0.6667, 0.5440]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01233499962836504
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[0.6796, 0.7231, 0.6574, 0.5212],
        [0.6338, 0.6809, 0.7327, 0.5888]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.6783, 0.7331, 0.7111, 0.5753],
        [0.6917, 0.6872, 0.6894, 0.5499]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007441999856382608
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0012, -1.0064, -1.9936, -3.0380],
        [-1.9944, -1.0007, -1.9989, -3.0713]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002343999920412898
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7225, 0.6826, 0.6979, 0.4957],
        [0.7079, 0.6807, 0.6990, 0.5517]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[0.7205, 0.7048, 0.7030, 0.5225],
        [0.7052, 0.7585, 0.7352, 0.5509]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007482999935746193
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-2.0137, -0.9967, -2.0440, -3.0529],
        [-1.9624, -0.9857, -1.9872, -3.0553]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022430000826716423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6857, 0.6937, 0.6826, 0.5343],
        [0.6855, 0.7597, 0.6643, 0.5891]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021589999087154865
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0014, -0.9978, -2.0088, -3.0253],
        [-1.9757, -0.9820, -1.9928, -3.0651]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021840000990778208
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9719, -0.9853, -1.9963, -2.9762],
        [-2.0388, -1.0159, -2.0261, -3.0271]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021250001154839993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0024, -0.9984, -1.9953, -3.0286],
        [-2.0346, -0.9646, -1.9746, -3.0533]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002078999998047948
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6695, 0.7262, 0.7279, 0.5201],
        [0.6813, 0.6776, 0.6908, 0.5666]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6531, 0.6879, 0.6804, 0.4911],
        [0.6896, 0.6717, 0.6861, 0.5667]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.6459, 0.7084, 0.7174, 0.4994],
        [0.6347, 0.6859, 0.7063, 0.4907]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6854, 0.7053, 0.7121, 0.4995],
        [0.6972, 0.7384, 0.7000, 0.5445]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.017611000686883926
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6724172234535217
scalar_q_values:tensor([[-2.0114, -1.0092, -1.9843, -2.9903],
        [-2.0109, -1.0236, -1.9716, -3.0202]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021460000425577164
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6816, 0.6756, 0.6904, 0.5196],
        [0.6184, 0.7295, 0.7010, 0.5523]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002174000022932887
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6639, 0.6890, 0.6603, 0.4878],
        [0.6794, 0.6833, 0.6544, 0.5165]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021230001002550125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9519, -1.0130, -1.9499, -3.0176],
        [-2.0038, -1.0068, -2.0097, -3.0736]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002056000055745244
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9889, -0.9891, -2.0227, -2.9906],
        [-1.9943, -0.9969, -2.0088, -3.0193]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0024300001095980406
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7021, 0.6922, 0.6807, 0.4949],
        [0.6885, 0.6699, 0.6725, 0.5038]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[0.6715, 0.7097, 0.6977, 0.4499],
        [0.6518, 0.6881, 0.7542, 0.5474]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.7124, 0.6898, 0.7151, 0.5049],
        [0.7063, 0.6862, 0.6982, 0.5374]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6901, 0.6924, 0.7079, 0.4780],
        [0.6994, 0.6907, 0.6929, 0.5178]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6991, 0.7292, 0.7180, 0.5079],
        [0.6983, 0.6470, 0.6541, 0.5268]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 5, Time: 0.025557000190019608
Total Reward: [ 0.7 -5. ], Discounted: [ 0.66569304 -4.851985  ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6656930446624756
scalar_q_values:tensor([[-2.0024, -0.9932, -1.9829, -2.9846],
        [-2.0282, -1.0037, -1.9935, -3.0357]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002203000010922551
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9825, -1.0134, -1.9993, -3.0221],
        [-2.0014, -1.0138, -1.9719, -2.9995]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6594, 0.6991, 0.6470, 0.5117],
        [0.6647, 0.6628, 0.6554, 0.5672]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002111000008881092
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6709, 0.7106, 0.7190, 0.5579],
        [0.6744, 0.6986, 0.6667, 0.5531]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6799, 0.6929, 0.7050, 0.5051],
        [0.6920, 0.6819, 0.6512, 0.5996]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6792, 0.6710, 0.6910, 0.5274],
        [0.6821, 0.6992, 0.6940, 0.5125]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013585999608039856
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[0.6800, 0.6996, 0.7490, 0.5222],
        [0.6891, 0.7041, 0.7618, 0.5468]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.7101, 0.6833, 0.7168, 0.4447],
        [0.6874, 0.6998, 0.7081, 0.5364]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.7087, 0.6975, 0.6833, 0.4578],
        [0.7153, 0.7821, 0.7078, 0.4473]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012621000409126282
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[0.6686, 0.7114, 0.6646, 0.4532],
        [0.7060, 0.7101, 0.6727, 0.5108]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020540000405162573
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0128, -0.9702, -1.9576, -3.0193],
        [-1.9793, -0.9914, -1.9205, -3.0402]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021250001154839993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6859, 0.6956, 0.6987, 0.4556],
        [0.6927, 0.7054, 0.6683, 0.5237]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6714, 0.8112, 0.6745, 0.3912],
        [0.6988, 0.7094, 0.6750, 0.4968]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6927, 0.6828, 0.6936, 0.4765],
        [0.7070, 0.6778, 0.6727, 0.4852]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.7019, 0.6744, 0.6917, 0.4812],
        [0.6901, 0.6717, 0.6861, 0.4780]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[0.6993, 0.7091, 0.7168, 0.5042],
        [0.6814, 0.6821, 0.6802, 0.5554]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6967, 0.7273, 0.7159, 0.5177],
        [0.6337, 0.7336, 0.7254, 0.5661]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 4, Time: 0.018974000588059425
Total Reward: [ 0.7 -4. ], Discounted: [ 0.6724172 -3.900995 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6724172234535217
scalar_q_values:tensor([[0.6977, 0.6926, 0.7006, 0.5005],
        [0.6752, 0.7112, 0.6399, 0.5747]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002225999953225255
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.7011, 0.7267, 0.7173, 0.5141],
        [0.6909, 0.6651, 0.6766, 0.6030]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002078999998047948
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.6797, 0.6683, 0.6890, 0.5288],
        [0.6568, 0.7232, 0.6038, 0.5037]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021840000990778208
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-2.0262, -1.0004, -1.9917, -3.0499],
        [-2.0343, -1.0057, -1.9948, -3.0759]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021039999555796385
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0383, -1.0135, -1.9788, -2.9882],
        [-2.0358, -1.0033, -1.9978, -3.0841]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022430000826716423
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0179, -0.9960, -1.9904, -3.0080],
        [-2.0059, -0.9808, -1.9995, -3.1211]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021649999544024467
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0123, -1.0059, -2.0023, -3.0245],
        [-2.0012, -1.0097, -1.9759, -3.0854]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002131999935954809
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9997, -0.9656, -2.0246, -3.0183],
        [-1.9664, -0.9944, -2.0101, -3.0233]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002070999937132001
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6889, 0.6956, 0.6959, 0.5407],
        [0.6738, 0.6854, 0.6681, 0.5989]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.7112, 0.7112, 0.6963, 0.5214],
        [0.6503, 0.6661, 0.6745, 0.6317]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 2, Time: 0.007242000196129084
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.6926, 0.6903, 0.7484, 0.5401],
        [0.7107, 0.6904, 0.6699, 0.5908]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.7212, 0.6657, 0.7706, 0.5495],
        [0.6946, 0.6889, 0.6945, 0.5971]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6967, 0.7031, 0.6932, 0.5257],
        [0.6897, 0.7687, 0.6629, 0.5624]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01282500009983778
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[0.6753, 0.6981, 0.7238, 0.5276],
        [0.6979, 0.7037, 0.7214, 0.5879]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[0.6844, 0.7161, 0.7060, 0.5273],
        [0.6742, 0.7217, 0.6544, 0.5807]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.007284000050276518
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[-1.9938, -1.0128, -2.0006, -2.9824],
        [-1.9264, -0.9812, -1.8965, -2.9606]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020639998838305473
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6878, 0.6962, 0.6743, 0.5422],
        [0.6787, 0.6782, 0.6559, 0.6184]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[-1.9780, -1.0032, -1.9818, -2.8988],
        [-1.9913, -1.0043, -1.9994, -3.0202]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022849999368190765
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9716, -0.9886, -1.9929, -2.9487],
        [-1.9968, -1.0214, -2.0021, -3.0220]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021440000273287296
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6840, 0.6846, 0.6680, 0.5608],
        [0.6551, 0.6923, 0.7029, 0.5623]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[0.7248, 0.6725, 0.6946, 0.5010],
        [0.6703, 0.6891, 0.6388, 0.5405]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[0.7191, 0.7228, 0.6655, 0.5474],
        [0.6385, 0.6755, 0.6751, 0.6258]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.015088999643921852
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[0.7375, 0.6870, 0.7346, 0.4959],
        [0.6993, 0.7428, 0.6888, 0.6452]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022700000554323196
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6929999589920044
scalar_q_values:tensor([[0.7252, 0.7088, 0.6721, 0.5701],
        [0.6873, 0.6992, 0.7080, 0.6203]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[0.6834, 0.6874, 0.7024, 0.5791],
        [0.6607, 0.7073, 0.6848, 0.6175]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 2, Time: 0.0074269999749958515
Total Reward: [ 0.7 -2. ], Discounted: [ 0.68606997 -1.9701    ]
Scalarized Reward: 0.699999988079071, Discounted: 0.6860699653625488
scalar_q_values:tensor([[0.6856, 0.6857, 0.6856, 0.5838],
        [0.7058, 0.6567, 0.6706, 0.6385]], device='cuda:0')	action:0	policy_index:1
scalar_q_values:tensor([[0.7132, 0.7099, 0.6854, 0.6459],
        [0.6605, 0.6897, 0.6886, 0.6739]], device='cuda:0')	action:0	policy_index:0
scalar_q_values:tensor([[0.6814, 0.6904, 0.6784, 0.6386],
        [0.6839, 0.6583, 0.6920, 0.7597]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3537, 2.9022, 0.5049, 0.1717],
        [0.6872, 4.8462, 1.0043, 0.1222]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[0.8345, 4.5410, 3.4394, 0.9102],
        [1.1477, 5.1153, 4.0427, 0.9066]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 5, Time: 0.023302000015974045
Total Reward: [ 8.2 -5. ], Discounted: [ 7.798118 -4.851985]
Scalarized Reward: 8.199999809265137, Discounted: 7.7981181144714355
scalar_q_values:tensor([[0.7069, 0.6633, 0.7166, 0.6954],
        [0.7280, 0.6729, 0.6802, 0.8411]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.2927, 3.3750, 0.7890, 0.2415],
        [0.6110, 4.7718, 1.1364, 0.2855]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[0.9097, 5.8357, 4.1481, 1.1031],
        [1.2182, 6.6139, 4.8856, 1.2502]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013612999580800533
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0327, -0.9941, -1.9978, -2.9598],
        [-2.0284, -1.0005, -1.9955, -3.0560]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021879998967051506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6751, 0.7184, 0.7046, 0.8835],
        [0.6803, 0.7546, 0.7155, 0.9710]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.2606, 4.5902, 1.0808, 0.2734],
        [0.6306, 6.0039, 1.7408, 0.2755]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[ 1.1649, 10.9369,  6.8325,  1.3063],
        [ 1.4441, 12.0356,  7.5992,  1.2947]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01271199993789196
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9718, -0.9949, -2.0202, -3.0464],
        [-1.9589, -0.9926, -2.0492, -3.1145]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002130999928340316
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7140, 0.7813, 0.7168, 0.9317],
        [0.7200, 0.7948, 0.7336, 0.9998]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3717, 2.7100, 0.5029, 0.3900],
        [0.5675, 3.7682, 0.8716, 0.3578]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.2475, 8.6909, 5.5547, 1.3484],
        [1.4167, 9.6340, 6.7807, 1.4269]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013264000415802002
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0017, -0.9941, -2.0294, -3.0577],
        [-2.0119, -1.0108, -2.0291, -3.1353]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002085000043734908
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9926, -0.9959, -2.0255, -3.0093],
        [-2.0080, -0.9993, -2.0110, -3.1130]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002099999925121665
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0079, -1.0136, -2.0085, -3.0309],
        [-2.0286, -0.9955, -2.0345, -3.0963]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020409999415278435
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6811, 0.6986, 0.7140, 1.0057],
        [0.6451, 0.6939, 0.6527, 1.1462]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.2973, 2.9299, 0.5869, 0.0462],
        [0.6448, 3.7196, 0.7473, 0.2319]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.2544, 8.5732, 5.8375, 1.1416],
        [1.4118, 8.8511, 6.1683, 1.2850]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012802000157535076
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6599, 0.6744, 0.7153, 1.0263],
        [0.7009, 0.7563, 0.7354, 1.1294]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3318, 3.5481, 0.6654, 0.0092],
        [0.7060, 4.4682, 0.9915, 0.1578]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.2984, 8.6091, 5.7730, 1.2809],
        [1.4177, 7.9021, 5.7073, 1.3650]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01260599959641695
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0057, -0.9738, -2.0287, -3.0036],
        [-1.9527, -0.9523, -2.0403, -3.0147]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9935, -0.9968, -2.0108, -2.9482],
        [-1.9728, -1.0007, -2.0243, -3.0511]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002067999914288521
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9713, -1.0055, -1.9872, -2.9870],
        [-1.9659, -1.0024, -1.9646, -3.0334]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023399998899549246
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0236, -0.9857, -1.9781, -2.9738],
        [-2.0124, -0.9644, -1.9790, -2.8878]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020830000285059214
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7165, 0.7417, 0.7054, 1.1860],
        [0.7145, 0.7521, 0.6974, 1.3142]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3941, 3.1665, 0.5117, 0.3194],
        [0.7817, 4.9696, 1.0186, 0.3720]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.1410, 6.8471, 4.8448, 1.5693],
        [1.2696, 7.1707, 5.3309, 1.7111]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012392000295221806
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9843, -0.9657, -2.0000, -3.0331],
        [-1.9915, -1.0035, -2.0184, -3.0404]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6686, 0.7008, 0.7084, 1.2673],
        [0.7091, 0.7212, 0.6899, 1.4265]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3481, 3.4171, 0.5911, 0.2539],
        [0.6930, 5.2344, 1.0609, 0.3453]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.1339, 7.9057, 5.2107, 1.5699],
        [1.3837, 8.5175, 6.0424, 1.7629]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012411000207066536
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6828, 0.6910, 0.6908, 1.3068],
        [0.6672, 0.6501, 0.6523, 1.4655]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3560, 2.9257, 0.3626, 0.2837],
        [0.7206, 4.5878, 0.7535, 0.3568]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.1495, 7.6002, 5.0252, 1.5477],
        [1.4220, 8.1284, 5.4047, 1.5333]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012714000418782234
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0215, -1.0047, -2.0062, -2.9974],
        [-2.0208, -1.0068, -2.0185, -3.0444]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021840000990778208
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0203, -0.9967, -1.9865, -3.0097],
        [-2.0113, -1.0073, -2.0157, -3.0568]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0027389999013394117
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6490, 0.6991, 0.7223, 1.3876],
        [0.6844, 0.7046, 0.7086, 1.5059]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3126, 3.7622, 0.4353, 0.0131],
        [0.7458, 5.1813, 0.9127, 0.1560]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.2759, 8.0989, 5.3406, 1.6880],
        [1.4823, 8.4769, 5.9474, 1.7912]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01333600003272295
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9404, -1.0087, -1.9369, -3.0229],
        [-1.9944, -0.9979, -2.0081, -3.1094]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022070000413805246
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0056, -1.0128, -1.9718, -3.0237],
        [-1.9532, -0.9890, -1.9460, -3.0871]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021649999544024467
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0221, -1.0087, -1.9920, -3.0308],
        [-1.9986, -1.0044, -1.9957, -3.0497]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002087000058963895
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6874, 0.6930, 0.6900, 1.5850],
        [0.6898, 0.7393, 0.7101, 1.7285]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3444, 3.2095, 0.4420, 0.2916],
        [0.7008, 4.0146, 0.6320, 0.3966]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.3793, 9.1830, 5.8903, 1.7242],
        [1.5034, 8.1906, 5.9293, 1.9705]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01307899970561266
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6879, 0.7020, 0.7179, 1.6551],
        [0.6742, 0.6855, 0.7114, 1.8524]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3923, 3.9932, 0.5817, 0.2489],
        [0.7575, 5.0027, 0.9569, 0.2859]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.2329, 8.1914, 5.5210, 1.9265],
        [1.4761, 7.7991, 5.6684, 2.1416]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012779000215232372
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9877, -0.9983, -2.0098, -3.0016],
        [-1.9774, -1.0041, -1.9988, -3.0462]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020600000862032175
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9956, -1.0044, -1.9992, -2.9838],
        [-1.9625, -0.9920, -1.9800, -3.0220]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9848, -1.0008, -1.9774, -3.0036],
        [-1.9942, -0.9964, -2.0025, -3.0717]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002068999921903014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6945, 0.6829, 0.7059, 1.7717],
        [0.7188, 0.6819, 0.6794, 2.0206]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4103, 3.5783, 0.6889, 0.3053],
        [0.8094, 4.6752, 1.0179, 0.4982]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.4266, 8.9459, 5.9321, 1.9748],
        [1.6563, 8.1815, 5.8534, 2.1806]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012757999822497368
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6647, 0.6947, 0.6476, 1.7681],
        [0.7251, 0.6732, 0.6667, 1.8258]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4066, 4.4878, 0.8123, 0.1435],
        [0.7406, 5.9603, 1.4752, 0.0656]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.3472, 8.5077, 5.5237, 1.8930],
        [1.5867, 7.8877, 5.6748, 2.1970]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012663000263273716
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7144, 0.6621, 0.7510, 1.8076],
        [0.6754, 0.7080, 0.6644, 2.0331]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3603, 3.4089, 0.5663, 0.1587],
        [0.7013, 4.2959, 0.7448, 0.1878]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.4428, 8.0604, 5.4816, 1.9103],
        [1.5510, 7.1268, 5.2308, 2.1879]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012555999681353569
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9708, -0.9978, -1.9888, -3.0305],
        [-1.9825, -0.9673, -1.9661, -3.0944]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002131999935954809
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7054, 0.7348, 0.7234, 1.8128],
        [0.7377, 0.7101, 0.7320, 2.0762]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3025, 3.4283, 0.6058, 0.1355],
        [0.6117, 4.2426, 0.5874, 0.1762]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.4717, 9.0640, 6.0961, 1.8601],
        [1.6495, 8.8401, 6.2210, 2.0595]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012645999900996685
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6381, 0.6820, 0.7425, 1.8732],
        [0.6531, 0.7320, 0.6977, 2.1595]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3048, 2.7911, 0.4285, 0.2665],
        [0.6929, 4.4701, 0.7457, 0.2872]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.2995, 7.5230, 5.1928, 2.0805],
        [1.5135, 7.5101, 5.4126, 2.3614]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013318999670445919
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6704, 0.7014, 0.6368, 2.0148],
        [0.6856, 0.7119, 0.6577, 2.3093]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3670, 3.3787, 0.5830, 0.2489],
        [0.8099, 5.1085, 0.9615, 0.4275]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.4509, 8.5059, 5.8519, 2.2153],
        [1.6613, 8.5774, 6.1848, 2.4930]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013120999559760094
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9834, -0.9998, -1.9362, -3.0162],
        [-1.9865, -1.0105, -1.9779, -3.0969]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002357000019401312
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7020, 0.6828, 0.7200, 2.1857],
        [0.6959, 0.6938, 0.6919, 2.5161]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4175, 2.7267, 0.4861, 0.5381],
        [0.7542, 4.3430, 0.7408, 0.5900]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.4053, 8.0657, 5.6967, 2.4325],
        [1.5806, 8.0146, 5.9541, 2.6731]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013299999758601189
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7252, 0.7094, 0.7487, 2.3947],
        [0.6709, 0.7080, 0.6960, 2.6957]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.2951, 3.7763, 0.6935, 0.4215],
        [0.7706, 5.1412, 1.1145, 0.5981]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.4854, 9.4880, 6.4167, 2.3837],
        [1.7214, 9.2607, 6.6318, 2.7498]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012884000316262245
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0263, -1.0087, -2.0185, -2.9529],
        [-1.9961, -0.9730, -2.0216, -3.0190]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023040000814944506
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0060, -1.0201, -2.0228, -2.9146],
        [-2.0026, -1.0067, -2.0108, -3.0503]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021150000393390656
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7058, 0.7199, 0.6934, 2.4379],
        [0.7197, 0.7128, 0.6526, 2.6915]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3422, 3.2329, 0.6522, 0.4240],
        [0.7631, 3.9518, 0.7593, 0.6216]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.3460, 7.4537, 5.2416, 2.2960],
        [1.6813, 8.0249, 6.0240, 2.7317]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012525999918580055
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6844, 0.7630, 0.7275, 2.3063],
        [0.6819, 0.7187, 0.6913, 2.6654]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3282, 3.5190, 0.7088, 0.2184],
        [0.7823, 4.8544, 0.9063, 0.3293]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.3621, 6.9995, 4.8205, 2.1509],
        [1.6435, 8.4437, 6.1945, 2.4677]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012520000338554382
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6893, 0.6670, 0.7109, 2.2215],
        [0.6690, 0.6271, 0.7071, 2.5207]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3559, 3.2042, 0.5818, 0.1796],
        [0.7036, 4.8368, 0.8557, 0.1530]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.2931, 6.9966, 4.8720, 2.1454],
        [1.7222, 8.2817, 6.1305, 2.5461]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012814000248908997
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0065, -1.0093, -2.0104, -3.0170],
        [-2.0098, -1.0036, -2.0176, -3.0723]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002203000010922551
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0109, -0.9889, -2.0188, -3.0041],
        [-2.0187, -1.0180, -1.9988, -3.0864]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021569998934865
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9750, -0.9895, -1.9901, -3.0433],
        [-1.9827, -0.9798, -1.9906, -3.0827]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021170000545680523
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9803, -0.9899, -1.9599, -2.9051],
        [-1.9694, -1.0195, -2.0129, -3.0441]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002113000024110079
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0269, -1.0020, -1.9731, -3.0163],
        [-2.0498, -0.9803, -2.0031, -3.0258]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021170000545680523
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6661, 0.7100, 0.7109, 2.0496],
        [0.6535, 0.6890, 0.6854, 2.4799]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3246, 3.2635, 0.7147, 0.0881],
        [0.7463, 4.6056, 0.7116, 0.0080]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.4561, 8.5465, 5.7583, 2.0111],
        [1.6443, 8.4718, 6.1406, 2.3229]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012595999985933304
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0055, -1.0009, -2.0154, -2.9898],
        [-1.9788, -0.9867, -2.0216, -3.0282]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020959998946636915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9899, -0.9892, -2.0063, -2.9880],
        [-1.9811, -0.9963, -2.0085, -3.0315]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021059999708086252
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0157, -1.0033, -1.9988, -2.9871],
        [-2.0037, -1.0246, -2.0317, -3.0534]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023270000237971544
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0174, -0.9893, -1.9904, -2.9908],
        [-1.9674, -1.0340, -1.9814, -2.9709]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002174000022932887
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0039, -0.9852, -2.0026, -2.9783],
        [-1.9736, -0.9995, -2.0304, -3.0397]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0156, -0.9980, -2.0284, -2.9426],
        [-2.0038, -0.9806, -1.9572, -2.9937]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021460000425577164
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6926, 0.6595, 0.6045, 1.8350],
        [0.7097, 0.6731, 0.6425, 2.4115]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.3112,  3.9067,  0.7675, -0.0371],
        [ 0.7223,  4.7742,  0.8439,  0.1415]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.4952, 8.9096, 6.1060, 2.1075],
        [1.5375, 7.7267, 5.6570, 2.4301]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013799999840557575
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0130, -1.0187, -1.9905, -2.9895],
        [-1.9708, -1.0260, -2.0205, -3.0734]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002225999953225255
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6413, 0.6485, 0.6601, 2.0263],
        [0.6556, 0.7124, 0.7391, 2.3709]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.2947, 3.2019, 0.6419, 0.2376],
        [0.7021, 4.3627, 0.8288, 0.2894]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.4001, 8.8008, 6.1413, 2.1303],
        [1.4986, 8.0029, 5.7624, 2.3994]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013914000242948532
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0183, -0.9862, -1.9973, -2.9631],
        [-1.9715, -1.0165, -1.9690, -3.0292]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002343999920412898
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7044, 0.6884, 0.6831, 2.0461],
        [0.7560, 0.7247, 0.6681, 2.2742]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.2871, 3.5284, 0.6360, 0.1518],
        [0.7309, 5.0227, 1.0450, 0.2515]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.4217, 8.3055, 5.8315, 2.2153],
        [1.5827, 8.4020, 6.0991, 2.5413]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013580000028014183
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6637, 0.6613, 0.7171, 2.1011],
        [0.6977, 0.6685, 0.6466, 2.5154]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.2983, 2.9617, 0.5339, 0.2255],
        [0.6544, 4.3171, 0.6468, 0.3703]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.2090, 8.1749, 5.5545, 2.0408],
        [1.3508, 7.9384, 5.7373, 2.4739]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012498999945819378
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6871, 0.7325, 0.7088, 2.1637],
        [0.7136, 0.7739, 0.7407, 2.6105]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3962, 3.6344, 0.7150, 0.0642],
        [0.8146, 5.0836, 0.9191, 0.1305]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.3610, 8.5461, 5.8796, 2.1765],
        [1.6482, 8.9888, 6.3727, 2.5700]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012589000165462494
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0031, -0.9962, -2.0011, -3.0075],
        [-1.9955, -1.0069, -2.0006, -3.0919]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021210000850260258
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7297, 0.6175, 0.6645, 2.2237],
        [0.6458, 0.6622, 0.6306, 2.7278]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3936, 3.2812, 0.6907, 0.1457],
        [0.6742, 4.5479, 0.6919, 0.2435]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.2857, 7.7262, 5.3303, 2.3382],
        [1.6194, 8.5483, 5.9615, 2.4728]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012900999747216702
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9898, -1.0039, -1.9834, -2.9522],
        [-2.0079, -1.0024, -2.0027, -3.0286]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022279999684542418
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6929, 0.7384, 0.6578, 2.1517],
        [0.6998, 0.7529, 0.7130, 2.7318]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.2941,  3.5432,  0.6074, -0.0395],
        [ 0.6980,  4.8152,  0.7245,  0.0411]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.2444, 7.1103, 4.9884, 2.2847],
        [1.5609, 7.9385, 5.6899, 2.5873]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01261099986732006
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9705, -0.9885, -1.9856, -2.8932],
        [-1.9858, -0.9924, -1.9745, -2.9927]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002133999951183796
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0024, -0.9950, -2.0080, -2.9456],
        [-1.9933, -0.9894, -1.9935, -3.0267]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020920000970363617
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6888, 0.7078, 0.6832, 2.1588],
        [0.6842, 0.7024, 0.6811, 2.6810]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.2639,  3.5763,  0.5905, -0.0816],
        [ 0.6536,  4.6387,  0.7325,  0.0776]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.3616, 8.6288, 5.8032, 2.0220],
        [1.6180, 8.6607, 6.2550, 2.5028]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012668999843299389
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7212, 0.6642, 0.7012, 2.1383],
        [0.6594, 0.7057, 0.6598, 2.6296]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3192, 3.0939, 0.5764, 0.1083],
        [0.6800, 4.1892, 0.6768, 0.2235]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.3177, 8.1421, 5.5824, 2.1953],
        [1.5978, 7.8751, 5.9522, 2.6511]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012524000369012356
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0189, -1.0120, -1.9883, -2.9101],
        [-2.0045, -1.0104, -2.0042, -3.0707]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020940001122653484
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9665, -1.0023, -1.9821, -2.9892],
        [-2.0312, -0.9997, -1.9599, -3.0667]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023479999508708715
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9936, -0.9914, -1.9837, -3.0054],
        [-1.9746, -0.9895, -1.9854, -3.0520]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00267999991774559
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0013, -1.0117, -2.0332, -2.9572],
        [-1.9631, -0.9871, -1.9599, -3.0790]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002827000105753541
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9849, -1.0138, -1.9834, -2.9235],
        [-1.9919, -0.9710, -2.0109, -3.0524]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021609999239444733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7193, 0.6800, 0.7513, 2.1862],
        [0.6658, 0.7170, 0.6872, 2.7213]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3532, 3.1419, 0.6872, 0.1982],
        [0.8820, 5.3830, 0.9493, 0.1528]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.3857, 7.6373, 5.4140, 2.3357],
        [1.6887, 8.4366, 6.1645, 2.5796]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01460499968379736
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7084, 0.6824, 0.7187, 2.0091],
        [0.6532, 0.7365, 0.7942, 2.5721]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.3791, 2.9447, 0.6465, 0.1301],
        [0.8305, 4.9689, 0.8983, 0.2108]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.4649, 7.8651, 5.5503, 2.3947],
        [1.7899, 8.4472, 6.2282, 2.6782]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.014421000145375729
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9873, -0.9612, -1.9663, -2.9390],
        [-2.0093, -0.9904, -2.0026, -3.0697]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002268000040203333
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9859, -1.0024, -2.0003, -2.9795],
        [-1.9928, -0.9964, -2.0039, -3.0626]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021379999816417694
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9935, -1.0225, -1.9834, -2.9924],
        [-1.9852, -1.0131, -2.0174, -3.0109]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021059999708086252
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6922, 0.7008, 0.7104, 2.0287],
        [0.6832, 0.6864, 0.7049, 2.6112]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4117, 3.3136, 0.7577, 0.0800],
        [0.8617, 4.1463, 0.6204, 0.3097]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.5467, 8.5158, 5.9712, 2.2774],
        [1.7940, 8.2660, 6.0256, 2.6544]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01272599957883358
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7003, 0.7083, 0.7101, 2.0453],
        [0.6809, 0.6849, 0.6456, 2.6474]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.5405, 3.7490, 0.9740, 0.0648],
        [0.9825, 5.0744, 0.8801, 0.1663]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.5912, 8.2890, 5.8318, 2.2518],
        [1.8237, 7.7692, 5.7768, 2.8685]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012780999764800072
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9948, -0.9997, -1.9717, -2.9587],
        [-2.0040, -1.0026, -2.0279, -3.0629]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021649999544024467
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9703, -0.9740, -2.0238, -3.0240],
        [-2.0119, -1.0031, -2.0569, -3.0523]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021669999696314335
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7091, 0.6946, 0.6752, 2.0411],
        [0.6998, 0.6929, 0.6050, 2.5944]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4740,  3.7220,  0.7407, -0.0486],
        [ 0.9767,  4.8427,  0.7806,  0.0942]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.5065, 8.2306, 5.7695, 2.2167],
        [1.7118, 7.9399, 5.9274, 2.6392]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012701000086963177
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9872, -0.9821, -1.9895, -2.9400],
        [-2.0116, -0.9821, -1.9933, -3.0353]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023139999248087406
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6678, 0.6866, 0.6847, 1.9785],
        [0.6797, 0.6932, 0.6875, 2.5769]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4149, 3.2681, 0.6880, 0.0896],
        [0.9662, 4.6083, 0.7649, 0.1503]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.5193, 8.3928, 5.9230, 2.2302],
        [1.8329, 8.5246, 6.2878, 2.6294]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.017791999503970146
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0194, -1.0137, -2.0240, -2.9671],
        [-2.0054, -0.9878, -2.0008, -2.9927]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002309999894350767
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0155, -1.0073, -2.0367, -2.9843],
        [-1.9987, -1.0001, -2.0102, -3.0356]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021669999696314335
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0213, -0.9986, -1.9982, -2.9732],
        [-2.0259, -0.9767, -2.0064, -3.0878]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9993, -0.9940, -2.0035, -2.9621],
        [-1.9872, -0.9996, -2.0005, -3.0466]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021989999804645777
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0248, -1.0306, -1.9855, -2.9177],
        [-2.0043, -0.9980, -2.0118, -3.0715]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020620001014322042
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0097, -1.0022, -1.9767, -2.8954],
        [-1.9790, -0.9889, -2.0005, -3.0433]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002007999923080206
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6646, 0.6657, 0.6708, 1.8107],
        [0.6703, 0.7396, 0.6861, 2.3624]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.3225,  3.3967,  0.6740, -0.0885],
        [ 0.9453,  5.1893,  0.8801, -0.0656]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.3974, 7.6358, 5.5757, 2.1403],
        [1.6602, 7.9611, 5.9376, 2.5960]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012438000179827213
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9830, -1.0004, -1.9976, -2.9885],
        [-1.9938, -1.0016, -2.0139, -3.0954]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020580000709742308
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9906, -0.9768, -1.9812, -2.9461],
        [-2.0036, -0.9980, -2.0152, -3.0865]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0026149998884648085
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6996, 0.7174, 0.7151, 1.6518],
        [0.7097, 0.7376, 0.7031, 2.4396]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 4.0786e-01,  3.3979e+00,  7.0673e-01, -3.6246e-03],
        [ 9.9885e-01,  4.6391e+00,  8.0537e-01,  2.0437e-01]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.5817, 8.9514, 6.3214, 2.0974],
        [1.7803, 8.7299, 6.4595, 2.5681]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013194999657571316
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6908, 0.6850, 0.6271, 1.6849],
        [0.7052, 0.7084, 0.6882, 2.5648]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4639, 3.1703, 0.6580, 0.1026],
        [0.9298, 4.2409, 0.6781, 0.2765]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.3194, 7.4416, 5.2931, 2.3347],
        [1.6943, 7.6995, 5.9190, 2.7502]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013330000452697277
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6840, 0.7286, 0.7006, 1.8622],
        [0.6816, 0.7259, 0.6949, 2.7519]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4506,  3.6783,  0.8007, -0.0163],
        [ 1.0165,  4.9315,  0.8036,  0.2299]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.6513, 8.2777, 5.9722, 2.5003],
        [1.7929, 8.2112, 6.0823, 2.3335]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013783000409603119
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6961, 0.6846, 0.7046, 1.8055],
        [0.6960, 0.6493, 0.6612, 2.7210]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4515,  3.4867,  0.7163, -0.0735],
        [ 0.9588,  4.6761,  0.7234,  0.3035]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7046, 8.5780, 6.1497, 2.2045],
        [1.8698, 8.0944, 6.1002, 2.7592]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.015660999342799187
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9514, -0.9900, -2.0104, -3.0071],
        [-2.0084, -0.9927, -2.0134, -3.1081]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002693000016734004
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7297, 0.7596, 0.6659, 1.7478],
        [0.6837, 0.7212, 0.6684, 2.6208]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5012,  3.2630,  0.5834, -0.0246],
        [ 1.0140,  4.6319,  0.6826,  0.1066]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.6377, 8.1730, 5.8035, 2.1824],
        [1.9687, 8.1663, 6.1288, 2.5738]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.015982000157237053
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7294, 0.6702, 0.6986, 1.5701],
        [0.6569, 0.7125, 0.7053, 2.5683]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4652,  3.0744,  0.5313, -0.1095],
        [ 1.0862,  4.8136,  0.6987,  0.1021]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.6360, 8.3419, 5.8949, 2.0249],
        [1.9710, 8.4639, 6.2360, 2.6700]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.016334999352693558
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6905, 0.7028, 0.7005, 1.6379],
        [0.6862, 0.7057, 0.6738, 2.6155]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4780,  3.1245,  0.5764, -0.0451],
        [ 1.1061,  4.7102,  0.8088,  0.1631]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.6599, 7.7154, 5.5604, 2.2151],
        [1.9457, 8.0068, 6.1066, 2.6396]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.015471000224351883
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0008, -0.9823, -2.0366, -2.9285],
        [-2.0201, -0.9888, -2.0082, -3.0529]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002713999943807721
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7315, 0.6960, 0.6831, 1.6720],
        [0.6996, 0.6925, 0.6563, 2.7026]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5299,  3.6674,  0.7199, -0.0539],
        [ 1.0982,  4.6414,  0.6480,  0.3012]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7053, 8.1113, 5.8206, 2.1711],
        [1.9913, 8.2267, 6.2252, 2.6934]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.015376999974250793
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9961, -1.0238, -1.9807, -3.0587],
        [-1.9761, -0.9908, -2.0028, -3.0966]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0029909999575465918
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9610, -0.9648, -1.9793, -2.9362],
        [-1.9897, -0.9761, -1.9788, -3.1130]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025390000082552433
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6785, 0.6992, 0.7001, 1.6023],
        [0.6681, 0.7002, 0.7328, 2.6933]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4846,  3.4366,  0.7102, -0.0128],
        [ 1.1035,  4.1357,  0.5102,  0.3121]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7414, 8.3613, 6.1048, 1.9924],
        [1.9789, 8.1291, 6.2616, 2.6287]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.015560000203549862
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7400, 0.7763, 0.6923, 1.3957],
        [0.6924, 0.6868, 0.7066, 2.6773]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4042, 3.2032, 0.7902, 0.0633],
        [1.1407, 4.4437, 0.6350, 0.2222]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.014182999730110168
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: 0.699999988079071, Discounted: 0.679209291934967
scalar_q_values:tensor([[0.6747, 0.7270, 0.6783, 1.4397],
        [0.6848, 0.6779, 0.5694, 2.6722]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4968,  3.2224,  0.7023, -0.0524],
        [ 1.2621,  4.7090,  0.7273,  0.0756]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7216, 8.3304, 5.8749, 2.0240],
        [2.0226, 8.1670, 6.1181, 2.6155]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.016502000391483307
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6910, 0.7071, 0.7113, 1.4705],
        [0.6561, 0.6799, 0.7047, 2.7321]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4626,  3.0941,  0.6209, -0.1144],
        [ 1.2692,  4.4216,  0.6618,  0.1482]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7468, 8.0315, 5.5508, 2.0885],
        [2.0015, 8.1261, 5.9863, 2.5834]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.015137000009417534
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9996, -0.9847, -2.0253, -2.9749],
        [-2.0031, -1.0088, -1.9966, -3.0518]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0030980000738054514
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6991, 0.6832, 0.6748, 1.3822],
        [0.6876, 0.6995, 0.6835, 2.7641]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5120,  3.2914,  0.6574, -0.1756],
        [ 1.2529,  4.8174,  0.6713,  0.0847]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7739, 8.2002, 5.4604, 1.8801],
        [2.0551, 8.2379, 5.7612, 2.4716]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01584799960255623
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0028, -0.9849, -1.9503, -2.9862],
        [-2.0138, -0.9958, -1.9850, -3.0796]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002827000105753541
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0027, -1.0068, -2.0067, -3.0022],
        [-1.9700, -1.0317, -1.9394, -3.0526]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.003031000029295683
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6863, 0.7449, 0.7048, 1.1175],
        [0.6651, 0.6795, 0.6625, 2.5415]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5605,  3.5666,  0.7130, -0.2701],
        [ 1.2594,  4.9318,  0.7137, -0.0150]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7319, 8.2120, 5.2422, 1.6085],
        [2.1390, 7.9635, 5.2677, 2.1891]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.016333000734448433
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0035, -0.9953, -1.9917, -2.9510],
        [-1.9627, -1.0061, -1.9997, -3.0486]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0025220001116394997
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0067, -1.0560, -2.0173, -2.9574],
        [-1.9769, -1.0095, -2.0005, -3.0246]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0027050001081079245
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7020, 0.6831, 0.6866, 1.0733],
        [0.6857, 0.6914, 0.6689, 2.5272]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4728,  3.0208,  0.5482, -0.2437],
        [ 1.2519,  4.6390,  0.6365,  0.0849]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7360, 8.2127, 5.1702, 1.7138],
        [2.0361, 8.2654, 5.3956, 2.4200]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.014802999794483185
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6800, 0.6701, 0.6542, 1.0267],
        [0.6721, 0.6904, 0.6690, 2.5885]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4522,  3.2059,  0.6009, -0.2048],
        [ 1.2288,  4.8212,  0.6747,  0.1875]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.6785, 8.0282, 5.0228, 1.8676],
        [1.9515, 8.3514, 5.3904, 2.5369]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01345400046557188
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7074, 0.7106, 0.6802, 0.9977],
        [0.6944, 0.7172, 0.7034, 2.7170]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4505,  3.2243,  0.5985, -0.3023],
        [ 1.2516,  4.9614,  0.7644,  0.2182]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.6316, 7.6057, 4.8432, 1.8689],
        [1.9893, 8.1434, 5.1771, 2.4636]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012671000324189663
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0276, -1.0085, -2.0008, -2.9989],
        [-1.9984, -1.0096, -2.0036, -3.0363]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0020830000285059214
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6873, 0.7018, 0.6836, 0.9472],
        [0.6962, 0.6966, 0.6894, 2.6632]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4611,  3.3231,  0.6311, -0.3359],
        [ 1.2023,  4.6608,  0.6488,  0.1586]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.6698, 8.2943, 4.9464, 1.7205],
        [2.0121, 8.3431, 5.2181, 2.3666]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013732999563217163
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9938, -0.9672, -1.9823, -3.0014],
        [-1.9655, -0.9888, -1.9903, -3.0569]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021780000533908606
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6766, 0.6657, 0.6898, 0.8980],
        [0.6777, 0.6824, 0.6857, 2.6167]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4416,  3.2351,  0.6763, -0.3250],
        [ 1.1023,  4.5876,  0.6797,  0.2716]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.6571, 8.3045, 4.8861, 1.5756],
        [1.9271, 8.0179, 4.9925, 2.3297]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013418000191450119
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9960, -0.9978, -2.0019, -2.9767],
        [-2.0440, -1.0352, -1.9469, -2.9811]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002191999927163124
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9904, -1.0031, -1.9924, -2.9652],
        [-1.9994, -0.9906, -1.9922, -3.0356]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00240899994969368
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0058, -0.9947, -1.9942, -2.9776],
        [-1.9852, -1.0082, -1.9739, -3.0292]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002205000026151538
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7065, 0.6815, 0.7034, 0.6943],
        [0.6929, 0.6964, 0.6695, 2.6760]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4454,  3.1068,  0.5591, -0.4703],
        [ 1.2065,  4.8222,  0.6889,  0.1617]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7656, 8.7765, 4.9729, 1.5451],
        [1.9239, 8.2110, 5.0247, 2.4707]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012737000361084938
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6961, 0.7627, 0.7270, 0.6934],
        [0.6695, 0.6920, 0.6956, 2.6260]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.3324,  3.0623,  0.6728, -0.3031],
        [ 1.1103,  5.0575,  0.6977,  0.2119]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.6056, 8.2254, 4.7668, 1.6691],
        [1.9495, 8.1212, 5.0272, 2.6329]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012659000232815742
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0011, -1.0027, -2.0055, -2.9698],
        [-1.9818, -1.0005, -1.9884, -3.0161]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002248999895527959
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7353, 0.7263, 0.6977, 0.4813],
        [0.6694, 0.6825, 0.7116, 2.6868]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4737,  3.1962,  0.7681, -0.4514],
        [ 1.1940,  4.8748,  0.6859,  0.2548]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.5534, 8.6204, 4.8470, 1.6078],
        [1.9504, 8.3080, 5.0782, 2.5003]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012907000258564949
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7045, 0.7449, 0.7039, 0.4591],
        [0.6696, 0.7008, 0.7289, 2.8170]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4422,  3.4375,  0.6559, -0.5367],
        [ 1.1824,  4.7473,  0.6322,  0.2301]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7139, 7.9123, 4.6464, 1.5659],
        [1.9592, 7.9648, 4.9300, 2.5550]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01233499962836504
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7013, 0.7282, 0.6895, 0.3671],
        [0.6763, 0.7223, 0.6748, 2.7347]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4333,  3.1686,  0.5576, -0.5617],
        [ 1.2167,  4.9920,  0.6384,  0.1885]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.5674, 7.9545, 4.5842, 1.5485],
        [1.9617, 8.1803, 4.8681, 2.4751]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013473999686539173
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7145, 0.7162, 0.7006, 0.2653],
        [0.6791, 0.6580, 0.6867, 2.7066]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4558,  2.9711,  0.5422, -0.6468],
        [ 1.1879,  4.6381,  0.6658,  0.1849]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.6772, 8.3321, 4.6927, 1.4996],
        [1.9476, 7.9998, 4.7837, 2.4539]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013403000310063362
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6957, 0.7011, 0.6871, 0.1811],
        [0.6803, 0.7168, 0.7197, 2.7091]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4391,  3.4107,  0.7004, -0.6952],
        [ 1.1967,  5.0277,  0.7655,  0.0737]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.6852, 8.2519, 4.5928, 1.4527],
        [1.9540, 8.0149, 4.6764, 2.4533]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012981000356376171
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9636, -0.9535, -2.0135, -2.9753],
        [-2.0303, -1.0096, -2.0291, -3.0308]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002133999951183796
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6984, 0.7384, 0.6742, 0.0592],
        [0.6790, 0.7118, 0.6540, 2.7079]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 4.0479e-01,  3.4187e+00,  6.0960e-01, -5.6688e-01],
        [ 1.1958e+00,  4.4869e+00,  6.3209e-01, -3.2591e-03]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7087, 8.2790, 4.4255, 1.2447],
        [2.1103, 8.4484, 4.8551, 2.3822]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01295899972319603
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.7184,  0.7261,  0.6560, -0.0516],
        [ 0.7168,  0.6882,  0.6624,  2.7005]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4830,  3.4541,  0.6930, -0.8177],
        [ 1.3427,  4.6928,  0.6128,  0.1406]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7613, 8.5824, 4.6024, 1.3279],
        [2.0737, 8.1712, 4.6507, 2.4351]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013484999537467957
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9974, -1.0170, -2.0132, -2.9592],
        [-2.0203, -1.0089, -2.0193, -3.0366]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00215000007301569
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6949, 0.7421, 0.7089, 0.0057],
        [0.6689, 0.6795, 0.6727, 2.4877]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4791,  3.3366,  0.6885, -0.7875],
        [ 1.3151,  4.7532,  0.6616,  0.0941]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7015, 8.0457, 4.3443, 1.3347],
        [1.9522, 7.8947, 4.6223, 2.3186]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012923999689519405
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9989, -1.0013, -1.9904, -3.0095],
        [-1.9913, -0.9633, -2.0351, -2.9960]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00215000007301569
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7099, 0.7073, 0.6970, 0.0212],
        [0.6948, 0.6932, 0.6759, 2.6720]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5233,  3.5716,  0.6469, -0.7299],
        [ 1.4234,  5.0129,  0.6706,  0.0984]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7954, 8.2531, 4.3724, 1.3511],
        [2.1334, 8.2745, 4.6038, 2.3630]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012757999822497368
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0136, -0.9978, -1.9848, -2.9893],
        [-2.0178, -0.9922, -1.9827, -3.0533]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0020890000741928816
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6826,  0.7018,  0.6858, -0.0171],
        [ 0.6993,  0.7064,  0.6775,  2.7537]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5039,  3.1452,  0.6105, -0.7230],
        [ 1.2595,  4.2924,  0.5693,  0.1905]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8147, 8.4593, 4.5779, 1.5197],
        [2.1173, 8.3430, 4.6789, 2.4192]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.014530000276863575
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6712, 0.7120, 0.6994, 0.0621],
        [0.6941, 0.7187, 0.7298, 2.7690]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4979,  3.5459,  0.6560, -0.6937],
        [ 1.4346,  4.7178,  0.8414,  0.1940]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8370, 8.4576, 4.5149, 1.5502],
        [2.1320, 8.2976, 4.6165, 2.4502]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01636499911546707
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 6.6002e-01,  7.1943e-01,  6.8649e-01, -1.2409e-03],
        [ 6.8279e-01,  7.0373e-01,  6.4528e-01,  2.7890e+00]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4945,  3.4044,  0.6673, -0.7787],
        [ 1.3898,  4.7628,  0.6738,  0.1709]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7760, 7.6408, 4.2125, 1.7174],
        [2.0974, 7.5183, 4.0974, 2.5233]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.015898000448942184
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.9766, -2.0921, -2.0622, -3.0765],
        [-3.0372, -2.1792, -2.0683, -3.2834]], device='cuda:0')	action:2	policy_index:0
scalar_q_values:tensor([[-1.9884, -1.0168, -2.0056, -3.0042],
        [-1.9921, -1.0112, -1.9619, -3.0026]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013953999616205692
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[-2.0028, -1.0043, -1.9909, -2.9750],
        [-2.0158, -1.0137, -2.0089, -3.0799]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002644000109285116
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9594, -0.9908, -1.9726, -2.9848],
        [-1.9953, -0.9936, -1.9919, -3.0773]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.003008000086992979
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7222, 0.6997, 0.7081, 0.0369],
        [0.6841, 0.7291, 0.7079, 2.6284]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5219,  3.4223,  0.6679, -0.5737],
        [ 1.5439,  4.9506,  0.7807,  0.2092]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8190, 8.2326, 4.2203, 1.7282],
        [2.3308, 8.2379, 4.3737, 2.4885]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.015312000177800655
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7030, 0.6871, 0.6683, 0.1179],
        [0.7093, 0.7143, 0.5697, 2.9225]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5609,  3.5369,  0.6707, -0.5462],
        [ 1.5390,  4.6907,  0.6891,  0.3467]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8277, 8.2650, 4.2921, 1.9232],
        [2.1997, 8.3410, 4.4521, 2.3624]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.0157299991697073
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9958, -1.0068, -2.0473, -3.0062],
        [-1.9739, -1.0067, -2.0104, -3.0563]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0026700000744313
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9963, -1.0112, -2.0292, -3.0421],
        [-1.9998, -1.0023, -2.0101, -3.0574]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002747999969869852
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9654, -0.9714, -1.9977, -2.9811],
        [-1.9916, -1.0197, -2.0234, -3.0167]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0028840000741183758
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7064, 0.6951, 0.6778, 0.0746],
        [0.7100, 0.6910, 0.6633, 2.2976]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5616,  3.2554,  0.6332, -0.5724],
        [ 1.4975,  4.4512,  0.6870,  0.1108]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8608, 8.3555, 4.4185, 1.4721],
        [2.1631, 8.2354, 4.4595, 2.3740]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.018787000328302383
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9967, -0.9615, -1.9405, -2.9280],
        [-1.9710, -0.9858, -1.9846, -2.9937]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.003077999921515584
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0221, -0.9911, -1.9939, -3.0032],
        [-2.0909, -1.0110, -1.9992, -3.0579]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002982999896630645
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9782, -1.0151, -2.0184, -3.0543],
        [-2.0405, -0.9901, -1.9939, -3.0457]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0026229999493807554
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.7076,  0.7172,  0.6771, -0.0315],
        [ 0.6696,  0.7901,  0.6341,  2.7605]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5817,  3.1463,  0.6215, -0.5783],
        [ 1.6313,  4.6676,  0.6882,  0.2598]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9651, 8.3284, 4.3313, 1.7882],
        [2.0610, 7.9447, 4.2158, 2.5004]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.016801999881863594
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9653, -0.9810, -2.0120, -3.0100],
        [-1.9793, -0.9707, -1.9905, -3.0220]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002644999884068966
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6955,  0.6855,  0.6821, -0.0394],
        [ 0.6942,  0.6763,  0.6732,  2.7206]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.6048,  3.3797,  0.7242, -0.5093],
        [ 1.5572,  4.7236,  0.7753,  0.1573]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8678, 8.5510, 4.5002, 1.6457],
        [2.1472, 8.2744, 4.4444, 2.3610]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.016037000343203545
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0011, -1.0056, -2.0012, -2.9944],
        [-1.9749, -1.0212, -1.9836, -2.9731]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00279600010253489
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6545, 0.6986, 0.6998, 0.1801],
        [0.6625, 0.6890, 0.6673, 2.6371]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.6048,  3.5789,  0.6642, -0.5941],
        [ 1.5909,  4.7501,  0.7346,  0.0382]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8624, 8.2480, 4.2429, 1.6214],
        [2.1514, 7.9931, 4.3488, 2.4393]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01623700000345707
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7188, 0.7509, 0.7118, 0.0252],
        [0.6888, 0.6727, 0.6775, 2.6920]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5886,  3.3471,  0.7665, -0.5969],
        [ 1.6182,  4.9743,  0.8069,  0.0668]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9531, 7.2717, 3.6856, 1.7807],
        [2.2409, 8.3579, 4.4992, 2.4571]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.014154000207781792
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7070, 0.6953, 0.7090, 0.0556],
        [0.6610, 0.6921, 0.7004, 2.7621]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.6509,  3.3296,  0.7197, -0.5680],
        [ 1.6638,  4.7359,  0.6604,  0.1843]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9024, 8.3532, 4.2293, 1.6231],
        [2.2311, 8.3248, 4.3964, 2.4186]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013965999707579613
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.6943,  0.7036,  0.7175, -0.0610],
        [ 0.7072,  0.6989,  0.7025,  2.4900]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5715,  3.2357,  0.5951, -0.5005],
        [ 1.5650,  4.7287,  0.6255,  0.1054]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9102, 8.2620, 4.1107, 1.5687],
        [2.1531, 8.1762, 4.3643, 2.3837]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013532999902963638
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6730, 0.6968, 0.7148, 0.0800],
        [0.6959, 0.6929, 0.7065, 2.6925]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5798,  3.2165,  0.6130, -0.4940],
        [ 1.5298,  4.4989,  0.5930,  0.0296]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9294, 8.5800, 4.2939, 1.4854],
        [2.2054, 8.4404, 4.3587, 2.3421]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013225999660789967
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.6979,  0.6856,  0.6900, -0.0148],
        [ 0.6816,  0.6873,  0.6947,  2.7347]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5903,  3.3400,  0.6459, -0.4562],
        [ 1.6316,  4.6546,  0.6232,  0.1712]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8504, 7.9232, 3.9014, 1.6019],
        [2.2665, 8.4422, 4.4216, 2.4369]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013240999542176723
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6442, 0.7307, 0.6640, 0.0658],
        [0.7010, 0.7625, 0.6570, 2.6110]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5885,  3.4362,  0.6761, -0.5188],
        [ 1.6036,  4.8961,  0.6674,  0.0542]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8825, 8.0956, 3.9840, 1.4819],
        [2.2305, 8.0392, 4.1726, 2.3492]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013088000006973743
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6806, 0.7148, 0.6973, 0.0703],
        [0.7218, 0.7151, 0.6950, 2.5631]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4724,  3.1765,  0.6548, -0.2960],
        [ 1.6399,  4.8433,  0.7359,  0.2061]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9666, 8.4323, 4.2166, 1.5158],
        [2.2030, 8.0337, 4.0981, 2.3299]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012570999562740326
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0038, -1.0011, -1.9952, -2.9829],
        [-2.0120, -0.9856, -1.9984, -3.0666]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002133999951183796
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6974, 0.6703, 0.7132, 0.0525],
        [0.6848, 0.6793, 0.7189, 2.8040]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.6259,  3.1421,  0.8392, -0.4136],
        [ 1.5448,  4.7794,  0.6706,  0.1832]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.7897, 8.7640, 4.4104, 1.5748],
        [2.2365, 7.9961, 4.2059, 2.4177]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013082999736070633
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9945, -1.0162, -1.9649, -2.9550],
        [-2.0082, -0.9458, -2.0003, -3.0340]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002171000000089407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6712,  0.6968,  0.6769, -0.0218],
        [ 0.6743,  0.7141,  0.6450,  2.3173]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5406,  3.3024,  0.6454, -0.3874],
        [ 1.5905,  4.7051,  0.6844,  0.1597]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8286, 8.1106, 4.0508, 1.6448],
        [2.2750, 8.2788, 4.3996, 2.3586]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013069000095129013
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0102, -0.9990, -2.0189, -2.9916],
        [-2.0262, -1.0169, -2.0314, -3.1060]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002174000022932887
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9925, -0.9957, -2.0128, -2.9869],
        [-1.9548, -1.0035, -1.9893, -3.0163]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002196999965235591
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7104, 0.7104, 0.6611, 0.0469],
        [0.6970, 0.6783, 0.6972, 2.6682]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5302,  3.2912,  0.6795, -0.2686],
        [ 1.5705,  4.1869,  0.5952,  0.3815]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8630, 7.9820, 4.0216, 1.6740],
        [2.2156, 8.1990, 4.2755, 2.4092]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013438999652862549
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6913, 0.7295, 0.7108, 0.0161],
        [0.6936, 0.7208, 0.7066, 2.7669]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5718,  3.4793,  0.6013, -0.3497],
        [ 1.6165,  4.8389,  0.6948,  0.1430]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8463, 8.1334, 4.0016, 1.4695],
        [2.2112, 8.3113, 4.3586, 2.3072]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013817000202834606
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6384, 0.7094, 0.7323, 0.0098],
        [0.6896, 0.7043, 0.6640, 2.6070]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5614,  3.3490,  0.6714, -0.3902],
        [ 1.4987,  4.6460,  0.6570,  0.0650]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8590, 8.3270, 4.2840, 1.4946],
        [2.1597, 8.0859, 4.4183, 2.2422]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012875000014901161
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9661, -0.9907, -1.9826, -3.0419],
        [-2.0118, -0.9894, -2.0028, -3.0485]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023169999476522207
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6946, 0.6740, 0.7190, 0.0270],
        [0.6836, 0.7185, 0.6865, 2.6972]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4774,  3.1633,  0.6442, -0.4509],
        [ 1.4664,  4.8057,  0.7842,  0.1361]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8959, 8.0737, 4.1391, 1.6036],
        [2.0554, 7.8671, 4.3928, 2.3907]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013253999873995781
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6978, 0.6767, 0.6873, 0.0259],
        [0.6584, 0.6855, 0.6734, 2.6813]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4508,  3.3132,  0.6670, -0.3925],
        [ 1.4941,  4.9808,  0.7523,  0.1013]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8204, 8.1111, 4.1741, 1.5859],
        [2.1419, 7.9018, 4.2740, 2.5089]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01306500006467104
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[7.4490e-01, 7.0829e-01, 7.1451e-01, 5.0486e-04],
        [6.6652e-01, 7.0048e-01, 7.0596e-01, 2.4264e+00]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4382,  3.0410,  0.6781, -0.3186],
        [ 1.4597,  4.7465,  0.6707,  0.0943]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8959, 8.6432, 4.4886, 1.5404],
        [2.2162, 8.5449, 4.6439, 2.3390]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012866999953985214
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7692, 0.6796, 0.7676, 0.0301],
        [0.6628, 0.6952, 0.7002, 2.7458]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4978,  3.3003,  0.6174, -0.4305],
        [ 1.5202,  4.7179,  0.7008,  0.1690]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8506, 8.3310, 4.2000, 1.7012],
        [2.0389, 7.6379, 4.1485, 2.5369]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012839999981224537
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6985, 0.7539, 0.7246, 0.0406],
        [0.6665, 0.7047, 0.6658, 2.8475]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5180,  3.5613,  0.6785, -0.3845],
        [ 1.5217,  4.9776,  0.7149,  0.0223]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8319, 8.1726, 4.0434, 1.4972],
        [2.1492, 8.1514, 4.2644, 2.3506]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012833000160753727
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7146, 0.6959, 0.7015, 0.0031],
        [0.6790, 0.6928, 0.6656, 2.6402]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4847,  3.2318,  0.6410, -0.2473],
        [ 1.5630,  4.7107,  0.6817,  0.2735]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8612, 8.1290, 3.8637, 1.5340],
        [2.0911, 7.6396, 3.9852, 2.4496]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01295899972319603
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0162, -0.9819, -1.9832, -3.0271],
        [-2.0100, -1.0064, -2.0287, -3.1073]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022239999379962683
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9902, -1.0174, -2.0123, -2.9835],
        [-2.0335, -0.9959, -2.0156, -3.0443]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002128999913111329
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6996, 0.6884, 0.7092, 0.0700],
        [0.7126, 0.7095, 0.6985, 2.7613]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5352,  3.3895,  0.6066, -0.3149],
        [ 1.6642,  4.9013,  0.6422,  0.1599]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9604, 8.4542, 4.0581, 1.6344],
        [2.2022, 8.0223, 3.9187, 2.3702]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013247000053524971
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6277, 0.7271, 0.5921, 0.2313],
        [0.6866, 0.6887, 0.6692, 2.7562]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4838,  3.3321,  0.6413, -0.3856],
        [ 1.5856,  4.5286,  0.5836,  0.2238]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8902, 8.2464, 3.9037, 1.4928],
        [2.1832, 7.8547, 3.7873, 2.4159]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012899000197649002
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9808, -0.9937, -1.9864, -2.9115],
        [-1.9780, -0.9981, -2.0024, -3.0174]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021230001002550125
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.7152,  0.6782,  0.7723, -0.0083],
        [ 0.6916,  0.7079,  0.7300,  2.6992]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5042,  3.3774,  0.6723, -0.3378],
        [ 1.5039,  4.8201,  0.6147,  0.2367]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8917, 8.0374, 3.8611, 1.4963],
        [2.1359, 7.9069, 3.6769, 2.2567]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013350999914109707
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9689, -0.9971, -2.0045, -2.9914],
        [-2.0104, -0.9938, -1.9962, -3.0397]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023350000847131014
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6969, 0.7330, 0.7111, 0.0529],
        [0.7592, 0.6979, 0.7758, 2.5103]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5116,  3.4333,  0.6580, -0.3926],
        [ 1.6712,  4.8195,  0.6926,  0.1961]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9308, 8.3929, 3.9560, 1.5272],
        [2.2803, 8.4148, 4.0616, 2.2661]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013318999670445919
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6967, 0.7030, 0.6751, 0.0798],
        [0.6973, 0.6820, 0.6753, 2.7495]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4896,  3.2083,  0.6345, -0.2478],
        [ 1.6046,  4.7323,  0.6782,  0.1535]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.8714, 7.9023, 3.7486, 1.4976],
        [2.2995, 8.4004, 4.2326, 2.2404]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013721000403165817
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9890, -0.9731, -1.9706, -2.9287],
        [-1.9897, -1.0033, -2.0152, -3.0419]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002320999978110194
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9803, -1.0071, -2.0040, -2.9867],
        [-1.9446, -0.9844, -2.0259, -3.0312]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022619999945163727
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9718, -0.9923, -1.9599, -3.0022],
        [-1.9944, -0.9998, -2.0042, -3.0471]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022509999107569456
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6609, 0.6527, 0.6856, 0.0380],
        [0.6892, 0.6882, 0.7140, 2.6577]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4604,  3.1671,  0.7156, -0.3175],
        [ 1.5806,  5.0093,  0.9470,  0.0500]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9554, 8.1082, 4.0238, 1.4378],
        [2.2870, 8.1379, 4.1813, 2.1964]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013345999643206596
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9933, -1.0019, -2.0105, -2.9678],
        [-2.0339, -0.9971, -2.0156, -3.0754]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002148000057786703
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0207, -1.0075, -2.0065, -2.9867],
        [-2.0051, -1.0077, -2.0107, -3.0666]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021929999347776175
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0077, -1.0113, -2.0010, -2.9848],
        [-1.9940, -1.0055, -2.0077, -3.0483]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.00215300009585917
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6765, 0.7051, 0.7139, 0.0843],
        [0.6862, 0.6957, 0.6907, 2.7149]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5258,  3.6198,  0.8775, -0.1680],
        [ 1.5980,  4.5014,  0.7289,  0.2332]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0018, 8.3513, 4.4241, 1.6620],
        [2.2570, 8.2127, 4.4636, 2.3916]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01270500011742115
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6936, 0.7381, 0.7183, 0.0547],
        [0.6804, 0.7226, 0.6929, 2.6886]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5091,  3.4470,  0.8711, -0.0541],
        [ 1.5562,  4.7512,  0.8121,  0.2260]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9227, 8.4900, 4.4582, 1.7935],
        [2.4124, 8.4639, 4.5167, 2.2980]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012761999852955341
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6838, 0.6780, 0.6611, 0.0881],
        [0.6740, 0.6737, 0.6681, 2.6255]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5074,  3.3664,  0.7299, -0.0780],
        [ 1.5895,  4.8212,  0.8073,  0.1563]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9740, 7.9186, 3.9517, 1.6574],
        [2.0775, 7.7342, 4.2388, 2.3487]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012531000189483166
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9926, -0.9962, -1.9952, -2.9625],
        [-2.0399, -1.0245, -1.9991, -3.0093]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021609999239444733
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0040, -0.9931, -2.0047, -2.9841],
        [-1.9846, -0.9935, -2.0036, -3.0756]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021840000990778208
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0075, -1.0058, -1.9982, -2.9598],
        [-1.9835, -1.0066, -1.9933, -3.0279]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002119000069797039
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7066, 0.6945, 0.7423, 0.1884],
        [0.6640, 0.6959, 0.6422, 2.7586]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5428,  3.4176,  0.7134, -0.2475],
        [ 1.5930,  4.7673,  0.7006,  0.1755]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9084, 7.9461, 4.0651, 1.8050],
        [2.3735, 8.3684, 4.4087, 2.4294]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013013999909162521
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0044, -1.0084, -1.9942, -3.0136],
        [-2.0087, -1.0121, -2.0096, -3.0339]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021800000686198473
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0260, -0.9936, -2.0056, -3.0277],
        [-2.0087, -0.9955, -2.0205, -3.0454]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021019999403506517
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0000, -1.0041, -2.0419, -2.9379],
        [-2.0154, -1.0054, -2.0141, -3.0519]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0027050001081079245
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9901, -1.0055, -2.0253, -2.9820],
        [-1.9969, -1.0396, -1.9753, -3.0008]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022509999107569456
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6801, 0.7176, 0.7023, 0.0472],
        [0.6879, 0.7146, 0.6893, 2.7060]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5339,  3.2468,  0.6901, -0.1598],
        [ 1.5346,  4.6484,  0.9016,  0.3461]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0854, 8.3716, 4.1747, 1.5339],
        [2.3510, 8.1655, 4.2564, 2.3715]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013358999975025654
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0154, -1.0180, -2.0134, -2.9966],
        [-2.0147, -0.9931, -2.0087, -3.0356]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0023610000498592854
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7051, 0.7351, 0.6815, 0.1367],
        [0.7152, 0.7570, 0.7025, 2.6926]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5478,  3.1989,  0.6104, -0.1211],
        [ 1.6035,  4.7311,  0.7111,  0.2977]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0033, 8.1053, 4.1012, 1.6053],
        [2.1620, 7.5211, 3.9874, 2.2573]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013381999917328358
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9936, -0.9907, -1.9706, -2.9928],
        [-1.9672, -0.9966, -1.9789, -3.0549]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002247000113129616
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7188, 0.7187, 0.6842, 0.0209],
        [0.6990, 0.7179, 0.6778, 2.7929]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5229,  3.5221,  0.6811, -0.1852],
        [ 1.6361,  4.6478,  0.6197,  0.1973]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0738, 8.3272, 4.2254, 1.6824],
        [2.3704, 8.1989, 4.3010, 2.2905]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013237000443041325
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9754, -1.0251, -1.9767, -2.9260],
        [-1.9740, -0.9837, -2.0335, -3.0357]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021170000545680523
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.7045,  0.6876,  0.7074, -0.0043],
        [ 0.6837,  0.7266,  0.6732,  2.7061]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5619,  3.1161,  0.6727, -0.2075],
        [ 1.6131,  4.4832,  0.6789,  0.2809]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1858, 8.3152, 4.2924, 1.8058],
        [2.4516, 8.2553, 4.4610, 2.3906]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01295899972319603
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9910, -1.0507, -1.9785, -2.9284],
        [-2.0471, -1.0377, -1.9807, -3.1000]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021840000990778208
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6875, 0.6987, 0.7092, 0.0310],
        [0.6648, 0.7368, 0.6773, 2.7952]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4776,  3.3305,  0.7929, -0.2532],
        [ 1.7511,  5.1271,  0.8160,  0.0631]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0540, 8.1549, 4.2771, 1.7307],
        [2.4341, 8.0243, 4.2337, 1.7327]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01297600008547306
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6950, 0.6627, 0.7155, 0.0345],
        [0.6904, 0.6856, 0.6677, 2.7165]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4670,  3.1532,  0.6207, -0.2793],
        [ 1.6331,  4.5719,  0.6960,  0.1337]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1175, 8.4126, 4.4805, 1.7015],
        [2.4138, 8.3428, 4.4924, 2.3296]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012594000436365604
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7003, 0.7098, 0.7254, 0.0428],
        [0.6922, 0.7296, 0.6115, 2.6035]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5065,  3.3029,  0.6610, -0.2289],
        [ 1.6467,  4.7436,  0.6876,  0.0913]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9793, 8.0722, 4.1774, 1.6029],
        [2.2745, 7.8920, 4.3085, 2.4740]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012481999583542347
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.6993,  0.6463,  0.6665, -0.0379],
        [ 0.7018,  0.6701,  0.6628,  2.6157]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4721,  3.2734,  0.6164, -0.2393],
        [ 1.5885,  4.2691,  0.5112,  0.0827]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0387, 8.1780, 4.2409, 1.8387],
        [2.3406, 8.6058, 4.4437, 2.1182]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01319700013846159
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9917, -0.9965, -1.9940, -3.0064],
        [-2.0082, -1.0034, -2.0093, -3.0931]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002148000057786703
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9879, -0.9904, -1.9908, -3.0140],
        [-2.0135, -0.9900, -2.0102, -3.0904]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021550001110881567
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9916, -0.9878, -1.9746, -3.0407],
        [-1.9965, -0.9935, -1.9945, -3.0683]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002176000038161874
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7057, 0.6780, 0.6969, 0.0528],
        [0.6605, 0.6993, 0.7092, 2.9406]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5472,  3.3843,  0.6563, -0.2015],
        [ 1.5654,  4.3781,  0.5536,  0.1666]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0450, 8.2094, 4.3394, 1.7214],
        [2.3275, 8.1684, 4.4659, 2.4567]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01284400001168251
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6523, 0.7701, 0.6620, 0.2553],
        [0.6793, 0.6973, 0.6921, 2.7308]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5083,  3.4896,  0.7105, -0.1787],
        [ 1.5295,  4.8898,  0.7590,  0.0591]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0073, 8.1364, 4.2898, 1.5609],
        [2.2621, 7.5798, 4.0711, 2.2726]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012954999692738056
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.6684,  0.7311,  0.6654, -0.1635],
        [ 0.6878,  0.6743,  0.6879,  2.6643]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5018,  3.3508,  0.7046, -0.2106],
        [ 1.6118,  4.9507,  0.7962,  0.1666]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0580, 8.3822, 4.4428, 1.6602],
        [2.3738, 8.5265, 4.7926, 2.5689]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012817000038921833
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9713, -0.9418, -1.9813, -2.9530],
        [-2.0244, -0.9973, -1.9894, -3.0841]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002225999953225255
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0096, -0.9961, -1.9988, -3.0109],
        [-1.9906, -1.0000, -1.9835, -3.0057]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002426000079140067
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.7087,  0.6829,  0.6623, -0.0340],
        [ 0.6509,  0.6881,  0.6737,  2.7449]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4678,  3.3399,  0.6957, -0.1298],
        [ 1.5907,  4.6676,  0.7329,  0.2004]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0915, 8.1537, 4.2496, 1.7380],
        [2.4371, 8.1012, 4.3725, 2.3563]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013016000390052795
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7066, 0.6982, 0.6930, 0.0317],
        [0.6827, 0.7334, 0.7142, 2.6841]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5210,  3.2561,  0.6924, -0.1457],
        [ 1.5664,  4.6596,  0.7491,  0.2047]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0416, 8.0068, 4.2180, 1.6426],
        [2.3598, 7.9859, 4.3118, 2.3482]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.012749000452458858
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9827, -1.0099, -1.9743, -2.9256],
        [-2.0248, -1.0101, -1.9976, -3.0427]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002107999986037612
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9969, -1.0070, -1.9962, -2.9691],
        [-1.9962, -0.9905, -1.9802, -2.9911]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002131999935954809
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9920, -0.9857, -2.0126, -3.0039],
        [-1.9931, -0.9726, -2.0003, -3.0423]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021800000686198473
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9995, -0.9880, -2.0002, -3.0094],
        [-1.9845, -0.9979, -2.0189, -3.0600]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0026970000471919775
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0073, -0.9950, -2.0017, -3.0234],
        [-1.9944, -1.0132, -1.9902, -3.0998]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021669999696314335
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9638, -1.0211, -2.0030, -3.0418],
        [-2.0395, -1.0396, -2.0091, -3.0932]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023189999628812075
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0309, -1.0131, -2.0091, -3.0119],
        [-2.0136, -1.0281, -1.9986, -3.0872]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002357000019401312
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0128, -0.9736, -1.9874, -2.9537],
        [-2.0084, -1.0038, -1.9879, -3.0497]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021100000012665987
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9912, -0.9920, -2.0116, -2.9944],
        [-2.0090, -1.0069, -2.0064, -3.1259]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002245000097900629
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9873, -1.0134, -2.0119, -2.9723],
        [-1.9910, -1.0093, -1.9944, -3.0454]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021250001154839993
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6936, 0.6776, 0.7068, 0.1679],
        [0.6978, 0.6813, 0.7081, 2.7331]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4340, 3.4384, 0.5652, 0.0352],
        [1.5661, 4.6811, 0.6794, 0.2511]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1228, 8.4856, 4.5836, 1.8366],
        [2.3246, 8.1311, 4.5205, 2.4868]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01270500011742115
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.6734,  0.6944,  0.7058, -0.0068],
        [ 0.6840,  0.7035,  0.6867,  2.7569]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5197,  3.5039,  0.7616, -0.0412],
        [ 1.6388,  4.7659,  0.6879,  0.1799]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1185, 8.1584, 4.6489, 1.7767],
        [2.4159, 8.2672, 4.6661, 2.4588]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012745000422000885
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-3.0231, -2.0607, -2.0587, -3.0455],
        [-3.1126, -2.0391, -2.0254, -3.2731]], device='cuda:0')	action:2	policy_index:1
scalar_q_values:tensor([[-2.0094, -0.9976, -2.0101, -2.9731],
        [-2.0060, -0.9948, -1.9995, -3.0536]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.011517000384628773
Total Reward: [ 0.7 -3. ], Discounted: [ 0.6792093 -2.940399 ]
Scalarized Reward: -3.0, Discounted: -2.940398931503296
scalar_q_values:tensor([[ 0.7056,  0.7547,  0.6888, -0.0473],
        [ 0.6635,  0.6756,  0.7028,  2.6086]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4566,  3.1027,  0.6026, -0.0179],
        [ 1.6532,  4.5961,  0.6965,  0.0722]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0393, 8.1607, 4.4895, 1.6949],
        [2.4137, 8.2760, 4.6653, 2.3467]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.012400000356137753
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.6951,  0.7233,  0.6973, -0.0125],
        [ 0.7434,  0.6883,  0.6570,  2.6377]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5583,  3.2855,  0.5829, -0.0501],
        [ 1.6565,  4.4942,  0.6408,  0.1102]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0248, 8.1313, 4.4709, 1.7421],
        [2.2619, 8.3253, 4.6151, 2.4122]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013508000411093235
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9463, -0.9922, -2.0097, -2.9476],
        [-1.9915, -1.0023, -1.9890, -3.0371]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002747999969869852
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9962, -0.9959, -2.0038, -2.9976],
        [-2.0071, -1.0099, -2.0180, -3.0642]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002481000032275915
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7073, 0.6752, 0.6765, 0.0705],
        [0.6904, 0.7060, 0.6890, 2.7847]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4900,  3.4210,  0.7181, -0.0447],
        [ 1.6305,  4.7629,  0.7564,  0.1417]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[1.9381, 8.0786, 4.5587, 1.8663],
        [2.2978, 8.3029, 4.7090, 2.4053]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.02398099936544895
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0119, -1.0130, -1.9977, -2.9126],
        [-2.0436, -1.0116, -2.0205, -3.0851]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002406999934464693
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.7231,  0.7397,  0.7018, -0.0111],
        [ 0.5990,  0.7331,  0.7082,  2.7960]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5521,  3.3950,  0.7051, -0.0422],
        [ 1.6626,  4.7936,  0.7161,  0.1254]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0608, 8.1456, 4.5942, 1.8376],
        [2.3377, 8.4089, 4.8304, 2.5419]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.015079000033438206
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9717, -0.9985, -2.0042, -2.9593],
        [-1.9780, -1.0176, -1.9521, -2.9296]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0026779999025166035
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9792, -0.9918, -2.0033, -2.9864],
        [-1.9749, -1.0050, -1.9936, -3.0180]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00279600010253489
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9734, -1.0058, -1.9961, -2.9776],
        [-1.9667, -1.0204, -2.0060, -3.0327]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002280999906361103
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9957, -1.0064, -2.0067, -2.9899],
        [-1.9991, -1.0074, -2.0166, -3.0603]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002274000085890293
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6837, 0.7671, 0.7202, 0.0630],
        [0.6860, 0.7273, 0.7031, 2.6760]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.5767, 3.3953, 0.6817, 0.0507],
        [1.6140, 4.6442, 0.6596, 0.2489]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0455, 8.1755, 4.6646, 1.8339],
        [2.3620, 8.2706, 4.9031, 2.4865]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013478999957442284
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0093, -0.9966, -2.0244, -2.9696],
        [-1.9970, -1.0087, -1.9873, -3.0515]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002148000057786703
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6660, 0.6909, 0.6931, 0.1001],
        [0.6770, 0.6933, 0.6759, 2.6677]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.5023, 3.1995, 0.6438, 0.0099],
        [1.7221, 4.7477, 0.7301, 0.1523]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0248, 8.0448, 4.4952, 1.2887],
        [2.4367, 8.0365, 4.6690, 2.5045]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013764999806880951
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9742, -0.9910, -1.9630, -2.9814],
        [-1.9972, -0.9804, -1.9776, -3.0545]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002133999951183796
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.7153,  0.7446,  0.6263, -0.0917],
        [ 0.6951,  0.7207,  0.6753,  2.7142]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.5764,  3.3979,  0.6530, -0.0292],
        [ 1.8220,  4.9044,  0.7502,  0.0622]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0878, 8.2334, 4.6554, 1.7361],
        [2.3869, 8.0880, 4.6485, 2.3776]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013697000220417976
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9978, -1.0022, -1.9927, -2.9854],
        [-1.9953, -0.9842, -1.9830, -3.0756]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0026700000744313
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9865, -0.9922, -1.9954, -2.9650],
        [-1.9904, -0.9968, -1.9876, -3.0582]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002259999979287386
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 6.3525e-01,  6.7146e-01,  7.3563e-01, -2.6329e-03],
        [ 6.7327e-01,  7.0082e-01,  6.8476e-01,  2.7700e+00]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4567,  3.2832,  0.6867, -0.0099],
        [ 1.6713,  4.7168,  0.7078,  0.1154]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0869, 8.6611, 4.7585, 1.5608],
        [2.3902, 8.2888, 4.7568, 2.4115]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013729000464081764
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[6.9340e-01, 6.9129e-01, 6.8664e-01, 2.3291e-03],
        [6.7037e-01, 6.7980e-01, 6.8870e-01, 2.7127e+00]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4935, 3.2826, 0.6960, 0.0083],
        [1.7013, 4.6112, 0.6675, 0.1515]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1375, 8.5984, 4.8670, 1.7408],
        [2.3567, 7.7965, 4.3129, 2.2775]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.014039999805390835
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7041, 0.6946, 0.6753, 0.0095],
        [0.6790, 0.7136, 0.6716, 2.5897]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.5623, 3.2186, 0.6908, 0.0492],
        [1.7184, 4.8355, 0.6927, 0.0615]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0691, 8.2024, 4.5745, 1.6790],
        [2.4013, 8.2621, 4.8844, 2.1914]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.014197999611496925
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.7147, 0.7085, 0.7098, 0.0398],
        [0.6496, 0.6952, 0.7108, 2.6673]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.5060, 3.3273, 0.6078, 0.0244],
        [1.7239, 4.7348, 0.7157, 0.1284]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0128, 7.9859, 4.3655, 1.6853],
        [2.4329, 8.4333, 4.7414, 2.1947]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013768999837338924
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6700, 0.6948, 0.6562, 0.0198],
        [0.6311, 0.7271, 0.6681, 2.6325]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4816,  3.2672,  0.7927, -0.0724],
        [ 1.7854,  4.7935,  0.7086,  0.1703]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0836, 8.1459, 4.4547, 1.6754],
        [2.4062, 8.2414, 4.6468, 2.2715]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013829999603331089
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0071, -0.9969, -2.0128, -3.0053],
        [-1.9830, -1.0119, -2.0211, -3.0359]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002498999936506152
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9935, -1.0042, -1.9842, -2.9637],
        [-1.9873, -0.9972, -2.0067, -3.0729]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002234000014141202
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6926,  0.6767,  0.6474, -0.0079],
        [ 0.6736,  0.7094,  0.6790,  2.6525]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 5.4436e-01,  3.2661e+00,  6.3334e-01, -4.0179e-04],
        [ 1.7497e+00,  4.8653e+00,  7.0650e-01,  1.8173e-01]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1099, 8.1304, 4.5457, 1.9011],
        [2.3214, 7.9985, 4.4718, 2.4314]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.014448000118136406
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6944, 0.6846, 0.7058, 0.0479],
        [0.6761, 0.7097, 0.6900, 2.7267]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.5433, 3.3166, 0.6086, 0.0385],
        [1.7588, 4.7832, 0.6805, 0.1662]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0626, 8.2415, 4.3702, 1.7458],
        [2.4051, 7.5942, 4.2441, 2.3007]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013643000274896622
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9928, -0.9980, -1.9990, -2.9670],
        [-1.9855, -1.0013, -2.0058, -3.0386]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0023769999388605356
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.6972,  0.7112,  0.6939, -0.0307],
        [ 0.7255,  0.6953,  0.6921,  2.7693]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.5278, 3.4434, 0.6496, 0.0886],
        [1.7252, 4.7844, 0.6575, 0.0417]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.2130, 7.6764, 4.1951, 1.8559],
        [2.3929, 8.2194, 4.5400, 2.1296]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.01375999953597784
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.7097,  0.7178,  0.6815, -0.0484],
        [ 0.6218,  0.7275,  0.6881,  2.7268]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[5.0998e-01, 3.4424e+00, 6.6435e-01, 3.5642e-03],
        [1.5861e+00, 4.7145e+00, 6.7654e-01, 1.2811e-01]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1384, 8.1245, 4.3735, 1.5844],
        [2.4323, 8.0686, 4.4074, 2.3294]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013732999563217163
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.7034,  0.7191,  0.6929, -0.0043],
        [ 0.6938,  0.7213,  0.6646,  2.7556]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[5.0602e-01, 3.2880e+00, 7.0562e-01, 5.5319e-04],
        [1.7112e+00, 4.2991e+00, 4.8838e-01, 1.3531e-01]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1987, 8.3998, 4.5652, 1.6705],
        [2.6096, 8.5159, 4.4350, 2.2374]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013914000242948532
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6612, 0.7186, 0.7315, 0.0397],
        [0.6812, 0.7112, 0.6688, 2.6997]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4839,  3.2833,  0.6231, -0.0073],
        [ 1.6714,  4.5178,  0.7606,  0.2424]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1932, 8.1507, 4.3807, 1.6668],
        [2.4989, 8.1177, 4.4621, 2.3183]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.025630999356508255
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6969, 0.7371, 0.7089, 0.0817],
        [0.6955, 0.7107, 0.6950, 2.6675]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4780,  3.2791,  0.6477, -0.0110],
        [ 1.7296,  4.7900,  0.8114,  0.2076]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.2271, 8.2215, 4.4151, 1.7034],
        [2.4833, 8.4861, 4.6762, 2.2166]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.015495000407099724
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9950, -0.9825, -1.9506, -2.9503],
        [-1.9972, -0.9962, -1.9791, -3.0420]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0021510000806301832
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6966, 0.6990, 0.6538, 0.0548],
        [0.7148, 0.6753, 0.6771, 2.6828]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.5609, 3.4859, 0.7096, 0.0566],
        [1.8672, 4.8561, 0.7600, 0.1873]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.2108, 8.2211, 4.4679, 1.5865],
        [2.5025, 8.1001, 4.5713, 2.2462]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013238999992609024
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.6762,  0.7094,  0.6950, -0.0686],
        [ 0.7247,  0.6477,  0.6812,  2.5423]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4831,  3.2488,  0.6830, -0.0496],
        [ 1.8205,  4.5336,  0.5477,  0.1642]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.3041, 8.4618, 4.6704, 1.6723],
        [2.5444, 8.3072, 4.6333, 2.1804]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013109000399708748
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9759, -0.9899, -2.0131, -2.9723],
        [-1.9774, -0.9800, -2.0168, -3.0164]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0021589999087154865
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9918, -0.9909, -2.0248, -2.9552],
        [-1.9908, -1.0090, -2.0209, -3.0820]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002065999899059534
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6450, 0.7246, 0.6447, 0.2382],
        [0.7298, 0.6637, 0.6590, 2.7321]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4916, 3.3653, 0.7182, 0.0209],
        [1.8127, 4.8524, 0.6376, 0.0506]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1943, 8.2927, 4.4486, 1.4954],
        [2.5529, 8.4096, 4.7751, 2.2601]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.014421000145375729
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-2.0005, -1.0059, -2.0024, -3.0436],
        [-2.0098, -1.0118, -1.9978, -3.0736]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.00215300009585917
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-1.9848, -0.9961, -2.0222, -2.9766],
        [-1.9553, -1.0125, -2.0494, -3.0436]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002131999935954809
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.7350,  0.7487,  0.6794, -0.0190],
        [ 0.6846,  0.6808,  0.6220,  2.5796]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4527,  3.2956,  0.7028, -0.0140],
        [ 1.7830,  4.7179,  0.6453,  0.1428]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.2433, 8.4922, 4.5890, 1.4384],
        [2.5373, 8.2768, 4.7214, 2.2324]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013433000072836876
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.7020,  0.7084,  0.7580, -0.0909],
        [ 0.7057,  0.7308,  0.6778,  2.8204]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4597, 3.1911, 0.6614, 0.0414],
        [1.7114, 4.2048, 0.6392, 0.3858]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0770, 7.6414, 4.3682, 1.7825],
        [2.4816, 8.0449, 4.5269, 2.2904]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013648999854922295
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[ 0.7156,  0.6779,  0.6703, -0.0113],
        [ 0.6765,  0.7597,  0.6574,  2.6911]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4375, 2.9034, 0.6792, 0.0642],
        [1.8319, 5.0280, 0.7361, 0.1754]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1975, 8.5115, 4.5929, 1.5885],
        [2.3874, 8.5065, 4.6004, 2.1526]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.01362599991261959
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[0.6954, 0.7366, 0.6774, 0.0504],
        [0.6458, 0.6946, 0.6849, 2.5798]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[ 0.4494,  3.2506,  0.5986, -0.0262],
        [ 1.7912,  4.5373,  0.5825,  0.1390]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.0949, 7.6134, 4.0754, 1.8128],
        [2.3600, 7.8004, 4.2214, 2.2908]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.013611000031232834
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9964, -1.0202, -2.0045, -2.9817],
        [-1.9986, -0.9864, -1.9937, -3.0561]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002320999978110194
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0154, -0.9685, -1.9879, -3.0147],
        [-2.0169, -0.9893, -2.0180, -3.0452]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002257999964058399
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0316, -1.0020, -2.0170, -2.9960],
        [-1.9953, -1.0067, -1.9855, -2.9984]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.0022279999684542418
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[-2.0024, -0.9864, -2.0427, -2.9990],
        [-1.9985, -0.9920, -2.0109, -3.0719]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 1, Time: 0.002171000000089407
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.6426, 0.7045, 0.6395, 0.3345],
        [0.6783, 0.7257, 0.6709, 2.7413]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4784, 3.1237, 0.6547, 0.0833],
        [1.8281, 4.7016, 0.7243, 0.2182]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.2323, 8.1261, 4.2043, 1.5895],
        [2.4831, 8.1272, 4.2730, 2.2937]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 3, Time: 0.014174000360071659
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9928, -0.9999, -1.9946, -2.9859],
        [-1.9869, -0.9950, -1.9953, -3.0393]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.0022529999259859324
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[ 0.7128,  0.7103,  0.6626, -0.1173],
        [ 0.6674,  0.7173,  0.6985,  2.7823]], device='cuda:0')	action:3	policy_index:1
scalar_q_values:tensor([[0.4540, 3.5190, 0.7377, 0.0343],
        [1.9271, 4.9977, 0.7223, 0.1910]], device='cuda:0')	action:1	policy_index:1
scalar_q_values:tensor([[2.1594, 8.3244, 4.2588, 1.5596],
        [2.5200, 8.2275, 4.3308, 2.2697]], device='cuda:0')	action:1	policy_index:0
Episode infos:
Steps: 3, Time: 0.013512000441551208
Total Reward: [ 8.2 -3. ], Discounted: [ 7.9564514 -2.940399 ]
Scalarized Reward: 8.199999809265137, Discounted: 7.956451416015625
scalar_q_values:tensor([[-1.9778, -0.9933, -1.9237, -2.9902],
        [-2.0018, -0.9930, -2.0369, -3.0464]], device='cuda:0')	action:1	policy_index:1
Episode infos:
Steps: 1, Time: 0.002203000010922551
Total Reward: [ 0.7 -1. ], Discounted: [ 0.69299996 -0.99      ]
Scalarized Reward: -1.0, Discounted: -0.9900000095367432
scalar_q_values:tensor([[0.7250, 0.7077, 0.6814, 0.0462],
